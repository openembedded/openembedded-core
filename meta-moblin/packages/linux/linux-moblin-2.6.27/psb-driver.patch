Index: linux-2.6.27/include/drm/drm.h
===================================================================
--- linux-2.6.27.orig/include/drm/drm.h	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/include/drm/drm.h	2009-02-05 13:29:33.000000000 +0000
@@ -173,6 +173,7 @@
 	_DRM_AGP = 3,		  /**< AGP/GART */
 	_DRM_SCATTER_GATHER = 4,  /**< Scatter/gather memory for PCI DMA */
 	_DRM_CONSISTENT = 5,	  /**< Consistent memory for PCI DMA */
+	_DRM_TTM = 7
 };
 
 /**
@@ -598,6 +599,400 @@
 	uint64_t size;
 };
 
+#define DRM_FENCE_FLAG_EMIT                0x00000001
+#define DRM_FENCE_FLAG_SHAREABLE           0x00000002
+#define DRM_FENCE_FLAG_WAIT_LAZY           0x00000004
+#define DRM_FENCE_FLAG_WAIT_IGNORE_SIGNALS 0x00000008
+#define DRM_FENCE_FLAG_NO_USER             0x00000010
+
+/* Reserved for driver use */
+#define DRM_FENCE_MASK_DRIVER              0xFF000000
+
+#define DRM_FENCE_TYPE_EXE                 0x00000001
+
+struct drm_fence_arg {
+	unsigned int handle;
+	unsigned int fence_class;
+	unsigned int type;
+	unsigned int flags;
+	unsigned int signaled;
+	unsigned int error;
+	unsigned int sequence;
+	unsigned int pad64;
+	uint64_t expand_pad[2]; /*Future expansion */
+};
+
+/* Buffer permissions, referring to how the GPU uses the buffers.
+ * these translate to fence types used for the buffers.
+ * Typically a texture buffer is read, A destination buffer is write and
+ *  a command (batch-) buffer is exe. Can be or-ed together.
+ */
+
+#define DRM_BO_FLAG_READ        (1ULL << 0)
+#define DRM_BO_FLAG_WRITE       (1ULL << 1)
+#define DRM_BO_FLAG_EXE         (1ULL << 2)
+
+/*
+ * Status flags. Can be read to determine the actual state of a buffer.
+ * Can also be set in the buffer mask before validation.
+ */
+
+/*
+ * Mask: Never evict this buffer. Not even with force. This type of buffer is only
+ * available to root and must be manually removed before buffer manager shutdown
+ * or lock.
+ * Flags: Acknowledge
+ */
+#define DRM_BO_FLAG_NO_EVICT    (1ULL << 4)
+
+/*
+ * Mask: Require that the buffer is placed in mappable memory when validated.
+ *       If not set the buffer may or may not be in mappable memory when validated.
+ * Flags: If set, the buffer is in mappable memory.
+ */
+#define DRM_BO_FLAG_MAPPABLE    (1ULL << 5)
+
+/* Mask: The buffer should be shareable with other processes.
+ * Flags: The buffer is shareable with other processes.
+ */
+#define DRM_BO_FLAG_SHAREABLE   (1ULL << 6)
+
+/* Mask: If set, place the buffer in cache-coherent memory if available.
+ *       If clear, never place the buffer in cache coherent memory if validated.
+ * Flags: The buffer is currently in cache-coherent memory.
+ */
+#define DRM_BO_FLAG_CACHED      (1ULL << 7)
+
+/* Mask: Make sure that every time this buffer is validated,
+ *       it ends up on the same location provided that the memory mask is the same.
+ *       The buffer will also not be evicted when claiming space for
+ *       other buffers. Basically a pinned buffer but it may be thrown out as
+ *       part of buffer manager shutdown or locking.
+ * Flags: Acknowledge.
+ */
+#define DRM_BO_FLAG_NO_MOVE     (1ULL << 8)
+
+/* Mask: Make sure the buffer is in cached memory when mapped
+ * Flags: Acknowledge.
+ * Buffers allocated with this flag should not be used for suballocators
+ * This type may have issues on CPUs with over-aggressive caching
+ * http://marc.info/?l=linux-kernel&m=102376926732464&w=2
+ */
+#define DRM_BO_FLAG_CACHED_MAPPED    (1ULL << 19)
+
+
+/* Mask: Force DRM_BO_FLAG_CACHED flag strictly also if it is set.
+ * Flags: Acknowledge.
+ */
+#define DRM_BO_FLAG_FORCE_CACHING  (1ULL << 13)
+
+/*
+ * Mask: Force DRM_BO_FLAG_MAPPABLE flag strictly also if it is clear.
+ * Flags: Acknowledge.
+ */
+#define DRM_BO_FLAG_FORCE_MAPPABLE (1ULL << 14)
+#define DRM_BO_FLAG_TILE           (1ULL << 15)
+
+/*
+ * Memory type flags that can be or'ed together in the mask, but only
+ * one appears in flags.
+ */
+
+/* System memory */
+#define DRM_BO_FLAG_MEM_LOCAL  (1ULL << 24)
+/* Translation table memory */
+#define DRM_BO_FLAG_MEM_TT     (1ULL << 25)
+/* Vram memory */
+#define DRM_BO_FLAG_MEM_VRAM   (1ULL << 26)
+/* Up to the driver to define. */
+#define DRM_BO_FLAG_MEM_PRIV0  (1ULL << 27)
+#define DRM_BO_FLAG_MEM_PRIV1  (1ULL << 28)
+#define DRM_BO_FLAG_MEM_PRIV2  (1ULL << 29)
+#define DRM_BO_FLAG_MEM_PRIV3  (1ULL << 30)
+#define DRM_BO_FLAG_MEM_PRIV4  (1ULL << 31)
+/* We can add more of these now with a 64-bit flag type */
+
+/* Memory flag mask */
+#define DRM_BO_MASK_MEM         0x00000000FF000000ULL
+#define DRM_BO_MASK_MEMTYPE     0x00000000FF0800A0ULL
+
+/* Driver-private flags */
+#define DRM_BO_MASK_DRIVER      0xFFFF000000000000ULL
+
+/* Don't block on validate and map */
+#define DRM_BO_HINT_DONT_BLOCK  0x00000002
+/* Don't place this buffer on the unfenced list.*/
+#define DRM_BO_HINT_DONT_FENCE  0x00000004
+#define DRM_BO_HINT_WAIT_LAZY   0x00000008
+#define DRM_BO_HINT_PRESUMED_OFFSET 0x00000010
+
+#define DRM_BO_INIT_MAGIC 0xfe769812
+#define DRM_BO_INIT_MAJOR 1
+#define DRM_BO_INIT_MINOR 0
+#define DRM_BO_INIT_PATCH 0
+
+
+struct drm_bo_info_req {
+	uint64_t mask;
+	uint64_t flags;
+	unsigned int handle;
+	unsigned int hint;
+	unsigned int fence_class;
+	unsigned int desired_tile_stride;
+	unsigned int tile_info;
+	unsigned int pad64;
+        uint64_t presumed_offset;
+};
+
+struct drm_bo_create_req {
+	uint64_t mask;
+	uint64_t size;
+	uint64_t buffer_start;
+	unsigned int hint;
+	unsigned int page_alignment;
+};
+
+
+/*
+ * Reply flags
+ */
+
+#define DRM_BO_REP_BUSY 0x00000001
+
+struct drm_bo_info_rep {
+	uint64_t flags;
+	uint64_t mask;
+	uint64_t size;
+	uint64_t offset;
+	uint64_t arg_handle;
+	uint64_t buffer_start;
+	unsigned int handle;
+	unsigned int fence_flags;
+	unsigned int rep_flags;
+	unsigned int page_alignment;
+	unsigned int desired_tile_stride;
+	unsigned int hw_tile_stride;
+	unsigned int tile_info;
+	unsigned int pad64;
+	uint64_t expand_pad[4]; /*Future expansion */
+};
+
+struct drm_bo_arg_rep {
+	struct drm_bo_info_rep bo_info;
+	int ret;
+	unsigned int pad64;
+};
+
+struct drm_bo_create_arg {
+	union {
+		struct drm_bo_create_req req;
+		struct drm_bo_info_rep rep;
+	} d;
+};
+
+struct drm_bo_handle_arg {
+	unsigned int handle;
+};
+
+struct drm_bo_reference_info_arg {
+	union {
+		struct drm_bo_handle_arg req;
+		struct drm_bo_info_rep rep;
+	} d;
+};
+
+struct drm_bo_map_wait_idle_arg {
+	union {
+		struct drm_bo_info_req req;
+		struct drm_bo_info_rep rep;
+	} d;
+};
+
+struct drm_bo_op_req {
+	enum {
+		drm_bo_validate,
+		drm_bo_fence,
+		drm_bo_ref_fence,
+	} op;
+	unsigned int arg_handle;
+	struct drm_bo_info_req bo_req;
+};
+
+
+struct drm_bo_op_arg {
+	uint64_t next;
+	union {
+		struct drm_bo_op_req req;
+		struct drm_bo_arg_rep rep;
+	} d;
+	int handled;
+	unsigned int pad64;
+};
+
+
+#define DRM_BO_MEM_LOCAL 0
+#define DRM_BO_MEM_TT 1
+#define DRM_BO_MEM_VRAM 2
+#define DRM_BO_MEM_PRIV0 3
+#define DRM_BO_MEM_PRIV1 4
+#define DRM_BO_MEM_PRIV2 5
+#define DRM_BO_MEM_PRIV3 6
+#define DRM_BO_MEM_PRIV4 7
+
+#define DRM_BO_MEM_TYPES 8 /* For now. */
+
+#define DRM_BO_LOCK_UNLOCK_BM       (1 << 0)
+#define DRM_BO_LOCK_IGNORE_NO_EVICT (1 << 1)
+
+struct drm_bo_version_arg {
+	uint32_t major;
+	uint32_t minor;
+	uint32_t patchlevel;
+};
+
+struct drm_mm_type_arg {
+	unsigned int mem_type;
+	unsigned int lock_flags;
+};
+
+struct drm_mm_init_arg {
+	unsigned int magic;
+	unsigned int major;
+	unsigned int minor;
+	unsigned int mem_type;
+	uint64_t p_offset;
+	uint64_t p_size;
+};
+
+/*
+ * Drm mode setting
+ */
+#define DRM_DISPLAY_INFO_LEN 32
+#define DRM_OUTPUT_NAME_LEN 32
+#define DRM_DISPLAY_MODE_LEN 32
+#define DRM_PROP_NAME_LEN 32
+
+#define DRM_MODE_TYPE_BUILTIN	(1<<0)
+#define DRM_MODE_TYPE_CLOCK_C	((1<<1) | DRM_MODE_TYPE_BUILTIN)
+#define DRM_MODE_TYPE_CRTC_C	((1<<2) | DRM_MODE_TYPE_BUILTIN)
+#define DRM_MODE_TYPE_PREFERRED	(1<<3)
+#define DRM_MODE_TYPE_DEFAULT	(1<<4)
+#define DRM_MODE_TYPE_USERDEF	(1<<5)
+#define DRM_MODE_TYPE_DRIVER	(1<<6)
+#define DRM_MODE_TYPE_USERPREF	(1<<7)
+
+struct drm_mode_modeinfo {
+
+	unsigned int id;
+
+	unsigned int clock;
+	unsigned short hdisplay, hsync_start, hsync_end, htotal, hskew;
+	unsigned short vdisplay, vsync_start, vsync_end, vtotal, vscan;
+
+	unsigned int vrefresh; /* vertical refresh * 1000 */
+
+	unsigned int flags;
+	unsigned int type;
+	char name[DRM_DISPLAY_MODE_LEN];
+};
+
+struct drm_mode_card_res {
+
+	int count_fbs;
+	unsigned int __user *fb_id;
+
+	int count_crtcs;
+	unsigned int __user *crtc_id;
+
+	int count_outputs;
+	unsigned int __user *output_id;
+
+	int count_modes;
+        struct drm_mode_modeinfo __user *modes;
+
+};
+
+struct drm_mode_crtc {
+	unsigned int crtc_id; /**< Id */
+	unsigned int fb_id; /**< Id of framebuffer */
+
+	int x, y; /**< Position on the frameuffer */
+
+	unsigned int mode; /**< Current mode used */
+
+	int count_outputs;
+	unsigned int outputs; /**< Outputs that are connected */
+
+	int count_possibles;
+	unsigned int possibles; /**< Outputs that can be connected */
+
+	unsigned int __user *set_outputs; /**< Outputs to be connected */
+
+	int gamma_size;
+
+};
+
+struct drm_mode_get_output {
+
+	unsigned int output; /**< Id */
+	unsigned int crtc; /**< Id of crtc */
+	unsigned char name[DRM_OUTPUT_NAME_LEN];
+
+	unsigned int connection;
+	unsigned int mm_width, mm_height; /**< HxW in millimeters */
+	unsigned int subpixel;
+
+	int count_crtcs;
+	unsigned int crtcs; /**< possible crtc to connect to */
+
+	int count_clones;
+	unsigned int clones; /**< list of clones */
+
+	int count_modes;
+	unsigned int __user *modes; /**< list of modes it supports */
+
+	int count_props;
+	unsigned int __user *props;
+	unsigned int __user *prop_values;
+};
+
+#define DRM_MODE_PROP_PENDING (1<<0)
+#define DRM_MODE_PROP_RANGE (1<<1)
+#define DRM_MODE_PROP_IMMUTABLE (1<<2)
+#define DRM_MODE_PROP_ENUM (1<<3) // enumerated type with text strings
+
+struct drm_mode_property_enum {
+	uint32_t value;
+	unsigned char name[DRM_PROP_NAME_LEN];
+};
+
+struct drm_mode_get_property {
+
+	unsigned int prop_id;
+	unsigned int flags;
+	unsigned char name[DRM_PROP_NAME_LEN];
+
+	int count_values;
+	uint32_t __user *values;
+
+	int count_enums;
+	struct drm_mode_property_enum *enums;
+};
+
+struct drm_mode_fb_cmd {
+        unsigned int buffer_id;
+        unsigned int width, height;
+        unsigned int pitch;
+        unsigned int bpp;
+        unsigned int handle;
+	unsigned int depth;
+};
+
+struct drm_mode_mode_cmd {
+	unsigned int output_id;
+	unsigned int mode_id;
+};
+
 #define DRM_IOCTL_BASE			'd'
 #define DRM_IO(nr)			_IO(DRM_IOCTL_BASE,nr)
 #define DRM_IOR(nr,type)		_IOR(DRM_IOCTL_BASE,nr,type)
@@ -664,6 +1059,47 @@
 
 #define DRM_IOCTL_UPDATE_DRAW		DRM_IOW(0x3f, struct drm_update_draw)
 
+#define DRM_IOCTL_MM_INIT               DRM_IOWR(0xc0, struct drm_mm_init_arg)
+#define DRM_IOCTL_MM_TAKEDOWN           DRM_IOWR(0xc1, struct drm_mm_type_arg)
+#define DRM_IOCTL_MM_LOCK               DRM_IOWR(0xc2, struct drm_mm_type_arg)
+#define DRM_IOCTL_MM_UNLOCK             DRM_IOWR(0xc3, struct drm_mm_type_arg)
+
+#define DRM_IOCTL_FENCE_CREATE          DRM_IOWR(0xc4, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_REFERENCE       DRM_IOWR(0xc6, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_UNREFERENCE     DRM_IOWR(0xc7, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_SIGNALED        DRM_IOWR(0xc8, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_FLUSH           DRM_IOWR(0xc9, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_WAIT            DRM_IOWR(0xca, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_EMIT            DRM_IOWR(0xcb, struct drm_fence_arg)
+#define DRM_IOCTL_FENCE_BUFFERS         DRM_IOWR(0xcc, struct drm_fence_arg)
+
+#define DRM_IOCTL_BO_CREATE             DRM_IOWR(0xcd, struct drm_bo_create_arg)
+#define DRM_IOCTL_BO_MAP                DRM_IOWR(0xcf, struct drm_bo_map_wait_idle_arg)
+#define DRM_IOCTL_BO_UNMAP              DRM_IOWR(0xd0, struct drm_bo_handle_arg)
+#define DRM_IOCTL_BO_REFERENCE          DRM_IOWR(0xd1, struct drm_bo_reference_info_arg)
+#define DRM_IOCTL_BO_UNREFERENCE        DRM_IOWR(0xd2, struct drm_bo_handle_arg)
+#define DRM_IOCTL_BO_SETSTATUS          DRM_IOWR(0xd3, struct drm_bo_map_wait_idle_arg)
+#define DRM_IOCTL_BO_INFO               DRM_IOWR(0xd4, struct drm_bo_reference_info_arg)
+#define DRM_IOCTL_BO_WAIT_IDLE          DRM_IOWR(0xd5, struct drm_bo_map_wait_idle_arg)
+#define DRM_IOCTL_BO_VERSION          DRM_IOR(0xd6, struct drm_bo_version_arg)
+
+
+#define DRM_IOCTL_MODE_GETRESOURCES     DRM_IOWR(0xA0, struct drm_mode_card_res)
+#define DRM_IOCTL_MODE_GETCRTC          DRM_IOWR(0xA1, struct drm_mode_crtc)
+#define DRM_IOCTL_MODE_GETOUTPUT        DRM_IOWR(0xA2, struct drm_mode_get_output)
+#define DRM_IOCTL_MODE_SETCRTC          DRM_IOWR(0xA3, struct drm_mode_crtc)
+#define DRM_IOCTL_MODE_ADDFB            DRM_IOWR(0xA4, struct drm_mode_fb_cmd)
+#define DRM_IOCTL_MODE_RMFB             DRM_IOWR(0xA5, unsigned int)
+#define DRM_IOCTL_MODE_GETFB            DRM_IOWR(0xA6, struct drm_mode_fb_cmd)
+
+#define DRM_IOCTL_MODE_ADDMODE         DRM_IOWR(0xA7, struct drm_mode_modeinfo)
+#define DRM_IOCTL_MODE_RMMODE          DRM_IOWR(0xA8, unsigned int)
+#define DRM_IOCTL_MODE_ATTACHMODE      DRM_IOWR(0xA9, struct drm_mode_mode_cmd)
+#define DRM_IOCTL_MODE_DETACHMODE      DRM_IOWR(0xAA, struct drm_mode_mode_cmd)
+
+#define DRM_IOCTL_MODE_GETPROPERTY     DRM_IOWR(0xAB, struct drm_mode_get_property)
+/*@}*/
+
 /**
  * Device specific ioctls should only be in their respective headers
  * The device specific ioctl range is from 0x40 to 0x99.
@@ -718,6 +1154,11 @@
 typedef struct drm_agp_info drm_agp_info_t;
 typedef struct drm_scatter_gather drm_scatter_gather_t;
 typedef struct drm_set_version drm_set_version_t;
+
+typedef struct drm_fence_arg drm_fence_arg_t;
+typedef struct drm_mm_type_arg drm_mm_type_arg_t;
+typedef struct drm_mm_init_arg drm_mm_init_arg_t;
+typedef enum drm_bo_type drm_bo_type_t;
 #endif
 
 #endif
Index: linux-2.6.27/include/drm/drmP.h
===================================================================
--- linux-2.6.27.orig/include/drm/drmP.h	2009-02-05 13:29:30.000000000 +0000
+++ linux-2.6.27/include/drm/drmP.h	2009-02-05 13:29:33.000000000 +0000
@@ -57,6 +57,7 @@
 #include <linux/dma-mapping.h>
 #include <linux/mm.h>
 #include <linux/cdev.h>
+#include <linux/i2c.h>
 #include <linux/mutex.h>
 #if defined(__alpha__) || defined(__powerpc__)
 #include <asm/pgtable.h>	/* For pte_wrprotect */
@@ -146,9 +147,24 @@
 #define DRM_MEM_CTXLIST   21
 #define DRM_MEM_MM        22
 #define DRM_MEM_HASHTAB   23
+#define DRM_MEM_OBJECTS   24
+#define DRM_MEM_FENCE     25
+#define DRM_MEM_TTM       26
+#define DRM_MEM_BUFOBJ    27
 
 #define DRM_MAX_CTXBITMAP (PAGE_SIZE * 8)
 #define DRM_MAP_HASH_OFFSET 0x10000000
+#define DRM_MAP_HASH_ORDER 12
+#define DRM_OBJECT_HASH_ORDER 12
+#define DRM_FILE_PAGE_OFFSET_START ((0xFFFFFFFFUL >> PAGE_SHIFT) + 1)
+#define DRM_FILE_PAGE_OFFSET_SIZE ((0xFFFFFFFFUL >> PAGE_SHIFT) * 16)
+/*
+ * This should be small enough to allow the use of kmalloc for hash tables
+ * instead of vmalloc.
+ */
+
+#define DRM_FILE_HASH_ORDER 8
+#define DRM_MM_INIT_MAX_PAGES 256
 
 /*@}*/
 
@@ -376,6 +392,14 @@
 	struct drm_freelist freelist;
 };
 
+
+enum drm_ref_type {
+	_DRM_REF_USE = 0,
+	_DRM_REF_TYPE1,
+	_DRM_NO_REF_TYPES
+};
+
+
 /** File private data */
 struct drm_file {
 	int authenticated;
@@ -388,12 +412,26 @@
 	struct drm_minor *minor;
 	int remove_auth_on_close;
 	unsigned long lock_count;
+
 	/** Mapping of mm object handles to object pointers. */
 	struct idr object_idr;
 	/** Lock for synchronization of access to object_idr. */
 	spinlock_t table_lock;
+
+	/*
+	 * The user object hash table is global and resides in the
+	 * drm_device structure. We protect the lists and hash tables with the
+	 * device struct_mutex. A bit coarse-grained but probably the best
+	 * option.
+	 */
+
+	struct list_head refd_objects;
+
+	struct drm_open_hash refd_object_hash[_DRM_NO_REF_TYPES];
 	struct file *filp;
 	void *driver_priv;
+
+	struct list_head fbs;
 };
 
 /** Wait queue */
@@ -523,6 +561,7 @@
 	struct drm_hash_item hash;
 	struct drm_map *map;			/**< mapping */
 	uint64_t user_token;
+	struct drm_mm_node *file_offset_node;
 };
 
 typedef struct drm_map drm_local_map_t;
@@ -612,6 +651,11 @@
 	void *driver_private;
 };
 
+
+#include "drm_objects.h"
+#include "drm_edid.h"
+#include "drm_crtc.h"
+
 /**
  * DRM driver structure. This structure represent the common code for
  * a family of cards. There will one drm_device for each card present
@@ -637,50 +681,8 @@
 	void (*kernel_context_switch_unlock) (struct drm_device *dev);
 	int (*dri_library_name) (struct drm_device *dev, char *buf);
 
-	/**
-	 * get_vblank_counter - get raw hardware vblank counter
-	 * @dev: DRM device
-	 * @crtc: counter to fetch
-	 *
-	 * Driver callback for fetching a raw hardware vblank counter
-	 * for @crtc.  If a device doesn't have a hardware counter, the
-	 * driver can simply return the value of drm_vblank_count and
-	 * make the enable_vblank() and disable_vblank() hooks into no-ops,
-	 * leaving interrupts enabled at all times.
-	 *
-	 * Wraparound handling and loss of events due to modesetting is dealt
-	 * with in the DRM core code.
-	 *
-	 * RETURNS
-	 * Raw vblank counter value.
-	 */
-	u32 (*get_vblank_counter) (struct drm_device *dev, int crtc);
-
-	/**
-	 * enable_vblank - enable vblank interrupt events
-	 * @dev: DRM device
-	 * @crtc: which irq to enable
-	 *
-	 * Enable vblank interrupts for @crtc.  If the device doesn't have
-	 * a hardware vblank counter, this routine should be a no-op, since
-	 * interrupts will have to stay on to keep the count accurate.
-	 *
-	 * RETURNS
-	 * Zero on success, appropriate errno if the given @crtc's vblank
-	 * interrupt cannot be enabled.
-	 */
-	int (*enable_vblank) (struct drm_device *dev, int crtc);
-
-	/**
-	 * disable_vblank - disable vblank interrupt events
-	 * @dev: DRM device
-	 * @crtc: which irq to enable
-	 *
-	 * Disable vblank interrupts for @crtc.  If the device doesn't have
-	 * a hardware vblank counter, this routine should be a no-op, since
-	 * interrupts will have to stay on to keep the count accurate.
-	 */
-	void (*disable_vblank) (struct drm_device *dev, int crtc);
+	int (*vblank_wait) (struct drm_device *dev, unsigned int *sequence);
+	int (*vblank_wait2) (struct drm_device *dev, unsigned int *sequence);
 
 	/**
 	 * Called by \c drm_device_is_agp.  Typically used to determine if a
@@ -715,6 +717,13 @@
 	int (*proc_init)(struct drm_minor *minor);
 	void (*proc_cleanup)(struct drm_minor *minor);
 
+	/* FB routines, if present */
+	int (*fb_probe)(struct drm_device *dev, struct drm_crtc *crtc);
+	int (*fb_remove)(struct drm_device *dev, struct drm_crtc *crtc);
+
+	struct drm_fence_driver *fence_driver;
+	struct drm_bo_driver *bo_driver;
+
 	/**
 	 * Driver-specific constructor for drm_gem_objects, to set up
 	 * obj->driver_private.
@@ -800,6 +809,10 @@
 	struct list_head maplist;	/**< Linked list of regions */
 	int map_count;			/**< Number of mappable regions */
 	struct drm_open_hash map_hash;	/**< User token hash table for maps */
+	struct drm_mm offset_manager;        /**< User token manager */
+	struct drm_open_hash object_hash;    /**< User token hash table for objects */
+	struct address_space *dev_mapping;  /**< For unmap_mapping_range() */
+	struct page *ttm_dummy_page;
 
 	/** \name Context handle management */
 	/*@{ */
@@ -848,20 +861,13 @@
 	 */
 	int vblank_disable_allowed;
 
-	wait_queue_head_t *vbl_queue;   /**< VBLANK wait queue */
-	atomic_t *_vblank_count;        /**< number of VBLANK interrupts (driver must alloc the right number of counters) */
+	wait_queue_head_t vbl_queue;	/**< VBLANK wait queue */
+	atomic_t vbl_received;
+	atomic_t vbl_received2;		/**< number of secondary VBLANK interrupts */
 	spinlock_t vbl_lock;
-	struct list_head *vbl_sigs;	/**< signal list to send on VBLANK */
-	atomic_t vbl_signal_pending;    /* number of signals pending on all crtcs*/
-	atomic_t *vblank_refcount;      /* number of users of vblank interruptsper crtc */
-	u32 *last_vblank;               /* protected by dev->vbl_lock, used */
-					/* for wraparound handling */
-	int *vblank_enabled;            /* so we don't call enable more than
-					   once per disable */
-	int *vblank_inmodeset;          /* Display driver is setting mode */
-	struct timer_list vblank_disable_timer;
-
-	u32 max_vblank_count;           /**< size of vblank counter register */
+	struct list_head vbl_sigs;		/**< signal list to send on VBLANK */
+	struct list_head vbl_sigs2;	/**< signals to send on secondary VBLANK */
+	unsigned int vbl_pending;
 	spinlock_t tasklet_lock;	/**< For drm_locked_tasklet */
 	void (*locked_tasklet_func)(struct drm_device *dev);
 
@@ -892,12 +898,18 @@
 	unsigned int agp_buffer_token;
 	struct drm_minor *primary;		/**< render type primary screen head */
 
+	struct drm_fence_manager fm;
+	struct drm_buffer_manager bm;
+
 	/** \name Drawable information */
 	/*@{ */
 	spinlock_t drw_lock;
 	struct idr drw_idr;
 	/*@} */
 
+	/* DRM mode setting */
+	struct drm_mode_config mode_config;
+
 	/** \name GEM information */
 	/*@{ */
 	spinlock_t object_name_lock;
@@ -915,6 +927,27 @@
 
 };
 
+#if __OS_HAS_AGP
+struct drm_agp_ttm_backend {
+	struct drm_ttm_backend backend;
+	DRM_AGP_MEM *mem;
+	struct agp_bridge_data *bridge;
+	int populated;
+};
+#endif
+
+typedef struct ati_pcigart_ttm_backend {
+	struct drm_ttm_backend backend;
+	int populated;
+	void (*gart_flush_fn)(struct drm_device *dev);
+	struct drm_ati_pcigart_info *gart_info;
+	unsigned long offset;
+	struct page **pages;
+	int num_pages;
+	int bound;
+	struct drm_device *dev;
+} ati_pcigart_ttm_backend_t;
+
 static __inline__ int drm_core_check_feature(struct drm_device *dev,
 					     int feature)
 {
@@ -979,8 +1012,12 @@
 /*@{*/
 
 				/* Driver support (drm_drv.h) */
-extern int drm_init(struct drm_driver *driver);
+extern int drm_init(struct drm_driver *driver,
+			      struct pci_device_id *pciidlist);
 extern void drm_exit(struct drm_driver *driver);
+extern void drm_cleanup_pci(struct pci_dev *pdev);
+extern void drm_vbl_send_signals(struct drm_device *dev);
+extern struct drm_ttm_backend *drm_agp_init_ttm(struct drm_device *dev);
 extern int drm_ioctl(struct inode *inode, struct file *filp,
 		     unsigned int cmd, unsigned long arg);
 extern long drm_compat_ioctl(struct file *filp,
Index: linux-2.6.27/include/drm/drm_pciids.h
===================================================================
--- linux-2.6.27.orig/include/drm/drm_pciids.h	2008-10-09 23:13:53.000000000 +0100
+++ linux-2.6.27/include/drm/drm_pciids.h	2009-02-05 13:29:33.000000000 +0000
@@ -413,3 +413,9 @@
 	{0x8086, 0x2e12, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0}, \
 	{0x8086, 0x2e22, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0}, \
 	{0, 0, 0}
+
+#define psb_PCI_IDS \
+	{0x8086, 0x8108, PCI_ANY_ID, PCI_ANY_ID, 0, 0, CHIP_PSB_8108}, \
+	{0x8086, 0x8109, PCI_ANY_ID, PCI_ANY_ID, 0, 0, CHIP_PSB_8109}, \
+	{0, 0, 0}
+
Index: linux-2.6.27/drivers/gpu/drm/Makefile
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/Makefile	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/Makefile	2009-02-05 13:29:33.000000000 +0000
@@ -9,11 +9,14 @@
 		drm_drv.o drm_fops.o drm_gem.o drm_ioctl.o drm_irq.o \
 		drm_lock.o drm_memory.o drm_proc.o drm_stub.o drm_vm.o \
 		drm_agpsupport.o drm_scatter.o ati_pcigart.o drm_pci.o \
-		drm_sysfs.o drm_hashtab.o drm_sman.o drm_mm.o
+		drm_sysfs.o drm_hashtab.o drm_sman.o drm_mm.o \
+		drm_fence.o drm_object.o drm_crtc.o drm_ttm.o drm_bo.o \
+		drm_bo_lock.o drm_bo_move.o drm_edid.o drm_modes.o drm_regman.o
 
 drm-$(CONFIG_COMPAT) += drm_ioc32.o
 
 obj-$(CONFIG_DRM)	+= drm.o
+obj-$(CONFIG_DRM_PSB)	+= psb/
 obj-$(CONFIG_DRM_TDFX)	+= tdfx/
 obj-$(CONFIG_DRM_R128)	+= r128/
 obj-$(CONFIG_DRM_RADEON)+= radeon/
@@ -24,4 +27,3 @@
 obj-$(CONFIG_DRM_SIS)   += sis/
 obj-$(CONFIG_DRM_SAVAGE)+= savage/
 obj-$(CONFIG_DRM_VIA)	+=via/
-
Index: linux-2.6.27/drivers/gpu/drm/drm_agpsupport.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_agpsupport.c	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_agpsupport.c	2009-02-05 13:29:33.000000000 +0000
@@ -453,47 +453,158 @@
 	return agp_unbind_memory(handle);
 }
 
-/**
- * Binds a collection of pages into AGP memory at the given offset, returning
- * the AGP memory structure containing them.
- *
- * No reference is held on the pages during this time -- it is up to the
- * caller to handle that.
+
+
+/*
+ * AGP ttm backend interface.
  */
-DRM_AGP_MEM *
-drm_agp_bind_pages(struct drm_device *dev,
-		   struct page **pages,
-		   unsigned long num_pages,
-		   uint32_t gtt_offset)
+
+#ifndef AGP_USER_TYPES
+#define AGP_USER_TYPES (1 << 16)
+#define AGP_USER_MEMORY (AGP_USER_TYPES)
+#define AGP_USER_CACHED_MEMORY (AGP_USER_TYPES + 1)
+#endif
+#define AGP_REQUIRED_MAJOR 0
+#define AGP_REQUIRED_MINOR 102
+
+static int drm_agp_needs_unbind_cache_adjust(struct drm_ttm_backend *backend)
 {
+	return ((backend->flags & DRM_BE_FLAG_BOUND_CACHED) ? 0 : 1);
+}
+
+
+static int drm_agp_populate(struct drm_ttm_backend *backend,
+			    unsigned long num_pages, struct page **pages)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+	struct page **cur_page, **last_page = pages + num_pages;
 	DRM_AGP_MEM *mem;
-	int ret, i;
 
-	DRM_DEBUG("\n");
+	DRM_DEBUG("drm_agp_populate_ttm\n");
+	mem = drm_agp_allocate_memory(agp_be->bridge, num_pages, AGP_USER_MEMORY);
+	if (!mem)
+		return -ENOMEM;
+
+	DRM_DEBUG("Current page count is %ld\n", (long) mem->page_count);
+	mem->page_count = 0;
+	for (cur_page = pages; cur_page < last_page; ++cur_page)
+		mem->memory[mem->page_count++] = phys_to_gart(page_to_phys(*cur_page));
+	agp_be->mem = mem;
+	return 0;
+}
+
+static int drm_agp_bind_ttm(struct drm_ttm_backend *backend,
+			    struct drm_bo_mem_reg *bo_mem)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+	DRM_AGP_MEM *mem = agp_be->mem;
+	int ret;
+	int snooped = (bo_mem->flags & DRM_BO_FLAG_CACHED) && !(bo_mem->flags & DRM_BO_FLAG_CACHED_MAPPED);
+
+	DRM_DEBUG("drm_agp_bind_ttm\n");
+	mem->is_flushed = 1;
+	mem->type = AGP_USER_MEMORY;
+	/* CACHED MAPPED implies not snooped memory */
+	if (snooped)
+		mem->type = AGP_USER_CACHED_MEMORY;
+
+	ret = drm_agp_bind_memory(mem, bo_mem->mm_node->start);
+	if (ret)
+		DRM_ERROR("AGP Bind memory failed\n");
+
+	DRM_FLAG_MASKED(backend->flags, (bo_mem->flags & DRM_BO_FLAG_CACHED) ?
+			DRM_BE_FLAG_BOUND_CACHED : 0,
+			DRM_BE_FLAG_BOUND_CACHED);
+	return ret;
+}
+
+static int drm_agp_unbind_ttm(struct drm_ttm_backend *backend)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+
+	DRM_DEBUG("drm_agp_unbind_ttm\n");
+	if (agp_be->mem->is_bound)
+		return drm_agp_unbind_memory(agp_be->mem);
+	else
+		return 0;
+}
+
+static void drm_agp_clear_ttm(struct drm_ttm_backend *backend)
+{
+	struct drm_agp_ttm_backend *agp_be =
+		container_of(backend, struct drm_agp_ttm_backend, backend);
+	DRM_AGP_MEM *mem = agp_be->mem;
+
+	DRM_DEBUG("drm_agp_clear_ttm\n");
+	if (mem) {
+		backend->func->unbind(backend);
+		agp_free_memory(mem);
+	}
+	agp_be->mem = NULL;
+}
+
+static void drm_agp_destroy_ttm(struct drm_ttm_backend *backend)
+{
+	struct drm_agp_ttm_backend *agp_be;
+
+	if (backend) {
+		DRM_DEBUG("drm_agp_destroy_ttm\n");
+		agp_be = container_of(backend, struct drm_agp_ttm_backend, backend);
+		if (agp_be && agp_be->mem)
+			backend->func->clear(backend);
+	}
+}
+
+static struct drm_ttm_backend_func agp_ttm_backend = {
+	.needs_ub_cache_adjust = drm_agp_needs_unbind_cache_adjust,
+	.populate = drm_agp_populate,
+	.clear = drm_agp_clear_ttm,
+	.bind = drm_agp_bind_ttm,
+	.unbind = drm_agp_unbind_ttm,
+	.destroy =  drm_agp_destroy_ttm,
+};
 
-	mem = drm_agp_allocate_memory(dev->agp->bridge, num_pages,
-				      AGP_USER_MEMORY);
-	if (mem == NULL) {
-		DRM_ERROR("Failed to allocate memory for %ld pages\n",
-			  num_pages);
+struct drm_ttm_backend *drm_agp_init_ttm(struct drm_device *dev)
+{
+
+	struct drm_agp_ttm_backend *agp_be;
+	struct agp_kern_info *info;
+
+	if (!dev->agp) {
+		DRM_ERROR("AGP is not initialized.\n");
 		return NULL;
 	}
+	info = &dev->agp->agp_info;
 
-	for (i = 0; i < num_pages; i++)
-		mem->memory[i] = phys_to_gart(page_to_phys(pages[i]));
-	mem->page_count = num_pages;
-
-	mem->is_flushed = true;
-	ret = drm_agp_bind_memory(mem, gtt_offset / PAGE_SIZE);
-	if (ret != 0) {
-		DRM_ERROR("Failed to bind AGP memory: %d\n", ret);
-		agp_free_memory(mem);
+	if (info->version.major != AGP_REQUIRED_MAJOR ||
+	    info->version.minor < AGP_REQUIRED_MINOR) {
+		DRM_ERROR("Wrong agpgart version %d.%d\n"
+			  "\tYou need at least version %d.%d.\n",
+			  info->version.major,
+			  info->version.minor,
+			  AGP_REQUIRED_MAJOR,
+			  AGP_REQUIRED_MINOR);
 		return NULL;
 	}
 
-	return mem;
+
+	agp_be = drm_calloc(1, sizeof(*agp_be), DRM_MEM_TTM);
+	if (!agp_be)
+		return NULL;
+
+	agp_be->mem = NULL;
+
+	agp_be->bridge = dev->agp->bridge;
+	agp_be->populated = 0;
+	agp_be->backend.func = &agp_ttm_backend;
+	agp_be->backend.dev = dev;
+
+	return &agp_be->backend;
 }
-EXPORT_SYMBOL(drm_agp_bind_pages);
+EXPORT_SYMBOL(drm_agp_init_ttm);
 
 void drm_agp_chipset_flush(struct drm_device *dev)
 {
Index: linux-2.6.27/drivers/gpu/drm/drm_bo.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_bo.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,2660 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+/*
+ * Locking may look a bit complicated but isn't really:
+ *
+ * The buffer usage atomic_t needs to be protected by dev->struct_mutex
+ * when there is a chance that it can be zero before or after the operation.
+ *
+ * dev->struct_mutex also protects all lists and list heads,
+ * Hash tables and hash heads.
+ *
+ * bo->mutex protects the buffer object itself excluding the usage field.
+ * bo->mutex does also protect the buffer list heads, so to manipulate those,
+ * we need both the bo->mutex and the dev->struct_mutex.
+ *
+ * Locking order is bo->mutex, dev->struct_mutex. Therefore list traversal
+ * is a bit complicated. When dev->struct_mutex is released to grab bo->mutex,
+ * the list traversal will, in general, need to be restarted.
+ *
+ */
+
+static void drm_bo_destroy_locked(struct drm_buffer_object *bo);
+static int drm_bo_setup_vm_locked(struct drm_buffer_object *bo);
+static void drm_bo_takedown_vm_locked(struct drm_buffer_object *bo);
+static void drm_bo_unmap_virtual(struct drm_buffer_object *bo);
+
+static inline uint64_t drm_bo_type_flags(unsigned type)
+{
+	return (1ULL << (24 + type));
+}
+
+/*
+ * bo locked. dev->struct_mutex locked.
+ */
+
+void drm_bo_add_to_pinned_lru(struct drm_buffer_object *bo)
+{
+	struct drm_mem_type_manager *man;
+
+	DRM_ASSERT_LOCKED(&bo->dev->struct_mutex);
+	DRM_ASSERT_LOCKED(&bo->mutex);
+
+	man = &bo->dev->bm.man[bo->pinned_mem_type];
+	list_add_tail(&bo->pinned_lru, &man->pinned);
+}
+
+void drm_bo_add_to_lru(struct drm_buffer_object *bo)
+{
+	struct drm_mem_type_manager *man;
+
+	DRM_ASSERT_LOCKED(&bo->dev->struct_mutex);
+
+	if (!(bo->mem.mask & (DRM_BO_FLAG_NO_MOVE | DRM_BO_FLAG_NO_EVICT))
+	    || bo->mem.mem_type != bo->pinned_mem_type) {
+		man = &bo->dev->bm.man[bo->mem.mem_type];
+		list_add_tail(&bo->lru, &man->lru);
+	} else {
+		INIT_LIST_HEAD(&bo->lru);
+	}
+}
+
+static int drm_bo_vm_pre_move(struct drm_buffer_object *bo, int old_is_pci)
+{
+#ifdef DRM_ODD_MM_COMPAT
+	int ret;
+
+	if (!bo->map_list.map)
+		return 0;
+
+	ret = drm_bo_lock_kmm(bo);
+	if (ret)
+		return ret;
+	drm_bo_unmap_virtual(bo);
+	if (old_is_pci)
+		drm_bo_finish_unmap(bo);
+#else
+	if (!bo->map_list.map)
+		return 0;
+
+	drm_bo_unmap_virtual(bo);
+#endif
+	return 0;
+}
+
+static void drm_bo_vm_post_move(struct drm_buffer_object *bo)
+{
+#ifdef DRM_ODD_MM_COMPAT
+	int ret;
+
+	if (!bo->map_list.map)
+		return;
+
+	ret = drm_bo_remap_bound(bo);
+	if (ret) {
+		DRM_ERROR("Failed to remap a bound buffer object.\n"
+			  "\tThis might cause a sigbus later.\n");
+	}
+	drm_bo_unlock_kmm(bo);
+#endif
+}
+
+/*
+ * Call bo->mutex locked.
+ */
+
+static int drm_bo_add_ttm(struct drm_buffer_object *bo)
+{
+	struct drm_device *dev = bo->dev;
+	int ret = 0;
+
+	DRM_ASSERT_LOCKED(&bo->mutex);
+	bo->ttm = NULL;
+
+	switch (bo->type) {
+	case drm_bo_type_dc:
+	case drm_bo_type_kernel:
+		bo->ttm = drm_ttm_init(dev, bo->num_pages << PAGE_SHIFT);
+		if (!bo->ttm)
+			ret = -ENOMEM;
+		break;
+	case drm_bo_type_user:
+		bo->ttm = drm_ttm_init(dev, bo->num_pages << PAGE_SHIFT);
+		if (!bo->ttm)
+			ret = -ENOMEM;
+
+		ret = drm_ttm_set_user(bo->ttm, current,
+				       bo->mem.mask & DRM_BO_FLAG_WRITE,
+				       bo->buffer_start,
+				       bo->num_pages,
+				       dev->bm.dummy_read_page);
+		if (ret)
+			return ret;
+
+		break;
+	default:
+		DRM_ERROR("Illegal buffer object type\n");
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+static int drm_bo_handle_move_mem(struct drm_buffer_object *bo,
+				  struct drm_bo_mem_reg *mem,
+				  int evict, int no_wait)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	int old_is_pci = drm_mem_reg_is_pci(dev, &bo->mem);
+	int new_is_pci = drm_mem_reg_is_pci(dev, mem);
+	struct drm_mem_type_manager *old_man = &bm->man[bo->mem.mem_type];
+	struct drm_mem_type_manager *new_man = &bm->man[mem->mem_type];
+	int ret = 0;
+
+	if (old_is_pci || new_is_pci ||
+	    ((mem->flags ^ bo->mem.flags) & DRM_BO_FLAG_CACHED))
+		ret = drm_bo_vm_pre_move(bo, old_is_pci);
+	if (ret)
+		return ret;
+
+	/*
+	 * Create and bind a ttm if required.
+	 */
+
+	if (!(new_man->flags & _DRM_FLAG_MEMTYPE_FIXED) && (bo->ttm == NULL)) {
+		ret = drm_bo_add_ttm(bo);
+		if (ret)
+			goto out_err;
+
+		if (mem->mem_type != DRM_BO_MEM_LOCAL) {
+			ret = drm_bind_ttm(bo->ttm, mem);
+			if (ret)
+				goto out_err;
+		}
+
+		if (bo->mem.mem_type == DRM_BO_MEM_LOCAL) {
+
+			struct drm_bo_mem_reg *old_mem = &bo->mem;
+			uint64_t save_flags = old_mem->flags;
+			uint64_t save_mask = old_mem->mask;
+
+			*old_mem = *mem;
+			mem->mm_node = NULL;
+			old_mem->mask = save_mask;
+			DRM_FLAG_MASKED(save_flags, mem->flags,
+					DRM_BO_MASK_MEMTYPE);
+			goto moved;
+		}
+
+	}
+
+	if (!(old_man->flags & _DRM_FLAG_MEMTYPE_FIXED) &&
+		   !(new_man->flags & _DRM_FLAG_MEMTYPE_FIXED)) {
+
+		ret = drm_bo_move_ttm(bo, evict, no_wait, mem);
+
+	} else if (dev->driver->bo_driver->move) {
+		ret = dev->driver->bo_driver->move(bo, evict, no_wait, mem);
+
+	} else {
+
+		ret = drm_bo_move_memcpy(bo, evict, no_wait, mem);
+
+	}
+
+	if (ret)
+		goto out_err;
+
+moved:
+	if (old_is_pci || new_is_pci)
+		drm_bo_vm_post_move(bo);
+
+	if (bo->priv_flags & _DRM_BO_FLAG_EVICTED) {
+		ret =
+		    dev->driver->bo_driver->invalidate_caches(dev,
+							      bo->mem.flags);
+		if (ret)
+			DRM_ERROR("Can not flush read caches\n");
+	}
+
+	DRM_FLAG_MASKED(bo->priv_flags,
+			(evict) ? _DRM_BO_FLAG_EVICTED : 0,
+			_DRM_BO_FLAG_EVICTED);
+
+	if (bo->mem.mm_node)
+		bo->offset = (bo->mem.mm_node->start << PAGE_SHIFT) +
+			bm->man[bo->mem.mem_type].gpu_offset;
+
+
+	return 0;
+
+out_err:
+	if (old_is_pci || new_is_pci)
+		drm_bo_vm_post_move(bo);
+
+	new_man = &bm->man[bo->mem.mem_type];
+	if ((new_man->flags & _DRM_FLAG_MEMTYPE_FIXED) && bo->ttm) {
+		drm_ttm_unbind(bo->ttm);
+		drm_destroy_ttm(bo->ttm);
+		bo->ttm = NULL;
+	}
+
+	return ret;
+}
+
+/*
+ * Call bo->mutex locked.
+ * Wait until the buffer is idle.
+ */
+
+int drm_bo_wait(struct drm_buffer_object *bo, int lazy, int ignore_signals,
+		int no_wait)
+{
+	int ret;
+
+	DRM_ASSERT_LOCKED(&bo->mutex);
+
+	if (bo->fence) {
+		if (drm_fence_object_signaled(bo->fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		if (no_wait)
+			return -EBUSY;
+
+		ret = drm_fence_object_wait(bo->fence, lazy, ignore_signals,
+					  bo->fence_type);
+		if (ret)
+			return ret;
+
+		drm_fence_usage_deref_unlocked(&bo->fence);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_wait);
+
+static int drm_bo_expire_fence(struct drm_buffer_object *bo, int allow_errors)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	if (bo->fence) {
+		if (bm->nice_mode) {
+			unsigned long _end = jiffies + 3 * DRM_HZ;
+			int ret;
+			do {
+				ret = drm_bo_wait(bo, 0, 1, 0);
+				if (ret && allow_errors)
+					return ret;
+
+			} while (ret && !time_after_eq(jiffies, _end));
+
+			if (bo->fence) {
+				bm->nice_mode = 0;
+				DRM_ERROR("Detected GPU lockup or "
+					  "fence driver was taken down. "
+					  "Evicting buffer.\n");
+			}
+		}
+		if (bo->fence)
+			drm_fence_usage_deref_unlocked(&bo->fence);
+	}
+	return 0;
+}
+
+/*
+ * Call dev->struct_mutex locked.
+ * Attempts to remove all private references to a buffer by expiring its
+ * fence object and removing from lru lists and memory managers.
+ */
+
+static void drm_bo_cleanup_refs(struct drm_buffer_object *bo, int remove_all)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	atomic_inc(&bo->usage);
+	mutex_unlock(&dev->struct_mutex);
+	mutex_lock(&bo->mutex);
+
+	DRM_FLAG_MASKED(bo->priv_flags, 0, _DRM_BO_FLAG_UNFENCED);
+
+	if (bo->fence && drm_fence_object_signaled(bo->fence,
+						   bo->fence_type))
+		drm_fence_usage_deref_unlocked(&bo->fence);
+
+	if (bo->fence && remove_all)
+		(void)drm_bo_expire_fence(bo, 0);
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (!atomic_dec_and_test(&bo->usage))
+		goto out;
+
+	if (!bo->fence) {
+		list_del_init(&bo->lru);
+		if (bo->mem.mm_node) {
+			drm_mm_put_block(bo->mem.mm_node);
+			if (bo->pinned_node == bo->mem.mm_node)
+				bo->pinned_node = NULL;
+			bo->mem.mm_node = NULL;
+		}
+		list_del_init(&bo->pinned_lru);
+		if (bo->pinned_node) {
+			drm_mm_put_block(bo->pinned_node);
+			bo->pinned_node = NULL;
+		}
+		list_del_init(&bo->ddestroy);
+		mutex_unlock(&bo->mutex);
+		drm_bo_destroy_locked(bo);
+		return;
+	}
+
+	if (list_empty(&bo->ddestroy)) {
+		drm_fence_object_flush(bo->fence, bo->fence_type);
+		list_add_tail(&bo->ddestroy, &bm->ddestroy);
+		schedule_delayed_work(&bm->wq,
+				      ((DRM_HZ / 100) < 1) ? 1 : DRM_HZ / 100);
+	}
+
+out:
+	mutex_unlock(&bo->mutex);
+	return;
+}
+
+static void drm_bo_unreserve_size(unsigned long size)
+{
+	//drm_free_memctl(size);
+}
+
+/*
+ * Verify that refcount is 0 and that there are no internal references
+ * to the buffer object. Then destroy it.
+ */
+
+static void drm_bo_destroy_locked(struct drm_buffer_object *bo)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	unsigned long reserved_size;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	if (list_empty(&bo->lru) && bo->mem.mm_node == NULL &&
+	    list_empty(&bo->pinned_lru) && bo->pinned_node == NULL &&
+	    list_empty(&bo->ddestroy) && atomic_read(&bo->usage) == 0) {
+		if (bo->fence != NULL) {
+			DRM_ERROR("Fence was non-zero.\n");
+			drm_bo_cleanup_refs(bo, 0);
+			return;
+		}
+
+#ifdef DRM_ODD_MM_COMPAT
+		BUG_ON(!list_empty(&bo->vma_list));
+		BUG_ON(!list_empty(&bo->p_mm_list));
+#endif
+
+		if (bo->ttm) {
+			drm_ttm_unbind(bo->ttm);
+			drm_destroy_ttm(bo->ttm);
+			bo->ttm = NULL;
+		}
+
+		atomic_dec(&bm->count);
+
+		reserved_size = bo->reserved_size;
+
+		drm_free(bo, sizeof(*bo), DRM_MEM_BUFOBJ);
+		drm_bo_unreserve_size(reserved_size);
+
+		return;
+	}
+
+	/*
+	 * Some stuff is still trying to reference the buffer object.
+	 * Get rid of those references.
+	 */
+
+	drm_bo_cleanup_refs(bo, 0);
+
+	return;
+}
+
+/*
+ * Call dev->struct_mutex locked.
+ */
+
+static void drm_bo_delayed_delete(struct drm_device *dev, int remove_all)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	struct drm_buffer_object *entry, *nentry;
+	struct list_head *list, *next;
+
+	list_for_each_safe(list, next, &bm->ddestroy) {
+		entry = list_entry(list, struct drm_buffer_object, ddestroy);
+
+		nentry = NULL;
+		if (next != &bm->ddestroy) {
+			nentry = list_entry(next, struct drm_buffer_object,
+					    ddestroy);
+			atomic_inc(&nentry->usage);
+		}
+
+		drm_bo_cleanup_refs(entry, remove_all);
+
+		if (nentry)
+			atomic_dec(&nentry->usage);
+	}
+}
+
+static void drm_bo_delayed_workqueue(struct work_struct *work)
+{
+	struct drm_buffer_manager *bm =
+	    container_of(work, struct drm_buffer_manager, wq.work);
+	struct drm_device *dev = container_of(bm, struct drm_device, bm);
+
+	DRM_DEBUG("Delayed delete Worker\n");
+
+	mutex_lock(&dev->struct_mutex);
+	if (!bm->initialized) {
+		mutex_unlock(&dev->struct_mutex);
+		return;
+	}
+	drm_bo_delayed_delete(dev, 0);
+	if (bm->initialized && !list_empty(&bm->ddestroy)) {
+		schedule_delayed_work(&bm->wq,
+				      ((DRM_HZ / 100) < 1) ? 1 : DRM_HZ / 100);
+	}
+	mutex_unlock(&dev->struct_mutex);
+}
+
+void drm_bo_usage_deref_locked(struct drm_buffer_object **bo)
+{
+	struct drm_buffer_object *tmp_bo = *bo;
+	bo = NULL;
+
+	DRM_ASSERT_LOCKED(&tmp_bo->dev->struct_mutex);
+
+	if (atomic_dec_and_test(&tmp_bo->usage))
+		drm_bo_destroy_locked(tmp_bo);
+}
+EXPORT_SYMBOL(drm_bo_usage_deref_locked);
+
+static void drm_bo_base_deref_locked(struct drm_file *file_priv,
+				     struct drm_user_object *uo)
+{
+	struct drm_buffer_object *bo =
+	    drm_user_object_entry(uo, struct drm_buffer_object, base);
+
+	DRM_ASSERT_LOCKED(&bo->dev->struct_mutex);
+
+	drm_bo_takedown_vm_locked(bo);
+	drm_bo_usage_deref_locked(&bo);
+}
+
+void drm_bo_usage_deref_unlocked(struct drm_buffer_object **bo)
+{
+	struct drm_buffer_object *tmp_bo = *bo;
+	struct drm_device *dev = tmp_bo->dev;
+
+	*bo = NULL;
+	if (atomic_dec_and_test(&tmp_bo->usage)) {
+		mutex_lock(&dev->struct_mutex);
+		if (atomic_read(&tmp_bo->usage) == 0)
+			drm_bo_destroy_locked(tmp_bo);
+		mutex_unlock(&dev->struct_mutex);
+	}
+}
+EXPORT_SYMBOL(drm_bo_usage_deref_unlocked);
+
+void drm_putback_buffer_objects(struct drm_device *dev)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct list_head *list = &bm->unfenced;
+	struct drm_buffer_object *entry, *next;
+
+	mutex_lock(&dev->struct_mutex);
+	list_for_each_entry_safe(entry, next, list, lru) {
+		atomic_inc(&entry->usage);
+		mutex_unlock(&dev->struct_mutex);
+
+		mutex_lock(&entry->mutex);
+		BUG_ON(!(entry->priv_flags & _DRM_BO_FLAG_UNFENCED));
+		mutex_lock(&dev->struct_mutex);
+
+		list_del_init(&entry->lru);
+		DRM_FLAG_MASKED(entry->priv_flags, 0, _DRM_BO_FLAG_UNFENCED);
+		wake_up_all(&entry->event_queue);
+
+		/*
+		 * FIXME: Might want to put back on head of list
+		 * instead of tail here.
+		 */
+
+		drm_bo_add_to_lru(entry);
+		mutex_unlock(&entry->mutex);
+		drm_bo_usage_deref_locked(&entry);
+	}
+	mutex_unlock(&dev->struct_mutex);
+}
+EXPORT_SYMBOL(drm_putback_buffer_objects);
+
+
+/*
+ * Note. The caller has to register (if applicable)
+ * and deregister fence object usage.
+ */
+
+int drm_fence_buffer_objects(struct drm_device *dev,
+			     struct list_head *list,
+			     uint32_t fence_flags,
+			     struct drm_fence_object *fence,
+			     struct drm_fence_object **used_fence)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_buffer_object *entry;
+	uint32_t fence_type = 0;
+	uint32_t fence_class = ~0;
+	int count = 0;
+	int ret = 0;
+	struct list_head *l;
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (!list)
+		list = &bm->unfenced;
+
+	if (fence)
+		fence_class = fence->fence_class;
+
+	list_for_each_entry(entry, list, lru) {
+		BUG_ON(!(entry->priv_flags & _DRM_BO_FLAG_UNFENCED));
+		fence_type |= entry->new_fence_type;
+		if (fence_class == ~0)
+			fence_class = entry->new_fence_class;
+		else if (entry->new_fence_class != fence_class) {
+			DRM_ERROR("Unmatching fence classes on unfenced list: "
+				  "%d and %d.\n",
+				  fence_class,
+				  entry->new_fence_class);
+			ret = -EINVAL;
+			goto out;
+		}
+		count++;
+	}
+
+	if (!count) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (fence) {
+		if ((fence_type & fence->type) != fence_type ||
+		    (fence->fence_class != fence_class)) {
+			DRM_ERROR("Given fence doesn't match buffers "
+				  "on unfenced list.\n");
+			ret = -EINVAL;
+			goto out;
+		}
+	} else {
+		mutex_unlock(&dev->struct_mutex);
+		ret = drm_fence_object_create(dev, fence_class, fence_type,
+					      fence_flags | DRM_FENCE_FLAG_EMIT,
+					      &fence);
+		mutex_lock(&dev->struct_mutex);
+		if (ret)
+			goto out;
+	}
+
+	count = 0;
+	l = list->next;
+	while (l != list) {
+		prefetch(l->next);
+		entry = list_entry(l, struct drm_buffer_object, lru);
+		atomic_inc(&entry->usage);
+		mutex_unlock(&dev->struct_mutex);
+		mutex_lock(&entry->mutex);
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(l);
+		if (entry->priv_flags & _DRM_BO_FLAG_UNFENCED) {
+			count++;
+			if (entry->fence)
+				drm_fence_usage_deref_locked(&entry->fence);
+			entry->fence = drm_fence_reference_locked(fence);
+			entry->fence_class = entry->new_fence_class;
+			entry->fence_type = entry->new_fence_type;
+			DRM_FLAG_MASKED(entry->priv_flags, 0,
+					_DRM_BO_FLAG_UNFENCED);
+			wake_up_all(&entry->event_queue);
+			drm_bo_add_to_lru(entry);
+		}
+		mutex_unlock(&entry->mutex);
+		drm_bo_usage_deref_locked(&entry);
+		l = list->next;
+	}
+	DRM_DEBUG("Fenced %d buffers\n", count);
+out:
+	mutex_unlock(&dev->struct_mutex);
+	*used_fence = fence;
+	return ret;
+}
+EXPORT_SYMBOL(drm_fence_buffer_objects);
+
+/*
+ * bo->mutex locked
+ */
+
+static int drm_bo_evict(struct drm_buffer_object *bo, unsigned mem_type,
+			int no_wait)
+{
+	int ret = 0;
+	struct drm_device *dev = bo->dev;
+	struct drm_bo_mem_reg evict_mem;
+
+	/*
+	 * Someone might have modified the buffer before we took the
+	 * buffer mutex.
+	 */
+
+	if (bo->priv_flags & _DRM_BO_FLAG_UNFENCED)
+		goto out;
+	if (bo->mem.mem_type != mem_type)
+		goto out;
+
+	ret = drm_bo_wait(bo, 0, 0, no_wait);
+
+	if (ret && ret != -EAGAIN) {
+		DRM_ERROR("Failed to expire fence before "
+			  "buffer eviction.\n");
+		goto out;
+	}
+
+	evict_mem = bo->mem;
+	evict_mem.mm_node = NULL;
+
+	evict_mem = bo->mem;
+	evict_mem.mask = dev->driver->bo_driver->evict_mask(bo);
+	ret = drm_bo_mem_space(bo, &evict_mem, no_wait);
+
+	if (ret) {
+		if (ret != -EAGAIN)
+			DRM_ERROR("Failed to find memory space for "
+				  "buffer 0x%p eviction.\n", bo);
+		goto out;
+	}
+
+	ret = drm_bo_handle_move_mem(bo, &evict_mem, 1, no_wait);
+
+	if (ret) {
+		if (ret != -EAGAIN)
+			DRM_ERROR("Buffer eviction failed\n");
+		goto out;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	if (evict_mem.mm_node) {
+		if (evict_mem.mm_node != bo->pinned_node)
+			drm_mm_put_block(evict_mem.mm_node);
+		evict_mem.mm_node = NULL;
+	}
+	list_del(&bo->lru);
+	drm_bo_add_to_lru(bo);
+	mutex_unlock(&dev->struct_mutex);
+
+	DRM_FLAG_MASKED(bo->priv_flags, _DRM_BO_FLAG_EVICTED,
+			_DRM_BO_FLAG_EVICTED);
+
+out:
+	return ret;
+}
+
+/**
+ * Repeatedly evict memory from the LRU for @mem_type until we create enough
+ * space, or we've evicted everything and there isn't enough space.
+ */
+static int drm_bo_mem_force_space(struct drm_device *dev,
+				  struct drm_bo_mem_reg *mem,
+				  uint32_t mem_type, int no_wait)
+{
+	struct drm_mm_node *node;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_buffer_object *entry;
+	struct drm_mem_type_manager *man = &bm->man[mem_type];
+	struct list_head *lru;
+	unsigned long num_pages = mem->num_pages;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	do {
+		node = drm_mm_search_free(&man->manager, num_pages,
+					  mem->page_alignment, 1);
+		if (node)
+			break;
+
+		lru = &man->lru;
+		if (lru->next == lru)
+			break;
+
+		entry = list_entry(lru->next, struct drm_buffer_object, lru);
+		atomic_inc(&entry->usage);
+		mutex_unlock(&dev->struct_mutex);
+		mutex_lock(&entry->mutex);
+		BUG_ON(entry->mem.flags & (DRM_BO_FLAG_NO_MOVE | DRM_BO_FLAG_NO_EVICT));
+
+		ret = drm_bo_evict(entry, mem_type, no_wait);
+		mutex_unlock(&entry->mutex);
+		drm_bo_usage_deref_unlocked(&entry);
+		if (ret)
+			return ret;
+		mutex_lock(&dev->struct_mutex);
+	} while (1);
+
+	if (!node) {
+		mutex_unlock(&dev->struct_mutex);
+		return -ENOMEM;
+	}
+
+	node = drm_mm_get_block(node, num_pages, mem->page_alignment);
+	if (!node) {
+		mutex_unlock(&dev->struct_mutex);
+		return -ENOMEM;
+	}
+
+	mutex_unlock(&dev->struct_mutex);
+	mem->mm_node = node;
+	mem->mem_type = mem_type;
+	return 0;
+}
+
+static int drm_bo_mt_compatible(struct drm_mem_type_manager *man,
+				int disallow_fixed,
+				uint32_t mem_type,
+				uint64_t mask, uint32_t *res_mask)
+{
+	uint64_t cur_flags = drm_bo_type_flags(mem_type);
+	uint64_t flag_diff;
+
+	if ((man->flags & _DRM_FLAG_MEMTYPE_FIXED) && disallow_fixed)
+		return 0;
+	if (man->flags & _DRM_FLAG_MEMTYPE_CACHED)
+		cur_flags |= DRM_BO_FLAG_CACHED;
+	if (man->flags & _DRM_FLAG_MEMTYPE_MAPPABLE)
+		cur_flags |= DRM_BO_FLAG_MAPPABLE;
+	if (man->flags & _DRM_FLAG_MEMTYPE_CSELECT)
+		DRM_FLAG_MASKED(cur_flags, mask, DRM_BO_FLAG_CACHED);
+
+	if ((cur_flags & mask & DRM_BO_MASK_MEM) == 0)
+		return 0;
+
+	if (mem_type == DRM_BO_MEM_LOCAL) {
+		*res_mask = cur_flags;
+		return 1;
+	}
+
+	flag_diff = (mask ^ cur_flags);
+	if (flag_diff & DRM_BO_FLAG_CACHED_MAPPED)
+		cur_flags |= DRM_BO_FLAG_CACHED_MAPPED;
+
+	if ((flag_diff & DRM_BO_FLAG_CACHED) &&
+	    (!(mask & DRM_BO_FLAG_CACHED) ||
+	     (mask & DRM_BO_FLAG_FORCE_CACHING)))
+		return 0;
+
+	if ((flag_diff & DRM_BO_FLAG_MAPPABLE) &&
+	    ((mask & DRM_BO_FLAG_MAPPABLE) ||
+	     (mask & DRM_BO_FLAG_FORCE_MAPPABLE)))
+		return 0;
+
+	*res_mask = cur_flags;
+	return 1;
+}
+
+/**
+ * Creates space for memory region @mem according to its type.
+ *
+ * This function first searches for free space in compatible memory types in
+ * the priority order defined by the driver.  If free space isn't found, then
+ * drm_bo_mem_force_space is attempted in priority order to evict and find
+ * space.
+ */
+int drm_bo_mem_space(struct drm_buffer_object *bo,
+		     struct drm_bo_mem_reg *mem, int no_wait)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man;
+
+	uint32_t num_prios = dev->driver->bo_driver->num_mem_type_prio;
+	const uint32_t *prios = dev->driver->bo_driver->mem_type_prio;
+	uint32_t i;
+	uint32_t mem_type = DRM_BO_MEM_LOCAL;
+	uint32_t cur_flags;
+	int type_found = 0;
+	int type_ok = 0;
+	int has_eagain = 0;
+	struct drm_mm_node *node = NULL;
+	int ret;
+
+	mem->mm_node = NULL;
+	for (i = 0; i < num_prios; ++i) {
+		mem_type = prios[i];
+		man = &bm->man[mem_type];
+
+		type_ok = drm_bo_mt_compatible(man,
+					       bo->type == drm_bo_type_user,
+					       mem_type, mem->mask,
+					       &cur_flags);
+
+		if (!type_ok)
+			continue;
+
+		if (mem_type == DRM_BO_MEM_LOCAL)
+			break;
+
+		if ((mem_type == bo->pinned_mem_type) &&
+		    (bo->pinned_node != NULL)) {
+			node = bo->pinned_node;
+			break;
+		}
+
+		mutex_lock(&dev->struct_mutex);
+		if (man->has_type && man->use_type) {
+			type_found = 1;
+			node = drm_mm_search_free(&man->manager, mem->num_pages,
+						  mem->page_alignment, 1);
+			if (node)
+				node = drm_mm_get_block(node, mem->num_pages,
+							mem->page_alignment);
+		}
+		mutex_unlock(&dev->struct_mutex);
+		if (node)
+			break;
+	}
+
+	if ((type_ok && (mem_type == DRM_BO_MEM_LOCAL)) || node) {
+		mem->mm_node = node;
+		mem->mem_type = mem_type;
+		mem->flags = cur_flags;
+		return 0;
+	}
+
+	if (!type_found)
+		return -EINVAL;
+
+	num_prios = dev->driver->bo_driver->num_mem_busy_prio;
+	prios = dev->driver->bo_driver->mem_busy_prio;
+
+	for (i = 0; i < num_prios; ++i) {
+		mem_type = prios[i];
+		man = &bm->man[mem_type];
+
+		if (!man->has_type)
+			continue;
+
+		if (!drm_bo_mt_compatible(man,
+					  bo->type == drm_bo_type_user,
+					  mem_type,
+					  mem->mask,
+					  &cur_flags))
+			continue;
+
+		ret = drm_bo_mem_force_space(dev, mem, mem_type, no_wait);
+
+		if (ret == 0 && mem->mm_node) {
+			mem->flags = cur_flags;
+			return 0;
+		}
+
+		if (ret == -EAGAIN)
+			has_eagain = 1;
+	}
+
+	ret = (has_eagain) ? -EAGAIN : -ENOMEM;
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_mem_space);
+
+static int drm_bo_new_mask(struct drm_buffer_object *bo,
+			   uint64_t new_flags, uint64_t used_mask)
+{
+	uint32_t new_props;
+
+	if (bo->type == drm_bo_type_user &&
+	    ((new_flags & (DRM_BO_FLAG_CACHED | DRM_BO_FLAG_FORCE_CACHING)) !=
+	     (DRM_BO_FLAG_CACHED | DRM_BO_FLAG_FORCE_CACHING))) {
+		DRM_ERROR("User buffers require cache-coherent memory.\n");
+		return -EINVAL;
+	}
+
+	if ((used_mask & DRM_BO_FLAG_NO_EVICT) && !DRM_SUSER(DRM_CURPROC)) {
+		DRM_ERROR("DRM_BO_FLAG_NO_EVICT is only available to priviliged processes.\n");
+		return -EPERM;
+	}
+
+	if (likely(used_mask & DRM_BO_MASK_MEM) &&
+	    (bo->mem.flags & DRM_BO_FLAG_NO_EVICT) &&
+	    !DRM_SUSER(DRM_CURPROC)) {
+		if (likely(bo->mem.flags & new_flags & used_mask &
+			   DRM_BO_MASK_MEM))
+			new_flags = (new_flags & ~DRM_BO_MASK_MEM) |
+				(bo->mem.flags & DRM_BO_MASK_MEM);
+		else {
+			DRM_ERROR("Incompatible memory type specification "
+				  "for NO_EVICT buffer.\n");
+			return -EPERM;
+		}
+	}
+
+	if ((new_flags & DRM_BO_FLAG_NO_MOVE)) {
+		DRM_ERROR("DRM_BO_FLAG_NO_MOVE is not properly implemented yet.\n");
+		return -EPERM;
+	}
+
+	new_props = new_flags & (DRM_BO_FLAG_EXE | DRM_BO_FLAG_WRITE |
+				 DRM_BO_FLAG_READ);
+
+	if (!new_props) {
+		DRM_ERROR("Invalid buffer object rwx properties\n");
+		return -EINVAL;
+	}
+
+	bo->mem.mask = new_flags;
+	return 0;
+}
+
+/*
+ * Call dev->struct_mutex locked.
+ */
+
+struct drm_buffer_object *drm_lookup_buffer_object(struct drm_file *file_priv,
+					      uint32_t handle, int check_owner)
+{
+	struct drm_user_object *uo;
+	struct drm_buffer_object *bo;
+
+	uo = drm_lookup_user_object(file_priv, handle);
+
+	if (!uo || (uo->type != drm_buffer_type)) {
+		DRM_ERROR("Could not find buffer object 0x%08x\n", handle);
+		return NULL;
+	}
+
+	if (check_owner && file_priv != uo->owner) {
+		if (!drm_lookup_ref_object(file_priv, uo, _DRM_REF_USE))
+			return NULL;
+	}
+
+	bo = drm_user_object_entry(uo, struct drm_buffer_object, base);
+	atomic_inc(&bo->usage);
+	return bo;
+}
+EXPORT_SYMBOL(drm_lookup_buffer_object);
+
+/*
+ * Call bo->mutex locked.
+ * Returns 1 if the buffer is currently rendered to or from. 0 otherwise.
+ * Doesn't do any fence flushing as opposed to the drm_bo_busy function.
+ */
+
+static int drm_bo_quick_busy(struct drm_buffer_object *bo)
+{
+	struct drm_fence_object *fence = bo->fence;
+
+	BUG_ON(bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (fence) {
+		if (drm_fence_object_signaled(fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		return 1;
+	}
+	return 0;
+}
+
+/*
+ * Call bo->mutex locked.
+ * Returns 1 if the buffer is currently rendered to or from. 0 otherwise.
+ */
+
+static int drm_bo_busy(struct drm_buffer_object *bo)
+{
+	struct drm_fence_object *fence = bo->fence;
+
+	BUG_ON(bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (fence) {
+		if (drm_fence_object_signaled(fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		drm_fence_object_flush(fence, DRM_FENCE_TYPE_EXE);
+		if (drm_fence_object_signaled(fence, bo->fence_type)) {
+			drm_fence_usage_deref_unlocked(&bo->fence);
+			return 0;
+		}
+		return 1;
+	}
+	return 0;
+}
+
+static int drm_bo_evict_cached(struct drm_buffer_object *bo)
+{
+	int ret = 0;
+
+	BUG_ON(bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (bo->mem.mm_node)
+		ret = drm_bo_evict(bo, DRM_BO_MEM_TT, 1);
+	return ret;
+}
+
+/*
+ * Wait until a buffer is unmapped.
+ */
+
+static int drm_bo_wait_unmapped(struct drm_buffer_object *bo, int no_wait)
+{
+	int ret = 0;
+
+	if ((atomic_read(&bo->mapped) >= 0) && no_wait)
+		return -EBUSY;
+
+	DRM_WAIT_ON(ret, bo->event_queue, 3 * DRM_HZ,
+		    atomic_read(&bo->mapped) == -1);
+
+	if (ret == -EINTR)
+		ret = -EAGAIN;
+
+	return ret;
+}
+
+static int drm_bo_check_unfenced(struct drm_buffer_object *bo)
+{
+	int ret;
+
+	mutex_lock(&bo->mutex);
+	ret = (bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	mutex_unlock(&bo->mutex);
+	return ret;
+}
+
+/*
+ * Wait until a buffer, scheduled to be fenced moves off the unfenced list.
+ * Until then, we cannot really do anything with it except delete it.
+ */
+
+static int drm_bo_wait_unfenced(struct drm_buffer_object *bo, int no_wait,
+				int eagain_if_wait)
+{
+	int ret = (bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+
+	if (ret && no_wait)
+		return -EBUSY;
+	else if (!ret)
+		return 0;
+
+	ret = 0;
+	mutex_unlock(&bo->mutex);
+	DRM_WAIT_ON(ret, bo->event_queue, 3 * DRM_HZ,
+		    !drm_bo_check_unfenced(bo));
+	mutex_lock(&bo->mutex);
+	if (ret == -EINTR)
+		return -EAGAIN;
+	ret = (bo->priv_flags & _DRM_BO_FLAG_UNFENCED);
+	if (ret) {
+		DRM_ERROR("Timeout waiting for buffer to become fenced\n");
+		return -EBUSY;
+	}
+	if (eagain_if_wait)
+		return -EAGAIN;
+
+	return 0;
+}
+
+/*
+ * Fill in the ioctl reply argument with buffer info.
+ * Bo locked.
+ */
+
+void drm_bo_fill_rep_arg(struct drm_buffer_object *bo,
+			 struct drm_bo_info_rep *rep)
+{
+	if (!rep)
+		return;
+
+	rep->handle = bo->base.hash.key;
+	rep->flags = bo->mem.flags;
+	rep->size = bo->num_pages * PAGE_SIZE;
+	rep->offset = bo->offset;
+
+	if (bo->type == drm_bo_type_dc)
+		rep->arg_handle = bo->map_list.user_token;
+	else
+		rep->arg_handle = 0;
+
+	rep->mask = bo->mem.mask;
+	rep->buffer_start = bo->buffer_start;
+	rep->fence_flags = bo->fence_type;
+	rep->rep_flags = 0;
+	rep->page_alignment = bo->mem.page_alignment;
+
+	if ((bo->priv_flags & _DRM_BO_FLAG_UNFENCED) || drm_bo_quick_busy(bo)) {
+		DRM_FLAG_MASKED(rep->rep_flags, DRM_BO_REP_BUSY,
+				DRM_BO_REP_BUSY);
+	}
+}
+EXPORT_SYMBOL(drm_bo_fill_rep_arg);
+
+/*
+ * Wait for buffer idle and register that we've mapped the buffer.
+ * Mapping is registered as a drm_ref_object with type _DRM_REF_TYPE1,
+ * so that if the client dies, the mapping is automatically
+ * unregistered.
+ */
+
+static int drm_buffer_object_map(struct drm_file *file_priv, uint32_t handle,
+				 uint32_t map_flags, unsigned hint,
+				 struct drm_bo_info_rep *rep)
+{
+	struct drm_buffer_object *bo;
+	struct drm_device *dev = file_priv->minor->dev;
+	int ret = 0;
+	int no_wait = hint & DRM_BO_HINT_DONT_BLOCK;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	mutex_lock(&bo->mutex);
+	ret = drm_bo_wait_unfenced(bo, no_wait, 0);
+	if (ret)
+		goto out;
+
+	/*
+	 * If this returns true, we are currently unmapped.
+	 * We need to do this test, because unmapping can
+	 * be done without the bo->mutex held.
+	 */
+
+	while (1) {
+		if (atomic_inc_and_test(&bo->mapped)) {
+			if (no_wait && drm_bo_busy(bo)) {
+				atomic_dec(&bo->mapped);
+				ret = -EBUSY;
+				goto out;
+			}
+			ret = drm_bo_wait(bo, 0, 0, no_wait);
+			if (ret) {
+				atomic_dec(&bo->mapped);
+				goto out;
+			}
+
+			if (bo->mem.flags & DRM_BO_FLAG_CACHED_MAPPED)
+				drm_bo_evict_cached(bo);
+
+			break;
+		} else if (bo->mem.flags & DRM_BO_FLAG_CACHED_MAPPED) {
+
+			/*
+			 * We are already mapped with different flags.
+			 * need to wait for unmap.
+			 */
+
+			ret = drm_bo_wait_unmapped(bo, no_wait);
+			if (ret)
+				goto out;
+
+			continue;
+		}
+		break;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_ref_object(file_priv, &bo->base, _DRM_REF_TYPE1);
+	mutex_unlock(&dev->struct_mutex);
+	if (ret) {
+		if (atomic_add_negative(-1, &bo->mapped))
+			wake_up_all(&bo->event_queue);
+
+	} else
+		drm_bo_fill_rep_arg(bo, rep);
+out:
+	mutex_unlock(&bo->mutex);
+	drm_bo_usage_deref_unlocked(&bo);
+	return ret;
+}
+
+static int drm_buffer_object_unmap(struct drm_file *file_priv, uint32_t handle)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+	struct drm_ref_object *ro;
+	int ret = 0;
+
+	mutex_lock(&dev->struct_mutex);
+
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	if (!bo) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ro = drm_lookup_ref_object(file_priv, &bo->base, _DRM_REF_TYPE1);
+	if (!ro) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	drm_remove_ref_object(file_priv, ro);
+	drm_bo_usage_deref_locked(&bo);
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/*
+ * Call struct-sem locked.
+ */
+
+static void drm_buffer_user_object_unmap(struct drm_file *file_priv,
+					 struct drm_user_object *uo,
+					 enum drm_ref_type action)
+{
+	struct drm_buffer_object *bo =
+	    drm_user_object_entry(uo, struct drm_buffer_object, base);
+
+	/*
+	 * We DON'T want to take the bo->lock here, because we want to
+	 * hold it when we wait for unmapped buffer.
+	 */
+
+	BUG_ON(action != _DRM_REF_TYPE1);
+
+	if (atomic_add_negative(-1, &bo->mapped))
+		wake_up_all(&bo->event_queue);
+}
+
+/*
+ * bo->mutex locked.
+ * Note that new_mem_flags are NOT transferred to the bo->mem.mask.
+ */
+
+int drm_bo_move_buffer(struct drm_buffer_object *bo, uint64_t new_mem_flags,
+		       int no_wait, int move_unfenced)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = 0;
+	struct drm_bo_mem_reg mem;
+	/*
+	 * Flush outstanding fences.
+	 */
+
+	drm_bo_busy(bo);
+
+	/*
+	 * Wait for outstanding fences.
+	 */
+
+	ret = drm_bo_wait(bo, 0, 0, no_wait);
+	if (ret)
+		return ret;
+
+	mem.num_pages = bo->num_pages;
+	mem.size = mem.num_pages << PAGE_SHIFT;
+	mem.mask = new_mem_flags;
+	mem.page_alignment = bo->mem.page_alignment;
+
+	mutex_lock(&bm->evict_mutex);
+	mutex_lock(&dev->struct_mutex);
+	list_del_init(&bo->lru);
+	mutex_unlock(&dev->struct_mutex);
+
+	/*
+	 * Determine where to move the buffer.
+	 */
+	ret = drm_bo_mem_space(bo, &mem, no_wait);
+	if (ret)
+		goto out_unlock;
+
+	ret = drm_bo_handle_move_mem(bo, &mem, 0, no_wait);
+
+out_unlock:
+	mutex_lock(&dev->struct_mutex);
+	if (ret || !move_unfenced) {
+		if (mem.mm_node) {
+			if (mem.mm_node != bo->pinned_node)
+				drm_mm_put_block(mem.mm_node);
+			mem.mm_node = NULL;
+		}
+		drm_bo_add_to_lru(bo);
+		if (bo->priv_flags & _DRM_BO_FLAG_UNFENCED) {
+			wake_up_all(&bo->event_queue);
+			DRM_FLAG_MASKED(bo->priv_flags, 0,
+					_DRM_BO_FLAG_UNFENCED);
+		}
+	} else {
+		list_add_tail(&bo->lru, &bm->unfenced);
+		DRM_FLAG_MASKED(bo->priv_flags, _DRM_BO_FLAG_UNFENCED,
+				_DRM_BO_FLAG_UNFENCED);
+	}
+	mutex_unlock(&dev->struct_mutex);
+	mutex_unlock(&bm->evict_mutex);
+	return ret;
+}
+
+static int drm_bo_mem_compat(struct drm_bo_mem_reg *mem)
+{
+	uint32_t flag_diff = (mem->mask ^ mem->flags);
+
+	if ((mem->mask & mem->flags & DRM_BO_MASK_MEM) == 0)
+		return 0;
+	if ((flag_diff & DRM_BO_FLAG_CACHED) &&
+	    (/* !(mem->mask & DRM_BO_FLAG_CACHED) ||*/
+	     (mem->mask & DRM_BO_FLAG_FORCE_CACHING)))
+		return 0;
+
+	if ((flag_diff & DRM_BO_FLAG_MAPPABLE) &&
+	    ((mem->mask & DRM_BO_FLAG_MAPPABLE) ||
+	     (mem->mask & DRM_BO_FLAG_FORCE_MAPPABLE)))
+		return 0;
+	return 1;
+}
+
+/*
+ * bo locked.
+ */
+
+static int drm_buffer_object_validate(struct drm_buffer_object *bo,
+				      uint32_t fence_class,
+				      int move_unfenced, int no_wait)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	uint32_t ftype;
+	int ret;
+
+	DRM_DEBUG("New flags 0x%016llx, Old flags 0x%016llx\n",
+		  (unsigned long long) bo->mem.mask,
+		  (unsigned long long) bo->mem.flags);
+
+	ret = driver->fence_type(bo, &fence_class, &ftype);
+
+	if (ret) {
+		DRM_ERROR("Driver did not support given buffer permissions\n");
+		return ret;
+	}
+
+	/*
+	 * We're switching command submission mechanism,
+	 * or cannot simply rely on the hardware serializing for us.
+	 *
+	 * Insert a driver-dependant barrier or wait for buffer idle.
+	 */
+
+	if ((fence_class != bo->fence_class) ||
+	    ((ftype ^ bo->fence_type) & bo->fence_type)) {
+
+		ret = -EINVAL;
+		if (driver->command_stream_barrier) {
+			ret = driver->command_stream_barrier(bo,
+							     fence_class,
+							     ftype,
+							     no_wait);
+		}
+		if (ret)
+			ret = drm_bo_wait(bo, 0, 0, no_wait);
+
+		if (ret)
+			return ret;
+
+	}
+
+	bo->new_fence_class = fence_class;
+	bo->new_fence_type = ftype;
+
+	ret = drm_bo_wait_unmapped(bo, no_wait);
+	if (ret) {
+		DRM_ERROR("Timed out waiting for buffer unmap.\n");
+		return ret;
+	}
+
+	/*
+	 * Check whether we need to move buffer.
+	 */
+
+	if (!drm_bo_mem_compat(&bo->mem)) {
+		ret = drm_bo_move_buffer(bo, bo->mem.mask, no_wait,
+					 move_unfenced);
+		if (ret) {
+			if (ret != -EAGAIN)
+				DRM_ERROR("Failed moving buffer.\n");
+			if (ret == -ENOMEM)
+				DRM_ERROR("Out of aperture space.\n");
+			return ret;
+		}
+	}
+
+	/*
+	 * Pinned buffers.
+	 */
+
+	if (bo->mem.mask & (DRM_BO_FLAG_NO_EVICT | DRM_BO_FLAG_NO_MOVE)) {
+		bo->pinned_mem_type = bo->mem.mem_type;
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(&bo->pinned_lru);
+		drm_bo_add_to_pinned_lru(bo);
+
+		if (bo->pinned_node != bo->mem.mm_node) {
+			if (bo->pinned_node != NULL)
+				drm_mm_put_block(bo->pinned_node);
+			bo->pinned_node = bo->mem.mm_node;
+		}
+
+		mutex_unlock(&dev->struct_mutex);
+
+	} else if (bo->pinned_node != NULL) {
+
+		mutex_lock(&dev->struct_mutex);
+
+		if (bo->pinned_node != bo->mem.mm_node)
+			drm_mm_put_block(bo->pinned_node);
+
+		list_del_init(&bo->pinned_lru);
+		bo->pinned_node = NULL;
+		mutex_unlock(&dev->struct_mutex);
+
+	}
+
+	/*
+	 * We might need to add a TTM.
+	 */
+
+	if (bo->mem.mem_type == DRM_BO_MEM_LOCAL && bo->ttm == NULL) {
+		ret = drm_bo_add_ttm(bo);
+		if (ret)
+			return ret;
+	}
+	DRM_FLAG_MASKED(bo->mem.flags, bo->mem.mask, ~DRM_BO_MASK_MEMTYPE);
+
+	/*
+	 * Finally, adjust lru to be sure.
+	 */
+
+	mutex_lock(&dev->struct_mutex);
+	list_del(&bo->lru);
+	if (move_unfenced) {
+		list_add_tail(&bo->lru, &bm->unfenced);
+		DRM_FLAG_MASKED(bo->priv_flags, _DRM_BO_FLAG_UNFENCED,
+				_DRM_BO_FLAG_UNFENCED);
+	} else {
+		drm_bo_add_to_lru(bo);
+		if (bo->priv_flags & _DRM_BO_FLAG_UNFENCED) {
+			wake_up_all(&bo->event_queue);
+			DRM_FLAG_MASKED(bo->priv_flags, 0,
+					_DRM_BO_FLAG_UNFENCED);
+		}
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
+
+int drm_bo_do_validate(struct drm_buffer_object *bo,
+		       uint64_t flags, uint64_t mask, uint32_t hint,
+		       uint32_t fence_class,
+		       int no_wait,
+		       struct drm_bo_info_rep *rep)
+{
+	int ret;
+
+	mutex_lock(&bo->mutex);
+	ret = drm_bo_wait_unfenced(bo, no_wait, 0);
+
+	if (ret)
+		goto out;
+
+	DRM_FLAG_MASKED(flags, bo->mem.mask, ~mask);
+	ret = drm_bo_new_mask(bo, flags, mask);
+	if (ret)
+		goto out;
+
+	ret = drm_buffer_object_validate(bo,
+					 fence_class,
+					 !(hint & DRM_BO_HINT_DONT_FENCE),
+					 no_wait);
+out:
+	if (rep)
+		drm_bo_fill_rep_arg(bo, rep);
+
+	mutex_unlock(&bo->mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_do_validate);
+
+
+int drm_bo_handle_validate(struct drm_file *file_priv, uint32_t handle,
+			   uint32_t fence_class,
+			   uint64_t flags, uint64_t mask,
+			   uint32_t hint,
+			   int use_old_fence_class,
+			   struct drm_bo_info_rep *rep,
+			   struct drm_buffer_object **bo_rep)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+	int ret;
+	int no_wait = hint & DRM_BO_HINT_DONT_BLOCK;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	if (use_old_fence_class)
+		fence_class = bo->fence_class;
+
+	/*
+	 * Only allow creator to change shared buffer mask.
+	 */
+
+	if (bo->base.owner != file_priv)
+		mask &= ~(DRM_BO_FLAG_NO_EVICT | DRM_BO_FLAG_NO_MOVE);
+
+
+	ret = drm_bo_do_validate(bo, flags, mask, hint, fence_class,
+				 no_wait, rep);
+
+	if (!ret && bo_rep)
+		*bo_rep = bo;
+	else
+		drm_bo_usage_deref_unlocked(&bo);
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_handle_validate);
+
+static int drm_bo_handle_info(struct drm_file *file_priv, uint32_t handle,
+			      struct drm_bo_info_rep *rep)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	mutex_lock(&bo->mutex);
+	if (!(bo->priv_flags & _DRM_BO_FLAG_UNFENCED))
+		(void)drm_bo_busy(bo);
+	drm_bo_fill_rep_arg(bo, rep);
+	mutex_unlock(&bo->mutex);
+	drm_bo_usage_deref_unlocked(&bo);
+	return 0;
+}
+
+static int drm_bo_handle_wait(struct drm_file *file_priv, uint32_t handle,
+			      uint32_t hint,
+			      struct drm_bo_info_rep *rep)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_buffer_object *bo;
+	int no_wait = hint & DRM_BO_HINT_DONT_BLOCK;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	bo = drm_lookup_buffer_object(file_priv, handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!bo)
+		return -EINVAL;
+
+	mutex_lock(&bo->mutex);
+	ret = drm_bo_wait_unfenced(bo, no_wait, 0);
+	if (ret)
+		goto out;
+	ret = drm_bo_wait(bo, hint & DRM_BO_HINT_WAIT_LAZY, 0, no_wait);
+	if (ret)
+		goto out;
+
+	drm_bo_fill_rep_arg(bo, rep);
+
+out:
+	mutex_unlock(&bo->mutex);
+	drm_bo_usage_deref_unlocked(&bo);
+	return ret;
+}
+
+static inline size_t drm_size_align(size_t size)
+{
+	size_t tmpSize = 4;
+	if (size > PAGE_SIZE)
+		return PAGE_ALIGN(size);
+	while (tmpSize < size)
+		tmpSize <<= 1;
+
+	return (size_t) tmpSize;
+}
+
+static int drm_bo_reserve_size(struct drm_device *dev,
+			       int user_bo,
+			       unsigned long num_pages,
+			       unsigned long *size)
+{
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+
+	*size = drm_size_align(sizeof(struct drm_buffer_object)) +
+		/* Always account for a TTM, even for fixed memory types */
+		drm_ttm_size(dev, num_pages, user_bo) +
+		/* user space mapping structure */
+		drm_size_align(sizeof(drm_local_map_t)) +
+		/* file offset space, aperture space, pinned space */
+		3*drm_size_align(sizeof(struct drm_mm_node *)) +
+		/* ttm backend */
+		driver->backend_size(dev, num_pages);
+
+	// FIXME - ENOMEM?
+	return 0;
+}
+
+int drm_buffer_object_create(struct drm_device *dev,
+			     unsigned long size,
+			     enum drm_bo_type type,
+			     uint64_t mask,
+			     uint32_t hint,
+			     uint32_t page_alignment,
+			     unsigned long buffer_start,
+			     struct drm_buffer_object **buf_obj)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_buffer_object *bo;
+	int ret = 0;
+	unsigned long num_pages;
+	unsigned long reserved_size;
+
+	size += buffer_start & ~PAGE_MASK;
+	num_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	if (num_pages == 0) {
+		DRM_ERROR("Illegal buffer object size.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_reserve_size(dev, type == drm_bo_type_user,
+				  num_pages, &reserved_size);
+
+	if (ret) {
+		DRM_DEBUG("Failed reserving space for buffer object.\n");
+		return ret;
+	}
+
+	bo = drm_calloc(1, sizeof(*bo), DRM_MEM_BUFOBJ);
+
+	if (!bo) {
+		drm_bo_unreserve_size(num_pages);
+		return -ENOMEM;
+	}
+
+	mutex_init(&bo->mutex);
+	mutex_lock(&bo->mutex);
+
+	bo->reserved_size = reserved_size;
+	atomic_set(&bo->usage, 1);
+	atomic_set(&bo->mapped, -1);
+	DRM_INIT_WAITQUEUE(&bo->event_queue);
+	INIT_LIST_HEAD(&bo->lru);
+	INIT_LIST_HEAD(&bo->pinned_lru);
+	INIT_LIST_HEAD(&bo->ddestroy);
+#ifdef DRM_ODD_MM_COMPAT
+	INIT_LIST_HEAD(&bo->p_mm_list);
+	INIT_LIST_HEAD(&bo->vma_list);
+#endif
+	bo->dev = dev;
+	bo->type = type;
+	bo->num_pages = num_pages;
+	bo->mem.mem_type = DRM_BO_MEM_LOCAL;
+	bo->mem.num_pages = bo->num_pages;
+	bo->mem.mm_node = NULL;
+	bo->mem.page_alignment = page_alignment;
+	bo->buffer_start = buffer_start & PAGE_MASK;
+	bo->priv_flags = 0;
+	bo->mem.flags = DRM_BO_FLAG_MEM_LOCAL | DRM_BO_FLAG_CACHED |
+		DRM_BO_FLAG_MAPPABLE;
+	bo->mem.mask = DRM_BO_FLAG_MEM_LOCAL | DRM_BO_FLAG_CACHED |
+		DRM_BO_FLAG_MAPPABLE;
+	atomic_inc(&bm->count);
+	ret = drm_bo_new_mask(bo, mask, mask);
+	if (ret)
+		goto out_err;
+
+	if (bo->type == drm_bo_type_dc) {
+		mutex_lock(&dev->struct_mutex);
+		ret = drm_bo_setup_vm_locked(bo);
+		mutex_unlock(&dev->struct_mutex);
+		if (ret)
+			goto out_err;
+	}
+
+	ret = drm_buffer_object_validate(bo, 0, 0, hint & DRM_BO_HINT_DONT_BLOCK);
+	if (ret)
+		goto out_err;
+
+	mutex_unlock(&bo->mutex);
+	*buf_obj = bo;
+	return 0;
+
+out_err:
+	mutex_unlock(&bo->mutex);
+
+	drm_bo_usage_deref_unlocked(&bo);
+	return ret;
+}
+EXPORT_SYMBOL(drm_buffer_object_create);
+
+
+static int drm_bo_add_user_object(struct drm_file *file_priv,
+				  struct drm_buffer_object *bo, int shareable)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_user_object(file_priv, &bo->base, shareable);
+	if (ret)
+		goto out;
+
+	bo->base.remove = drm_bo_base_deref_locked;
+	bo->base.type = drm_buffer_type;
+	bo->base.ref_struct_locked = NULL;
+	bo->base.unref = drm_buffer_user_object_unmap;
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+int drm_bo_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_create_arg *arg = data;
+	struct drm_bo_create_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	struct drm_buffer_object *entry;
+	enum drm_bo_type bo_type;
+	int ret = 0;
+
+	DRM_DEBUG("drm_bo_create_ioctl: %dkb, %dkb align\n",
+	    (int)(req->size / 1024), req->page_alignment * 4);
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	bo_type = (req->buffer_start) ? drm_bo_type_user : drm_bo_type_dc;
+
+	if (bo_type == drm_bo_type_user)
+		req->mask &= ~DRM_BO_FLAG_SHAREABLE;
+
+	ret = drm_buffer_object_create(file_priv->minor->dev,
+				       req->size, bo_type, req->mask,
+				       req->hint, req->page_alignment,
+				       req->buffer_start, &entry);
+	if (ret)
+		goto out;
+
+	ret = drm_bo_add_user_object(file_priv, entry,
+				     req->mask & DRM_BO_FLAG_SHAREABLE);
+	if (ret) {
+		drm_bo_usage_deref_unlocked(&entry);
+		goto out;
+	}
+
+	mutex_lock(&entry->mutex);
+	drm_bo_fill_rep_arg(entry, rep);
+	mutex_unlock(&entry->mutex);
+
+out:
+	return ret;
+}
+
+int drm_bo_setstatus_ioctl(struct drm_device *dev,
+			   void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_map_wait_idle_arg *arg = data;
+	struct drm_bo_info_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_read_lock(&dev->bm.bm_lock);
+	if (ret)
+		return ret;
+
+	ret = drm_bo_handle_validate(file_priv, req->handle, req->fence_class,
+				     req->flags,
+				     req->mask,
+				     req->hint | DRM_BO_HINT_DONT_FENCE,
+				     1,
+				     rep, NULL);
+
+	(void) drm_bo_read_unlock(&dev->bm.bm_lock);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_map_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_map_wait_idle_arg *arg = data;
+	struct drm_bo_info_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_buffer_object_map(file_priv, req->handle, req->mask,
+				    req->hint, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_unmap_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_handle_arg *arg = data;
+	int ret;
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_buffer_object_unmap(file_priv, arg->handle);
+	return ret;
+}
+
+
+int drm_bo_reference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_reference_info_arg *arg = data;
+	struct drm_bo_handle_arg *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	struct drm_user_object *uo;
+	int ret;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_user_object_ref(file_priv, req->handle,
+				  drm_buffer_type, &uo);
+	if (ret)
+		return ret;
+
+	ret = drm_bo_handle_info(file_priv, req->handle, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_unreference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_handle_arg *arg = data;
+	int ret = 0;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_user_object_unref(file_priv, arg->handle, drm_buffer_type);
+	return ret;
+}
+
+int drm_bo_info_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_reference_info_arg *arg = data;
+	struct drm_bo_handle_arg *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_handle_info(file_priv, req->handle, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_bo_wait_idle_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_bo_map_wait_idle_arg *arg = data;
+	struct drm_bo_info_req *req = &arg->d.req;
+	struct drm_bo_info_rep *rep = &arg->d.rep;
+	int ret;
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_handle_wait(file_priv, req->handle,
+				 req->hint, rep);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int drm_bo_leave_list(struct drm_buffer_object *bo,
+			     uint32_t mem_type,
+			     int free_pinned,
+			     int allow_errors)
+{
+	struct drm_device *dev = bo->dev;
+	int ret = 0;
+
+	mutex_lock(&bo->mutex);
+
+	ret = drm_bo_expire_fence(bo, allow_errors);
+	if (ret)
+		goto out;
+
+	if (free_pinned) {
+		DRM_FLAG_MASKED(bo->mem.flags, 0, DRM_BO_FLAG_NO_MOVE);
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(&bo->pinned_lru);
+		if (bo->pinned_node == bo->mem.mm_node)
+			bo->pinned_node = NULL;
+		if (bo->pinned_node != NULL) {
+			drm_mm_put_block(bo->pinned_node);
+			bo->pinned_node = NULL;
+		}
+		mutex_unlock(&dev->struct_mutex);
+	}
+
+	if (bo->mem.flags & DRM_BO_FLAG_NO_EVICT) {
+		DRM_ERROR("A DRM_BO_NO_EVICT buffer present at "
+			  "cleanup. Removing flag and evicting.\n");
+		bo->mem.flags &= ~DRM_BO_FLAG_NO_EVICT;
+		bo->mem.mask &= ~DRM_BO_FLAG_NO_EVICT;
+	}
+
+	if (bo->mem.mem_type == mem_type)
+		ret = drm_bo_evict(bo, mem_type, 0);
+
+	if (ret) {
+		if (allow_errors) {
+			goto out;
+		} else {
+			ret = 0;
+			DRM_ERROR("Cleanup eviction failed\n");
+		}
+	}
+
+out:
+	mutex_unlock(&bo->mutex);
+	return ret;
+}
+
+
+static struct drm_buffer_object *drm_bo_entry(struct list_head *list,
+					 int pinned_list)
+{
+	if (pinned_list)
+		return list_entry(list, struct drm_buffer_object, pinned_lru);
+	else
+		return list_entry(list, struct drm_buffer_object, lru);
+}
+
+/*
+ * dev->struct_mutex locked.
+ */
+
+static int drm_bo_force_list_clean(struct drm_device *dev,
+				   struct list_head *head,
+				   unsigned mem_type,
+				   int free_pinned,
+				   int allow_errors,
+				   int pinned_list)
+{
+	struct list_head *list, *next, *prev;
+	struct drm_buffer_object *entry, *nentry;
+	int ret;
+	int do_restart;
+
+	/*
+	 * The list traversal is a bit odd here, because an item may
+	 * disappear from the list when we release the struct_mutex or
+	 * when we decrease the usage count. Also we're not guaranteed
+	 * to drain pinned lists, so we can't always restart.
+	 */
+
+restart:
+	nentry = NULL;
+	list_for_each_safe(list, next, head) {
+		prev = list->prev;
+
+		entry = (nentry != NULL) ? nentry: drm_bo_entry(list, pinned_list);
+		atomic_inc(&entry->usage);
+		if (nentry) {
+			atomic_dec(&nentry->usage);
+			nentry = NULL;
+		}
+
+		/*
+		 * Protect the next item from destruction, so we can check
+		 * its list pointers later on.
+		 */
+
+		if (next != head) {
+			nentry = drm_bo_entry(next, pinned_list);
+			atomic_inc(&nentry->usage);
+		}
+		mutex_unlock(&dev->struct_mutex);
+
+		ret = drm_bo_leave_list(entry, mem_type, free_pinned,
+					allow_errors);
+		mutex_lock(&dev->struct_mutex);
+
+		drm_bo_usage_deref_locked(&entry);
+		if (ret)
+			return ret;
+
+		/*
+		 * Has the next item disappeared from the list?
+		 */
+
+		do_restart = ((next->prev != list) && (next->prev != prev));
+
+		if (nentry != NULL && do_restart)
+			drm_bo_usage_deref_locked(&nentry);
+
+		if (do_restart)
+			goto restart;
+	}
+	return 0;
+}
+
+int drm_bo_clean_mm(struct drm_device *dev, unsigned mem_type)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem_type];
+	int ret = -EINVAL;
+
+	if (mem_type >= DRM_BO_MEM_TYPES) {
+		DRM_ERROR("Illegal memory type %d\n", mem_type);
+		return ret;
+	}
+
+	if (!man->has_type) {
+		DRM_ERROR("Trying to take down uninitialized "
+			  "memory manager type %u\n", mem_type);
+		return ret;
+	}
+	man->use_type = 0;
+	man->has_type = 0;
+
+	ret = 0;
+	if (mem_type > 0) {
+		BUG_ON(!list_empty(&bm->unfenced));
+		drm_bo_force_list_clean(dev, &man->lru, mem_type, 1, 0, 0);
+		drm_bo_force_list_clean(dev, &man->pinned, mem_type, 1, 0, 1);
+
+		if (drm_mm_clean(&man->manager)) {
+			drm_mm_takedown(&man->manager);
+		} else {
+			ret = -EBUSY;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_clean_mm);
+
+/**
+ *Evict all buffers of a particular mem_type, but leave memory manager
+ *regions for NO_MOVE buffers intact. New buffers cannot be added at this
+ *point since we have the hardware lock.
+ */
+
+static int drm_bo_lock_mm(struct drm_device *dev, unsigned mem_type)
+{
+	int ret;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem_type];
+
+	if (mem_type == 0 || mem_type >= DRM_BO_MEM_TYPES) {
+		DRM_ERROR("Illegal memory manager memory type %u.\n", mem_type);
+		return -EINVAL;
+	}
+
+	if (!man->has_type) {
+		DRM_ERROR("Memory type %u has not been initialized.\n",
+			  mem_type);
+		return 0;
+	}
+
+	ret = drm_bo_force_list_clean(dev, &man->lru, mem_type, 0, 1, 0);
+	if (ret)
+		return ret;
+	ret = drm_bo_force_list_clean(dev, &man->pinned, mem_type, 0, 1, 1);
+
+	return ret;
+}
+
+int drm_bo_init_mm(struct drm_device *dev,
+		   unsigned type,
+		   unsigned long p_offset, unsigned long p_size)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = -EINVAL;
+	struct drm_mem_type_manager *man;
+
+	if (type >= DRM_BO_MEM_TYPES) {
+		DRM_ERROR("Illegal memory type %d\n", type);
+		return ret;
+	}
+
+	man = &bm->man[type];
+	if (man->has_type) {
+		DRM_ERROR("Memory manager already initialized for type %d\n",
+			  type);
+		return ret;
+	}
+
+	ret = dev->driver->bo_driver->init_mem_type(dev, type, man);
+	if (ret)
+		return ret;
+
+	ret = 0;
+	if (type != DRM_BO_MEM_LOCAL) {
+		if (!p_size) {
+			DRM_ERROR("Zero size memory manager type %d\n", type);
+			return ret;
+		}
+		ret = drm_mm_init(&man->manager, p_offset, p_size);
+		if (ret)
+			return ret;
+	}
+	man->has_type = 1;
+	man->use_type = 1;
+
+	INIT_LIST_HEAD(&man->lru);
+	INIT_LIST_HEAD(&man->pinned);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_init_mm);
+
+/*
+ * This function is intended to be called on drm driver unload.
+ * If you decide to call it from lastclose, you must protect the call
+ * from a potentially racing drm_bo_driver_init in firstopen.
+ * (This may happen on X server restart).
+ */
+
+int drm_bo_driver_finish(struct drm_device *dev)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = 0;
+	unsigned i = DRM_BO_MEM_TYPES;
+	struct drm_mem_type_manager *man;
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (!bm->initialized)
+		goto out;
+	bm->initialized = 0;
+
+	while (i--) {
+		man = &bm->man[i];
+		if (man->has_type) {
+			man->use_type = 0;
+			if ((i != DRM_BO_MEM_LOCAL) && drm_bo_clean_mm(dev, i)) {
+				ret = -EBUSY;
+				DRM_ERROR("DRM memory manager type %d "
+					  "is not clean.\n", i);
+			}
+			man->has_type = 0;
+		}
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	if (!cancel_delayed_work(&bm->wq))
+		flush_scheduled_work();
+
+	mutex_lock(&dev->struct_mutex);
+	drm_bo_delayed_delete(dev, 1);
+	if (list_empty(&bm->ddestroy))
+		DRM_DEBUG("Delayed destroy list was clean\n");
+
+	if (list_empty(&bm->man[0].lru))
+		DRM_DEBUG("Swap list was clean\n");
+
+	if (list_empty(&bm->man[0].pinned))
+		DRM_DEBUG("NO_MOVE list was clean\n");
+
+	if (list_empty(&bm->unfenced))
+		DRM_DEBUG("Unfenced list was clean\n");
+
+	__free_page(bm->dummy_read_page);
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_driver_finish);
+
+/*
+ * This function is intended to be called on drm driver load.
+ * If you decide to call it from firstopen, you must protect the call
+ * from a potentially racing drm_bo_driver_finish in lastclose.
+ * (This may happen on X server restart).
+ */
+
+int drm_bo_driver_init(struct drm_device *dev)
+{
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	struct drm_buffer_manager *bm = &dev->bm;
+	int ret = -EINVAL;
+
+	bm->dummy_read_page = NULL;
+	drm_bo_init_lock(&bm->bm_lock);
+	mutex_lock(&dev->struct_mutex);
+	if (!driver)
+		goto out_unlock;
+
+	bm->dummy_read_page = alloc_page(__GFP_ZERO | GFP_DMA32);
+	if (!bm->dummy_read_page) {
+		ret = -ENOMEM;
+		goto out_unlock;
+	}
+
+
+	/*
+	 * Initialize the system memory buffer type.
+	 * Other types need to be driver / IOCTL initialized.
+	 */
+	ret = drm_bo_init_mm(dev, DRM_BO_MEM_LOCAL, 0, 0);
+	if (ret)
+		goto out_unlock;
+
+	INIT_DELAYED_WORK(&bm->wq, drm_bo_delayed_workqueue);
+
+	bm->initialized = 1;
+	bm->nice_mode = 1;
+	atomic_set(&bm->count, 0);
+	bm->cur_pages = 0;
+	INIT_LIST_HEAD(&bm->unfenced);
+	INIT_LIST_HEAD(&bm->ddestroy);
+out_unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_driver_init);
+
+int drm_mm_init_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_mm_init_arg *arg = data;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_write_lock(&bm->bm_lock, file_priv);
+	if (ret)
+		return ret;
+
+	ret = -EINVAL;
+	if (arg->magic != DRM_BO_INIT_MAGIC) {
+		DRM_ERROR("You are using an old libdrm that is not compatible with\n"
+			  "\tthe kernel DRM module. Please upgrade your libdrm.\n");
+		return -EINVAL;
+	}
+	if (arg->major != DRM_BO_INIT_MAJOR) {
+		DRM_ERROR("libdrm and kernel DRM buffer object interface major\n"
+			  "\tversion don't match. Got %d, expected %d.\n",
+			  arg->major, DRM_BO_INIT_MAJOR);
+		return -EINVAL;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	if (!bm->initialized) {
+		DRM_ERROR("DRM memory manager was not initialized.\n");
+		goto out;
+	}
+	if (arg->mem_type == 0) {
+		DRM_ERROR("System memory buffers already initialized.\n");
+		goto out;
+	}
+	ret = drm_bo_init_mm(dev, arg->mem_type,
+			     arg->p_offset, arg->p_size);
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+	(void) drm_bo_write_unlock(&bm->bm_lock, file_priv);
+
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_mm_takedown_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_mm_type_arg *arg = data;
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_write_lock(&bm->bm_lock, file_priv);
+	if (ret)
+		return ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = -EINVAL;
+	if (!bm->initialized) {
+		DRM_ERROR("DRM memory manager was not initialized\n");
+		goto out;
+	}
+	if (arg->mem_type == 0) {
+		DRM_ERROR("No takedown for System memory buffers.\n");
+		goto out;
+	}
+	ret = 0;
+	if (drm_bo_clean_mm(dev, arg->mem_type)) {
+		DRM_ERROR("Memory manager type %d not clean. "
+			  "Delaying takedown\n", arg->mem_type);
+	}
+out:
+	mutex_unlock(&dev->struct_mutex);
+	(void) drm_bo_write_unlock(&bm->bm_lock, file_priv);
+
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+int drm_mm_lock_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	struct drm_mm_type_arg *arg = data;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	if (arg->lock_flags & DRM_BO_LOCK_IGNORE_NO_EVICT) {
+		DRM_ERROR("Lock flag DRM_BO_LOCK_IGNORE_NO_EVICT not supported yet.\n");
+		return -EINVAL;
+	}
+
+	if (arg->lock_flags & DRM_BO_LOCK_UNLOCK_BM) {
+		ret = drm_bo_write_lock(&dev->bm.bm_lock, file_priv);
+		if (ret)
+			return ret;
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_bo_lock_mm(dev, arg->mem_type);
+	mutex_unlock(&dev->struct_mutex);
+	if (ret) {
+		(void) drm_bo_write_unlock(&dev->bm.bm_lock, file_priv);
+		return ret;
+	}
+
+	return 0;
+}
+
+int drm_mm_unlock_ioctl(struct drm_device *dev,
+			void *data,
+			struct drm_file *file_priv)
+{
+	struct drm_mm_type_arg *arg = data;
+	struct drm_bo_driver *driver = dev->driver->bo_driver;
+	int ret;
+
+	if (!driver) {
+		DRM_ERROR("Buffer objects are not supported by this driver\n");
+		return -EINVAL;
+	}
+
+	if (arg->lock_flags & DRM_BO_LOCK_UNLOCK_BM) {
+		ret = drm_bo_write_unlock(&dev->bm.bm_lock, file_priv);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+/*
+ * buffer object vm functions.
+ */
+
+int drm_mem_reg_is_pci(struct drm_device *dev, struct drm_bo_mem_reg *mem)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem->mem_type];
+
+	if (!(man->flags & _DRM_FLAG_MEMTYPE_FIXED)) {
+		if (mem->mem_type == DRM_BO_MEM_LOCAL)
+			return 0;
+
+		if (man->flags & _DRM_FLAG_MEMTYPE_CMA)
+			return 0;
+
+		if (mem->flags & DRM_BO_FLAG_CACHED)
+			return 0;
+	}
+	return 1;
+}
+EXPORT_SYMBOL(drm_mem_reg_is_pci);
+
+/**
+ * \c Get the PCI offset for the buffer object memory.
+ *
+ * \param bo The buffer object.
+ * \param bus_base On return the base of the PCI region
+ * \param bus_offset On return the byte offset into the PCI region
+ * \param bus_size On return the byte size of the buffer object or zero if
+ *     the buffer object memory is not accessible through a PCI region.
+ * \return Failure indication.
+ *
+ * Returns -EINVAL if the buffer object is currently not mappable.
+ * Otherwise returns zero.
+ */
+
+int drm_bo_pci_offset(struct drm_device *dev,
+		      struct drm_bo_mem_reg *mem,
+		      unsigned long *bus_base,
+		      unsigned long *bus_offset, unsigned long *bus_size)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem->mem_type];
+
+	*bus_size = 0;
+	if (!(man->flags & _DRM_FLAG_MEMTYPE_MAPPABLE))
+		return -EINVAL;
+
+	if (drm_mem_reg_is_pci(dev, mem)) {
+		*bus_offset = mem->mm_node->start << PAGE_SHIFT;
+		*bus_size = mem->num_pages << PAGE_SHIFT;
+		*bus_base = man->io_offset;
+	}
+
+	return 0;
+}
+
+/**
+ * \c Kill all user-space virtual mappings of this buffer object.
+ *
+ * \param bo The buffer object.
+ *
+ * Call bo->mutex locked.
+ */
+
+void drm_bo_unmap_virtual(struct drm_buffer_object *bo)
+{
+	struct drm_device *dev = bo->dev;
+	loff_t offset = ((loff_t) bo->map_list.hash.key) << PAGE_SHIFT;
+	loff_t holelen = ((loff_t) bo->mem.num_pages) << PAGE_SHIFT;
+
+	if (!dev->dev_mapping)
+		return;
+
+	unmap_mapping_range(dev->dev_mapping, offset, holelen, 1);
+}
+
+static void drm_bo_takedown_vm_locked(struct drm_buffer_object *bo)
+{
+	struct drm_map_list *list;
+	drm_local_map_t *map;
+	struct drm_device *dev = bo->dev;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	if (bo->type != drm_bo_type_dc)
+		return;
+
+	list = &bo->map_list;
+	if (list->user_token) {
+		drm_ht_remove_item(&dev->map_hash, &list->hash);
+		list->user_token = 0;
+	}
+	if (list->file_offset_node) {
+		drm_mm_put_block(list->file_offset_node);
+		list->file_offset_node = NULL;
+	}
+
+	map = list->map;
+	if (!map)
+		return;
+
+	drm_free(map, sizeof(*map), DRM_MEM_BUFOBJ);
+	list->map = NULL;
+	list->user_token = 0ULL;
+	drm_bo_usage_deref_locked(&bo);
+}
+
+static int drm_bo_setup_vm_locked(struct drm_buffer_object *bo)
+{
+	struct drm_map_list *list = &bo->map_list;
+	drm_local_map_t *map;
+	struct drm_device *dev = bo->dev;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	list->map = drm_calloc(1, sizeof(*map), DRM_MEM_BUFOBJ);
+	if (!list->map)
+		return -ENOMEM;
+
+	map = list->map;
+	map->offset = 0;
+	map->type = _DRM_TTM;
+	map->flags = _DRM_REMOVABLE;
+	map->size = bo->mem.num_pages * PAGE_SIZE;
+	atomic_inc(&bo->usage);
+	map->handle = (void *)bo;
+
+	list->file_offset_node = drm_mm_search_free(&dev->offset_manager,
+						    bo->mem.num_pages, 0, 0);
+
+	if (!list->file_offset_node) {
+		drm_bo_takedown_vm_locked(bo);
+		return -ENOMEM;
+	}
+
+	list->file_offset_node = drm_mm_get_block(list->file_offset_node,
+						  bo->mem.num_pages, 0);
+	if (!list->file_offset_node) {
+		drm_bo_takedown_vm_locked(bo);
+		return -ENOMEM;
+	}
+
+	list->hash.key = list->file_offset_node->start;
+	if (drm_ht_insert_item(&dev->map_hash, &list->hash)) {
+		drm_bo_takedown_vm_locked(bo);
+		return -ENOMEM;
+	}
+
+	list->user_token = ((uint64_t) list->hash.key) << PAGE_SHIFT;
+
+	return 0;
+}
+
+int drm_bo_version_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv)
+{
+	struct drm_bo_version_arg *arg = (struct drm_bo_version_arg *)data;
+
+	arg->major = DRM_BO_INIT_MAJOR;
+	arg->minor = DRM_BO_INIT_MINOR;
+	arg->patchlevel = DRM_BO_INIT_PATCH;
+
+	return 0;
+}
Index: linux-2.6.27/drivers/gpu/drm/drm_bo_lock.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_bo_lock.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,175 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+/*
+ * This file implements a simple replacement for the buffer manager use
+ * of the heavyweight hardware lock.
+ * The lock is a read-write lock. Taking it in read mode is fast, and
+ * intended for in-kernel use only.
+ * Taking it in write mode is slow.
+ *
+ * The write mode is used only when there is a need to block all
+ * user-space processes from allocating a
+ * new memory area.
+ * Typical use in write mode is X server VT switching, and it's allowed
+ * to leave kernel space with the write lock held. If a user-space process
+ * dies while having the write-lock, it will be released during the file
+ * descriptor release.
+ *
+ * The read lock is typically placed at the start of an IOCTL- or
+ * user-space callable function that may end up allocating a memory area.
+ * This includes setstatus, super-ioctls and no_pfn; the latter may move
+ * unmappable regions to mappable. It's a bug to leave kernel space with the
+ * read lock held.
+ *
+ * Both read- and write lock taking is interruptible for low signal-delivery
+ * latency. The locking functions will return -EAGAIN if interrupted by a
+ * signal.
+ *
+ * Locking order: The lock should be taken BEFORE any kernel mutexes
+ * or spinlocks.
+ */
+
+#include "drmP.h"
+
+void drm_bo_init_lock(struct drm_bo_lock *lock)
+{
+	DRM_INIT_WAITQUEUE(&lock->queue);
+	atomic_set(&lock->write_lock_pending, 0);
+	atomic_set(&lock->readers, 0);
+}
+
+void drm_bo_read_unlock(struct drm_bo_lock *lock)
+{
+	if (unlikely(atomic_add_negative(-1, &lock->readers)))
+		BUG();
+	if (atomic_read(&lock->readers) == 0)
+		wake_up_interruptible(&lock->queue);
+}
+EXPORT_SYMBOL(drm_bo_read_unlock);
+
+int drm_bo_read_lock(struct drm_bo_lock *lock)
+{
+	while (unlikely(atomic_read(&lock->write_lock_pending) != 0)) {
+		int ret;
+		ret = wait_event_interruptible
+		    (lock->queue, atomic_read(&lock->write_lock_pending) == 0);
+		if (ret)
+			return -EAGAIN;
+	}
+
+	while (unlikely(!atomic_add_unless(&lock->readers, 1, -1))) {
+		int ret;
+		ret = wait_event_interruptible
+		    (lock->queue, atomic_add_unless(&lock->readers, 1, -1));
+		if (ret)
+			return -EAGAIN;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_read_lock);
+
+static int __drm_bo_write_unlock(struct drm_bo_lock *lock)
+{
+	if (unlikely(atomic_cmpxchg(&lock->readers, -1, 0) != -1))
+		return -EINVAL;
+	if (unlikely(atomic_cmpxchg(&lock->write_lock_pending, 1, 0) != 1))
+		return -EINVAL;
+	wake_up_interruptible(&lock->queue);
+	return 0;
+}
+
+static void drm_bo_write_lock_remove(struct drm_file *file_priv,
+				     struct drm_user_object *item)
+{
+	struct drm_bo_lock *lock = container_of(item, struct drm_bo_lock, base);
+	int ret;
+
+	ret = __drm_bo_write_unlock(lock);
+	BUG_ON(ret);
+}
+
+int drm_bo_write_lock(struct drm_bo_lock *lock, struct drm_file *file_priv)
+{
+	int ret = 0;
+	struct drm_device *dev;
+
+	if (unlikely(atomic_cmpxchg(&lock->write_lock_pending, 0, 1) != 0))
+		return -EINVAL;
+
+	while (unlikely(atomic_cmpxchg(&lock->readers, 0, -1) != 0)) {
+		ret = wait_event_interruptible
+		    (lock->queue, atomic_cmpxchg(&lock->readers, 0, -1) == 0);
+
+		if (ret) {
+			atomic_set(&lock->write_lock_pending, 0);
+			wake_up_interruptible(&lock->queue);
+			return -EAGAIN;
+		}
+	}
+
+	/*
+	 * Add a dummy user-object, the destructor of which will
+	 * make sure the lock is released if the client dies
+	 * while holding it.
+	 */
+
+	dev = file_priv->minor->dev;
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_user_object(file_priv, &lock->base, 0);
+	lock->base.remove = &drm_bo_write_lock_remove;
+	lock->base.type = drm_lock_type;
+	if (ret)
+		(void)__drm_bo_write_unlock(lock);
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int drm_bo_write_unlock(struct drm_bo_lock *lock, struct drm_file *file_priv)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_ref_object *ro;
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (lock->base.owner != file_priv) {
+		mutex_unlock(&dev->struct_mutex);
+		return -EINVAL;
+	}
+	ro = drm_lookup_ref_object(file_priv, &lock->base, _DRM_REF_USE);
+	BUG_ON(!ro);
+	drm_remove_ref_object(file_priv, ro);
+	lock->base.owner = NULL;
+
+	mutex_unlock(&dev->struct_mutex);
+	return 0;
+}
Index: linux-2.6.27/drivers/gpu/drm/drm_bo_move.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_bo_move.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,597 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+/**
+ * Free the old memory node unless it's a pinned region and we
+ * have not been requested to free also pinned regions.
+ */
+
+static void drm_bo_free_old_node(struct drm_buffer_object *bo)
+{
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+
+	if (old_mem->mm_node && (old_mem->mm_node != bo->pinned_node)) {
+		mutex_lock(&bo->dev->struct_mutex);
+		drm_mm_put_block(old_mem->mm_node);
+		mutex_unlock(&bo->dev->struct_mutex);
+	}
+	old_mem->mm_node = NULL;
+}
+
+int drm_bo_move_ttm(struct drm_buffer_object *bo,
+		    int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_ttm *ttm = bo->ttm;
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	uint64_t save_flags = old_mem->flags;
+	uint64_t save_mask = old_mem->mask;
+	int ret;
+
+	if (old_mem->mem_type != DRM_BO_MEM_LOCAL) {
+		if (evict)
+			drm_ttm_evict(ttm);
+		else
+			drm_ttm_unbind(ttm);
+
+		drm_bo_free_old_node(bo);
+		DRM_FLAG_MASKED(old_mem->flags,
+				DRM_BO_FLAG_CACHED | DRM_BO_FLAG_MAPPABLE |
+				DRM_BO_FLAG_MEM_LOCAL, DRM_BO_MASK_MEMTYPE);
+		old_mem->mem_type = DRM_BO_MEM_LOCAL;
+		save_flags = old_mem->flags;
+	}
+	if (new_mem->mem_type != DRM_BO_MEM_LOCAL) {
+		ret = drm_bind_ttm(ttm, new_mem);
+		if (ret)
+			return ret;
+	}
+
+	*old_mem = *new_mem;
+	new_mem->mm_node = NULL;
+	old_mem->mask = save_mask;
+	DRM_FLAG_MASKED(save_flags, new_mem->flags, DRM_BO_MASK_MEMTYPE);
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_move_ttm);
+
+/**
+ * \c Return a kernel virtual address to the buffer object PCI memory.
+ *
+ * \param bo The buffer object.
+ * \return Failure indication.
+ *
+ * Returns -EINVAL if the buffer object is currently not mappable.
+ * Returns -ENOMEM if the ioremap operation failed.
+ * Otherwise returns zero.
+ *
+ * After a successfull call, bo->iomap contains the virtual address, or NULL
+ * if the buffer object content is not accessible through PCI space.
+ * Call bo->mutex locked.
+ */
+
+int drm_mem_reg_ioremap(struct drm_device *dev, struct drm_bo_mem_reg *mem,
+			void **virtual)
+{
+	struct drm_buffer_manager *bm = &dev->bm;
+	struct drm_mem_type_manager *man = &bm->man[mem->mem_type];
+	unsigned long bus_offset;
+	unsigned long bus_size;
+	unsigned long bus_base;
+	int ret;
+	void *addr;
+
+	*virtual = NULL;
+	ret = drm_bo_pci_offset(dev, mem, &bus_base, &bus_offset, &bus_size);
+	if (ret || bus_size == 0)
+		return ret;
+
+	if (!(man->flags & _DRM_FLAG_NEEDS_IOREMAP))
+		addr = (void *)(((u8 *) man->io_addr) + bus_offset);
+	else {
+		addr = ioremap_nocache(bus_base + bus_offset, bus_size);
+		if (!addr)
+			return -ENOMEM;
+	}
+	*virtual = addr;
+	return 0;
+}
+EXPORT_SYMBOL(drm_mem_reg_ioremap);
+
+/**
+ * \c Unmap mapping obtained using drm_bo_ioremap
+ *
+ * \param bo The buffer object.
+ *
+ * Call bo->mutex locked.
+ */
+
+void drm_mem_reg_iounmap(struct drm_device *dev, struct drm_bo_mem_reg *mem,
+			 void *virtual)
+{
+	struct drm_buffer_manager *bm;
+	struct drm_mem_type_manager *man;
+
+	bm = &dev->bm;
+	man = &bm->man[mem->mem_type];
+
+	if (virtual && (man->flags & _DRM_FLAG_NEEDS_IOREMAP))
+		iounmap(virtual);
+}
+EXPORT_SYMBOL(drm_mem_reg_iounmap);
+
+static int drm_copy_io_page(void *dst, void *src, unsigned long page)
+{
+	uint32_t *dstP =
+	    (uint32_t *) ((unsigned long)dst + (page << PAGE_SHIFT));
+	uint32_t *srcP =
+	    (uint32_t *) ((unsigned long)src + (page << PAGE_SHIFT));
+
+	int i;
+	for (i = 0; i < PAGE_SIZE / sizeof(uint32_t); ++i)
+		iowrite32(ioread32(srcP++), dstP++);
+	return 0;
+}
+
+static int drm_copy_io_ttm_page(struct drm_ttm *ttm, void *src,
+				unsigned long page)
+{
+	struct page *d = drm_ttm_get_page(ttm, page);
+	void *dst;
+
+	if (!d)
+		return -ENOMEM;
+
+	src = (void *)((unsigned long)src + (page << PAGE_SHIFT));
+	dst = kmap(d);
+	if (!dst)
+		return -ENOMEM;
+
+	memcpy_fromio(dst, src, PAGE_SIZE);
+	kunmap(d);
+	return 0;
+}
+
+static int drm_copy_ttm_io_page(struct drm_ttm *ttm, void *dst, unsigned long page)
+{
+	struct page *s = drm_ttm_get_page(ttm, page);
+	void *src;
+
+	if (!s)
+		return -ENOMEM;
+
+	dst = (void *)((unsigned long)dst + (page << PAGE_SHIFT));
+	src = kmap(s);
+	if (!src)
+		return -ENOMEM;
+
+	memcpy_toio(dst, src, PAGE_SIZE);
+	kunmap(s);
+	return 0;
+}
+
+int drm_bo_move_memcpy(struct drm_buffer_object *bo,
+		       int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_mem_type_manager *man = &dev->bm.man[new_mem->mem_type];
+	struct drm_ttm *ttm = bo->ttm;
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	struct drm_bo_mem_reg old_copy = *old_mem;
+	void *old_iomap;
+	void *new_iomap;
+	int ret;
+	uint64_t save_flags = old_mem->flags;
+	uint64_t save_mask = old_mem->mask;
+	unsigned long i;
+	unsigned long page;
+	unsigned long add = 0;
+	int dir;
+
+	ret = drm_mem_reg_ioremap(dev, old_mem, &old_iomap);
+	if (ret)
+		return ret;
+	ret = drm_mem_reg_ioremap(dev, new_mem, &new_iomap);
+	if (ret)
+		goto out;
+
+	if (old_iomap == NULL && new_iomap == NULL)
+		goto out2;
+	if (old_iomap == NULL && ttm == NULL)
+		goto out2;
+
+	add = 0;
+	dir = 1;
+
+	if ((old_mem->mem_type == new_mem->mem_type) &&
+	    (new_mem->mm_node->start <
+	     old_mem->mm_node->start + old_mem->mm_node->size)) {
+		dir = -1;
+		add = new_mem->num_pages - 1;
+	}
+
+	for (i = 0; i < new_mem->num_pages; ++i) {
+		page = i * dir + add;
+		if (old_iomap == NULL)
+			ret = drm_copy_ttm_io_page(ttm, new_iomap, page);
+		else if (new_iomap == NULL)
+			ret = drm_copy_io_ttm_page(ttm, old_iomap, page);
+		else
+			ret = drm_copy_io_page(new_iomap, old_iomap, page);
+		if (ret)
+			goto out1;
+	}
+	mb();
+out2:
+	drm_bo_free_old_node(bo);
+
+	*old_mem = *new_mem;
+	new_mem->mm_node = NULL;
+	old_mem->mask = save_mask;
+	DRM_FLAG_MASKED(save_flags, new_mem->flags, DRM_BO_MASK_MEMTYPE);
+
+	if ((man->flags & _DRM_FLAG_MEMTYPE_FIXED) && (ttm != NULL)) {
+		drm_ttm_unbind(ttm);
+		drm_destroy_ttm(ttm);
+		bo->ttm = NULL;
+	}
+
+out1:
+	drm_mem_reg_iounmap(dev, new_mem, new_iomap);
+out:
+	drm_mem_reg_iounmap(dev, &old_copy, old_iomap);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_move_memcpy);
+
+/*
+ * Transfer a buffer object's memory and LRU status to a newly
+ * created object. User-space references remains with the old
+ * object. Call bo->mutex locked.
+ */
+
+int drm_buffer_object_transfer(struct drm_buffer_object *bo,
+			       struct drm_buffer_object **new_obj)
+{
+	struct drm_buffer_object *fbo;
+	struct drm_device *dev = bo->dev;
+	struct drm_buffer_manager *bm = &dev->bm;
+
+	fbo = drm_calloc(1, sizeof(*fbo), DRM_MEM_BUFOBJ);
+	if (!fbo)
+		return -ENOMEM;
+
+	*fbo = *bo;
+	mutex_init(&fbo->mutex);
+	mutex_lock(&fbo->mutex);
+	mutex_lock(&dev->struct_mutex);
+
+	DRM_INIT_WAITQUEUE(&bo->event_queue);
+	INIT_LIST_HEAD(&fbo->ddestroy);
+	INIT_LIST_HEAD(&fbo->lru);
+	INIT_LIST_HEAD(&fbo->pinned_lru);
+#ifdef DRM_ODD_MM_COMPAT
+	INIT_LIST_HEAD(&fbo->vma_list);
+	INIT_LIST_HEAD(&fbo->p_mm_list);
+#endif
+
+	fbo->fence = drm_fence_reference_locked(bo->fence);
+	fbo->pinned_node = NULL;
+	fbo->mem.mm_node->private = (void *)fbo;
+	atomic_set(&fbo->usage, 1);
+	atomic_inc(&bm->count);
+	mutex_unlock(&dev->struct_mutex);
+	mutex_unlock(&fbo->mutex);
+	bo->reserved_size = 0;
+	*new_obj = fbo;
+	return 0;
+}
+
+/*
+ * Since move is underway, we need to block signals in this function.
+ * We cannot restart until it has finished.
+ */
+
+int drm_bo_move_accel_cleanup(struct drm_buffer_object *bo,
+			      int evict, int no_wait, uint32_t fence_class,
+			      uint32_t fence_type, uint32_t fence_flags,
+			      struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_mem_type_manager *man = &dev->bm.man[new_mem->mem_type];
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	int ret;
+	uint64_t save_flags = old_mem->flags;
+	uint64_t save_mask = old_mem->mask;
+	struct drm_buffer_object *old_obj;
+
+	if (bo->fence)
+		drm_fence_usage_deref_unlocked(&bo->fence);
+	ret = drm_fence_object_create(dev, fence_class, fence_type,
+				      fence_flags | DRM_FENCE_FLAG_EMIT,
+				      &bo->fence);
+	bo->fence_type = fence_type;
+	if (ret)
+		return ret;
+
+#ifdef DRM_ODD_MM_COMPAT
+	/*
+	 * In this mode, we don't allow pipelining a copy blit,
+	 * since the buffer will be accessible from user space
+	 * the moment we return and rebuild the page tables.
+	 *
+	 * With normal vm operation, page tables are rebuilt
+	 * on demand using fault(), which waits for buffer idle.
+	 */
+	if (1)
+#else
+	if (evict || ((bo->mem.mm_node == bo->pinned_node) &&
+		      bo->mem.mm_node != NULL))
+#endif
+	{
+		ret = drm_bo_wait(bo, 0, 1, 0);
+		if (ret)
+			return ret;
+
+		drm_bo_free_old_node(bo);
+
+		if ((man->flags & _DRM_FLAG_MEMTYPE_FIXED) && (bo->ttm != NULL)) {
+			drm_ttm_unbind(bo->ttm);
+			drm_destroy_ttm(bo->ttm);
+			bo->ttm = NULL;
+		}
+	} else {
+
+		/* This should help pipeline ordinary buffer moves.
+		 *
+		 * Hang old buffer memory on a new buffer object,
+		 * and leave it to be released when the GPU
+		 * operation has completed.
+		 */
+
+		ret = drm_buffer_object_transfer(bo, &old_obj);
+
+		if (ret)
+			return ret;
+
+		if (!(man->flags & _DRM_FLAG_MEMTYPE_FIXED))
+			old_obj->ttm = NULL;
+		else
+			bo->ttm = NULL;
+
+		mutex_lock(&dev->struct_mutex);
+		list_del_init(&old_obj->lru);
+		DRM_FLAG_MASKED(bo->priv_flags, 0, _DRM_BO_FLAG_UNFENCED);
+		drm_bo_add_to_lru(old_obj);
+
+		drm_bo_usage_deref_locked(&old_obj);
+		mutex_unlock(&dev->struct_mutex);
+
+	}
+
+	*old_mem = *new_mem;
+	new_mem->mm_node = NULL;
+	old_mem->mask = save_mask;
+	DRM_FLAG_MASKED(save_flags, new_mem->flags, DRM_BO_MASK_MEMTYPE);
+	return 0;
+}
+EXPORT_SYMBOL(drm_bo_move_accel_cleanup);
+
+int drm_bo_same_page(unsigned long offset,
+		     unsigned long offset2)
+{
+	return (offset & PAGE_MASK) == (offset2 & PAGE_MASK);
+}
+EXPORT_SYMBOL(drm_bo_same_page);
+
+unsigned long drm_bo_offset_end(unsigned long offset,
+				unsigned long end)
+{
+	offset = (offset + PAGE_SIZE) & PAGE_MASK;
+	return (end < offset) ? end : offset;
+}
+EXPORT_SYMBOL(drm_bo_offset_end);
+
+static pgprot_t drm_kernel_io_prot(uint32_t map_type)
+{
+	pgprot_t tmp = PAGE_KERNEL;
+
+#if defined(__i386__) || defined(__x86_64__)
+#ifdef USE_PAT_WC
+#warning using pat
+	if (drm_use_pat() && map_type == _DRM_TTM) {
+		pgprot_val(tmp) |= _PAGE_PAT;
+		return tmp;
+	}
+#endif
+	if (boot_cpu_data.x86 > 3 && map_type != _DRM_AGP) {
+		pgprot_val(tmp) |= _PAGE_PCD;
+		pgprot_val(tmp) &= ~_PAGE_PWT;
+	}
+#elif defined(__powerpc__)
+	pgprot_val(tmp) |= _PAGE_NO_CACHE;
+	if (map_type == _DRM_REGISTERS)
+		pgprot_val(tmp) |= _PAGE_GUARDED;
+#endif
+#if defined(__ia64__)
+	if (map_type == _DRM_TTM)
+		tmp = pgprot_writecombine(tmp);
+	else
+		tmp = pgprot_noncached(tmp);
+#endif
+	return tmp;
+}
+
+static int drm_bo_ioremap(struct drm_buffer_object *bo, unsigned long bus_base,
+			  unsigned long bus_offset, unsigned long bus_size,
+			  struct drm_bo_kmap_obj *map)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_bo_mem_reg *mem = &bo->mem;
+	struct drm_mem_type_manager *man = &dev->bm.man[mem->mem_type];
+
+	if (!(man->flags & _DRM_FLAG_NEEDS_IOREMAP)) {
+		map->bo_kmap_type = bo_map_premapped;
+		map->virtual = (void *)(((u8 *) man->io_addr) + bus_offset);
+	} else {
+		map->bo_kmap_type = bo_map_iomap;
+		map->virtual = ioremap_nocache(bus_base + bus_offset, bus_size);
+	}
+	return (!map->virtual) ? -ENOMEM : 0;
+}
+
+static int drm_bo_kmap_ttm(struct drm_buffer_object *bo,
+			   unsigned long start_page, unsigned long num_pages,
+			   struct drm_bo_kmap_obj *map)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_bo_mem_reg *mem = &bo->mem;
+	struct drm_mem_type_manager *man = &dev->bm.man[mem->mem_type];
+	pgprot_t prot;
+	struct drm_ttm *ttm = bo->ttm;
+	struct page *d;
+	int i;
+
+	BUG_ON(!ttm);
+
+	if (num_pages == 1 && (mem->flags & DRM_BO_FLAG_CACHED)) {
+
+		/*
+		 * We're mapping a single page, and the desired
+		 * page protection is consistent with the bo.
+		 */
+
+		map->bo_kmap_type = bo_map_kmap;
+		map->page = drm_ttm_get_page(ttm, start_page);
+		map->virtual = kmap(map->page);
+	} else {
+		/*
+		 * Populate the part we're mapping;
+		 */
+
+		for (i = start_page; i < start_page + num_pages; ++i) {
+			d = drm_ttm_get_page(ttm, i);
+			if (!d)
+				return -ENOMEM;
+		}
+
+		/*
+		 * We need to use vmap to get the desired page protection
+		 * or to make the buffer object look contigous.
+		 */
+
+		prot = (mem->flags & DRM_BO_FLAG_CACHED) ?
+			PAGE_KERNEL :
+			drm_kernel_io_prot(man->drm_bus_maptype);
+		map->bo_kmap_type = bo_map_vmap;
+		map->virtual = vmap(ttm->pages + start_page,
+				    num_pages, 0, prot);
+	}
+	return (!map->virtual) ? -ENOMEM : 0;
+}
+
+/*
+ * This function is to be used for kernel mapping of buffer objects.
+ * It chooses the appropriate mapping method depending on the memory type
+ * and caching policy the buffer currently has.
+ * Mapping multiple pages or buffers that live in io memory is a bit slow and
+ * consumes vmalloc space. Be restrictive with such mappings.
+ * Mapping single pages usually returns the logical kernel address,
+ * (which is fast)
+ * BUG may use slower temporary mappings for high memory pages or
+ * uncached / write-combined pages.
+ *
+ * The function fills in a drm_bo_kmap_obj which can be used to return the
+ * kernel virtual address of the buffer.
+ *
+ * Code servicing a non-priviliged user request is only allowed to map one
+ * page at a time. We might need to implement a better scheme to stop such
+ * processes from consuming all vmalloc space.
+ */
+
+int drm_bo_kmap(struct drm_buffer_object *bo, unsigned long start_page,
+		unsigned long num_pages, struct drm_bo_kmap_obj *map)
+{
+	int ret;
+	unsigned long bus_base;
+	unsigned long bus_offset;
+	unsigned long bus_size;
+
+	map->virtual = NULL;
+
+	if (num_pages > bo->num_pages)
+		return -EINVAL;
+	if (start_page > bo->num_pages)
+		return -EINVAL;
+#if 0
+	if (num_pages > 1 && !DRM_SUSER(DRM_CURPROC))
+		return -EPERM;
+#endif
+	ret = drm_bo_pci_offset(bo->dev, &bo->mem, &bus_base,
+				&bus_offset, &bus_size);
+
+	if (ret)
+		return ret;
+
+	if (bus_size == 0) {
+		return drm_bo_kmap_ttm(bo, start_page, num_pages, map);
+	} else {
+		bus_offset += start_page << PAGE_SHIFT;
+		bus_size = num_pages << PAGE_SHIFT;
+		return drm_bo_ioremap(bo, bus_base, bus_offset, bus_size, map);
+	}
+}
+EXPORT_SYMBOL(drm_bo_kmap);
+
+void drm_bo_kunmap(struct drm_bo_kmap_obj *map)
+{
+	if (!map->virtual)
+		return;
+
+	switch (map->bo_kmap_type) {
+	case bo_map_iomap:
+		iounmap(map->virtual);
+		break;
+	case bo_map_vmap:
+		vunmap(map->virtual);
+		break;
+	case bo_map_kmap:
+		kunmap(map->page);
+		break;
+	case bo_map_premapped:
+		break;
+	default:
+		BUG();
+	}
+	map->virtual = NULL;
+	map->page = NULL;
+}
+EXPORT_SYMBOL(drm_bo_kunmap);
Index: linux-2.6.27/drivers/gpu/drm/drm_bufs.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_bufs.c	2008-10-09 23:13:53.000000000 +0100
+++ linux-2.6.27/drivers/gpu/drm/drm_bufs.c	2009-02-05 13:29:33.000000000 +0000
@@ -409,6 +409,7 @@
 		break;
 	case _DRM_SHM:
 		vfree(map->handle);
+		dev->sigdata.lock = dev->lock.hw_lock = NULL;   /* SHM removed */
 		break;
 	case _DRM_AGP:
 	case _DRM_SCATTER_GATHER:
@@ -419,6 +420,8 @@
 		dmah.size = map->size;
 		__drm_pci_free(dev, &dmah);
 		break;
+	case _DRM_TTM:
+		BUG_ON(1);
 	}
 	drm_free(map, sizeof(*map), DRM_MEM_MAPS);
 
Index: linux-2.6.27/drivers/gpu/drm/drm_crtc.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_crtc.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,2170 @@
+/*
+ * Copyright (c) 2006-2007 Intel Corporation
+ * Copyright (c) 2007 Dave Airlie <airlied@linux.ie>
+ *
+ * DRM core CRTC related functions
+ *
+ * Permission to use, copy, modify, distribute, and sell this software and its
+ * documentation for any purpose is hereby granted without fee, provided that
+ * the above copyright notice appear in all copies and that both that copyright
+ * notice and this permission notice appear in supporting documentation, and
+ * that the name of the copyright holders not be used in advertising or
+ * publicity pertaining to distribution of the software without specific,
+ * written prior permission.  The copyright holders make no representations
+ * about the suitability of this software for any purpose.  It is provided "as
+ * is" without express or implied warranty.
+ *
+ * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
+ * INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO
+ * EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY SPECIAL, INDIRECT OR
+ * CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,
+ * DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER
+ * TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE
+ * OF THIS SOFTWARE.
+ *
+ * Authors:
+ *      Keith Packard
+ *	Eric Anholt <eric@anholt.net>
+ *      Dave Airlie <airlied@linux.ie>
+ *      Jesse Barnes <jesse.barnes@intel.com>
+ */
+#include <linux/list.h>
+#include "drm.h"
+#include "drmP.h"
+#include "drm_crtc.h"
+
+/**
+ * drm_idr_get - allocate a new identifier
+ * @dev: DRM device
+ * @ptr: object pointer, used to generate unique ID
+ *
+ * LOCKING:
+ * Caller must hold DRM mode_config lock.
+ *
+ * Create a unique identifier based on @ptr in @dev's identifier space.  Used
+ * for tracking modes, CRTCs and outputs.
+ *
+ * RETURNS:
+ * New unique (relative to other objects in @dev) integer identifier for the
+ * object.
+ */
+int drm_idr_get(struct drm_device *dev, void *ptr)
+{
+	int new_id = 0;
+	int ret;
+again:
+	if (idr_pre_get(&dev->mode_config.crtc_idr, GFP_KERNEL) == 0) {
+		DRM_ERROR("Ran out memory getting a mode number\n");
+		return 0;
+	}
+
+	ret = idr_get_new_above(&dev->mode_config.crtc_idr, ptr, 1, &new_id);
+	if (ret == -EAGAIN)
+		goto again;
+
+	return new_id;
+}
+
+/**
+ * drm_idr_put - free an identifer
+ * @dev: DRM device
+ * @id: ID to free
+ *
+ * LOCKING:
+ * Caller must hold DRM mode_config lock.
+ *
+ * Free @id from @dev's unique identifier pool.
+ */
+void drm_idr_put(struct drm_device *dev, int id)
+{
+	idr_remove(&dev->mode_config.crtc_idr, id);
+}
+
+/**
+ * drm_crtc_from_fb - find the CRTC structure associated with an fb
+ * @dev: DRM device
+ * @fb: framebuffer in question
+ *
+ * LOCKING:
+ * Caller must hold mode_config lock.
+ *
+ * Find CRTC in the mode_config structure that matches @fb.
+ *
+ * RETURNS:
+ * Pointer to the CRTC or NULL if it wasn't found.
+ */
+struct drm_crtc *drm_crtc_from_fb(struct drm_device *dev,
+				  struct drm_framebuffer *fb)
+{
+	struct drm_crtc *crtc;
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		if (crtc->fb == fb)
+			return crtc;
+	}
+	return NULL;
+}
+
+/**
+ * drm_framebuffer_create - create a new framebuffer object
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Creates a new framebuffer objects and adds it to @dev's DRM mode_config.
+ *
+ * RETURNS:
+ * Pointer to new framebuffer or NULL on error.
+ */
+struct drm_framebuffer *drm_framebuffer_create(struct drm_device *dev)
+{
+	struct drm_framebuffer *fb;
+
+	/* Limit to single framebuffer for now */
+	if (dev->mode_config.num_fb > 1) {
+		mutex_unlock(&dev->mode_config.mutex);
+		DRM_ERROR("Attempt to add multiple framebuffers failed\n");
+		return NULL;
+	}
+
+	fb = kzalloc(sizeof(struct drm_framebuffer), GFP_KERNEL);
+	if (!fb)
+		return NULL;
+
+	fb->id = drm_idr_get(dev, fb);
+	fb->dev = dev;
+	dev->mode_config.num_fb++;
+	list_add(&fb->head, &dev->mode_config.fb_list);
+
+	return fb;
+}
+EXPORT_SYMBOL(drm_framebuffer_create);
+
+/**
+ * drm_framebuffer_destroy - remove a framebuffer object
+ * @fb: framebuffer to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Scans all the CRTCs in @dev's mode_config.  If they're using @fb, removes
+ * it, setting it to NULL.
+ */
+void drm_framebuffer_destroy(struct drm_framebuffer *fb)
+{
+	struct drm_device *dev = fb->dev;
+	struct drm_crtc *crtc;
+
+	/* remove from any CRTC */
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		if (crtc->fb == fb)
+			crtc->fb = NULL;
+	}
+
+	drm_idr_put(dev, fb->id);
+	list_del(&fb->head);
+	dev->mode_config.num_fb--;
+
+	kfree(fb);
+}
+EXPORT_SYMBOL(drm_framebuffer_destroy);
+
+/**
+ * drm_crtc_create - create a new CRTC object
+ * @dev: DRM device
+ * @funcs: callbacks for the new CRTC
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Creates a new CRTC object and adds it to @dev's mode_config structure.
+ *
+ * RETURNS:
+ * Pointer to new CRTC object or NULL on error.
+ */
+struct drm_crtc *drm_crtc_create(struct drm_device *dev,
+				 const struct drm_crtc_funcs *funcs)
+{
+	struct drm_crtc *crtc;
+
+	crtc = kzalloc(sizeof(struct drm_crtc), GFP_KERNEL);
+	if (!crtc)
+		return NULL;
+
+	crtc->dev = dev;
+	crtc->funcs = funcs;
+
+	crtc->id = drm_idr_get(dev, crtc);
+
+	list_add_tail(&crtc->head, &dev->mode_config.crtc_list);
+	dev->mode_config.num_crtc++;
+
+	return crtc;
+}
+EXPORT_SYMBOL(drm_crtc_create);
+
+/**
+ * drm_crtc_destroy - remove a CRTC object
+ * @crtc: CRTC to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Cleanup @crtc.  Calls @crtc's cleanup function, then removes @crtc from
+ * its associated DRM device's mode_config.  Frees it afterwards.
+ */
+void drm_crtc_destroy(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+
+	if (crtc->funcs->cleanup)
+		(*crtc->funcs->cleanup)(crtc);
+
+	drm_idr_put(dev, crtc->id);
+	list_del(&crtc->head);
+	dev->mode_config.num_crtc--;
+	kfree(crtc);
+}
+EXPORT_SYMBOL(drm_crtc_destroy);
+
+/**
+ * drm_crtc_in_use - check if a given CRTC is in a mode_config
+ * @crtc: CRTC to check
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Walk @crtc's DRM device's mode_config and see if it's in use.
+ *
+ * RETURNS:
+ * True if @crtc is part of the mode_config, false otherwise.
+ */
+bool drm_crtc_in_use(struct drm_crtc *crtc)
+{
+	struct drm_output *output;
+	struct drm_device *dev = crtc->dev;
+	/* FIXME: Locking around list access? */
+	list_for_each_entry(output, &dev->mode_config.output_list, head)
+		if (output->crtc == crtc)
+			return true;
+	return false;
+}
+EXPORT_SYMBOL(drm_crtc_in_use);
+
+/*
+ * Detailed mode info for a standard 640x480@60Hz monitor
+ */
+static struct drm_display_mode std_mode[] = {
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DEFAULT, 25200, 640, 656,
+		   752, 800, 0, 480, 490, 492, 525, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@60Hz */
+};
+
+/**
+ * drm_crtc_probe_output_modes - get complete set of display modes
+ * @dev: DRM device
+ * @maxX: max width for modes
+ * @maxY: max height for modes
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Based on @dev's mode_config layout, scan all the outputs and try to detect
+ * modes on them.  Modes will first be added to the output's probed_modes
+ * list, then culled (based on validity and the @maxX, @maxY parameters) and
+ * put into the normal modes list.
+ *
+ * Intended to be used either at bootup time or when major configuration
+ * changes have occurred.
+ *
+ * FIXME: take into account monitor limits
+ */
+void drm_crtc_probe_output_modes(struct drm_device *dev, int maxX, int maxY)
+{
+	struct drm_output *output;
+	struct drm_display_mode *mode, *t;
+	int ret;
+	//if (maxX == 0 || maxY == 0)
+	// TODO
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		/* set all modes to the unverified state */
+		list_for_each_entry_safe(mode, t, &output->modes, head)
+			mode->status = MODE_UNVERIFIED;
+
+		output->status = (*output->funcs->detect)(output);
+
+		if (output->status == output_status_disconnected) {
+			DRM_DEBUG("%s is disconnected\n", output->name);
+			/* TODO set EDID to NULL */
+			continue;
+		}
+
+		ret = (*output->funcs->get_modes)(output);
+
+		if (ret) {
+			drm_mode_output_list_update(output);
+		}
+
+		if (maxX && maxY)
+			drm_mode_validate_size(dev, &output->modes, maxX,
+					       maxY, 0);
+		list_for_each_entry_safe(mode, t, &output->modes, head) {
+			if (mode->status == MODE_OK)
+				mode->status = (*output->funcs->mode_valid)(output,mode);
+		}
+
+
+		drm_mode_prune_invalid(dev, &output->modes, 1);
+
+		if (list_empty(&output->modes)) {
+			struct drm_display_mode *stdmode;
+
+			DRM_DEBUG("No valid modes on %s\n", output->name);
+
+			/* Should we do this here ???
+			 * When no valid EDID modes are available we end up
+			 * here and bailed in the past, now we add a standard
+			 * 640x480@60Hz mode and carry on.
+			 */
+			stdmode = drm_mode_duplicate(dev, &std_mode[0]);
+			drm_mode_probed_add(output, stdmode);
+			drm_mode_list_concat(&output->probed_modes,
+					     &output->modes);
+
+			DRM_DEBUG("Adding standard 640x480 @ 60Hz to %s\n",
+								output->name);
+		}
+
+		drm_mode_sort(&output->modes);
+
+		DRM_DEBUG("Probed modes for %s\n", output->name);
+		list_for_each_entry_safe(mode, t, &output->modes, head) {
+			mode->vrefresh = drm_mode_vrefresh(mode);
+
+			drm_mode_set_crtcinfo(mode, CRTC_INTERLACE_HALVE_V);
+			drm_mode_debug_printmodeline(dev, mode);
+		}
+	}
+}
+EXPORT_SYMBOL(drm_crtc_probe_output_modes);
+
+/**
+ * drm_crtc_set_mode - set a mode
+ * @crtc: CRTC to program
+ * @mode: mode to use
+ * @x: width of mode
+ * @y: height of mode
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Try to set @mode on @crtc.  Give @crtc and its associated outputs a chance
+ * to fixup or reject the mode prior to trying to set it.
+ *
+ * RETURNS:
+ * True if the mode was set successfully, or false otherwise.
+ */
+bool drm_crtc_set_mode(struct drm_crtc *crtc, struct drm_display_mode *mode,
+		       int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_display_mode *adjusted_mode, saved_mode;
+	int saved_x, saved_y;
+	bool didLock = false;
+	bool ret = false;
+	struct drm_output *output;
+
+	adjusted_mode = drm_mode_duplicate(dev, mode);
+
+	crtc->enabled = drm_crtc_in_use(crtc);
+
+	if (!crtc->enabled) {
+		return true;
+	}
+
+	didLock = crtc->funcs->lock(crtc);
+
+	saved_mode = crtc->mode;
+	saved_x = crtc->x;
+	saved_y = crtc->y;
+
+	/* Update crtc values up front so the driver can rely on them for mode
+	 * setting.
+	 */
+	crtc->mode = *mode;
+	crtc->x = x;
+	crtc->y = y;
+
+	/* XXX short-circuit changes to base location only */
+
+	/* Pass our mode to the outputs and the CRTC to give them a chance to
+	 * adjust it according to limitations or output properties, and also
+	 * a chance to reject the mode entirely.
+	 */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (output->crtc != crtc)
+			continue;
+
+		if (!output->funcs->mode_fixup(output, mode, adjusted_mode)) {
+			goto done;
+		}
+	}
+
+	if (!crtc->funcs->mode_fixup(crtc, mode, adjusted_mode)) {
+		goto done;
+	}
+
+	/* Prepare the outputs and CRTCs before setting the mode. */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (output->crtc != crtc)
+			continue;
+
+		/* Disable the output as the first thing we do. */
+		output->funcs->prepare(output);
+	}
+
+	crtc->funcs->prepare(crtc);
+
+	/* Set up the DPLL and any output state that needs to adjust or depend
+	 * on the DPLL.
+	 */
+	crtc->funcs->mode_set(crtc, mode, adjusted_mode, x, y);
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (output->crtc != crtc)
+			continue;
+
+		DRM_INFO("%s: set mode %s %x\n", output->name, mode->name, mode->mode_id);
+
+		output->funcs->mode_set(output, mode, adjusted_mode);
+	}
+
+	/* Now, enable the clocks, plane, pipe, and outputs that we set up. */
+	crtc->funcs->commit(crtc);
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (output->crtc != crtc)
+			continue;
+
+		output->funcs->commit(output);
+
+#if 0 // TODO def RANDR_12_INTERFACE
+		if (output->randr_output)
+			RRPostPendingProperties (output->randr_output);
+#endif
+	}
+
+	/* XXX free adjustedmode */
+	drm_mode_destroy(dev, adjusted_mode);
+	ret = 1;
+	/* TODO */
+//	if (scrn->pScreen)
+//		drm_crtc_set_screen_sub_pixel_order(dev);
+
+done:
+	if (!ret) {
+		crtc->x = saved_x;
+		crtc->y = saved_y;
+		crtc->mode = saved_mode;
+	}
+
+	if (didLock)
+		crtc->funcs->unlock (crtc);
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_crtc_set_mode);
+
+/**
+ * drm_disable_unused_functions - disable unused objects
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * If an output or CRTC isn't part of @dev's mode_config, it can be disabled
+ * by calling its dpms function, which should power it off.
+ */
+void drm_disable_unused_functions(struct drm_device *dev)
+{
+	struct drm_output *output;
+	struct drm_crtc *crtc;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		if (!output->crtc)
+			(*output->funcs->dpms)(output, DPMSModeOff);
+	}
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		if (!crtc->enabled)
+			crtc->funcs->dpms(crtc, DPMSModeOff);
+	}
+}
+EXPORT_SYMBOL(drm_disable_unused_functions);
+
+/**
+ * drm_mode_probed_add - add a mode to the specified output's probed mode list
+ * @output: output the new mode
+ * @mode: mode data
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Add @mode to @output's mode list for later use.
+ */
+void drm_mode_probed_add(struct drm_output *output,
+			 struct drm_display_mode *mode)
+{
+	list_add(&mode->head, &output->probed_modes);
+}
+EXPORT_SYMBOL(drm_mode_probed_add);
+
+/**
+ * drm_mode_remove - remove and free a mode
+ * @output: output list to modify
+ * @mode: mode to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Remove @mode from @output's mode list, then free it.
+ */
+void drm_mode_remove(struct drm_output *output, struct drm_display_mode *mode)
+{
+	list_del(&mode->head);
+	kfree(mode);
+}
+EXPORT_SYMBOL(drm_mode_remove);
+
+/**
+ * drm_output_create - create a new output
+ * @dev: DRM device
+ * @funcs: callbacks for this output
+ * @name: user visible name of the output
+ *
+ * LOCKING:
+ * Caller must hold @dev's mode_config lock.
+ *
+ * Creates a new drm_output structure and adds it to @dev's mode_config
+ * structure.
+ *
+ * RETURNS:
+ * Pointer to the new output or NULL on error.
+ */
+struct drm_output *drm_output_create(struct drm_device *dev,
+				     const struct drm_output_funcs *funcs,
+				     const char *name)
+{
+	struct drm_output *output = NULL;
+
+	output = kzalloc(sizeof(struct drm_output), GFP_KERNEL);
+	if (!output)
+		return NULL;
+
+	output->dev = dev;
+	output->funcs = funcs;
+	output->id = drm_idr_get(dev, output);
+	if (name)
+		strncpy(output->name, name, DRM_OUTPUT_LEN);
+	output->name[DRM_OUTPUT_LEN - 1] = 0;
+	output->subpixel_order = SubPixelUnknown;
+	INIT_LIST_HEAD(&output->probed_modes);
+	INIT_LIST_HEAD(&output->modes);
+	/* randr_output? */
+	/* output_set_monitor(output)? */
+	/* check for output_ignored(output)? */
+
+	mutex_lock(&dev->mode_config.mutex);
+	list_add_tail(&output->head, &dev->mode_config.output_list);
+	dev->mode_config.num_output++;
+
+	mutex_unlock(&dev->mode_config.mutex);
+
+	return output;
+
+}
+EXPORT_SYMBOL(drm_output_create);
+
+/**
+ * drm_output_destroy - remove an output
+ * @output: output to remove
+ *
+ * LOCKING:
+ * Caller must hold @dev's mode_config lock.
+ *
+ * Call @output's cleanup function, then remove the output from the DRM
+ * mode_config after freeing @output's modes.
+ */
+void drm_output_destroy(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_display_mode *mode, *t;
+
+	if (*output->funcs->cleanup)
+		(*output->funcs->cleanup)(output);
+
+	list_for_each_entry_safe(mode, t, &output->probed_modes, head)
+		drm_mode_remove(output, mode);
+
+	list_for_each_entry_safe(mode, t, &output->modes, head)
+		drm_mode_remove(output, mode);
+
+	mutex_lock(&dev->mode_config.mutex);
+	drm_idr_put(dev, output->id);
+	list_del(&output->head);
+	mutex_unlock(&dev->mode_config.mutex);
+	kfree(output);
+}
+EXPORT_SYMBOL(drm_output_destroy);
+
+/**
+ * drm_output_rename - rename an output
+ * @output: output to rename
+ * @name: new user visible name
+ *
+ * LOCKING:
+ * None.
+ *
+ * Simply stuff a new name into @output's name field, based on @name.
+ *
+ * RETURNS:
+ * True if the name was changed, false otherwise.
+ */
+bool drm_output_rename(struct drm_output *output, const char *name)
+{
+	if (!name)
+		return false;
+
+	strncpy(output->name, name, DRM_OUTPUT_LEN);
+	output->name[DRM_OUTPUT_LEN - 1] = 0;
+
+	DRM_DEBUG("Changed name to %s\n", output->name);
+//	drm_output_set_monitor(output);
+//	if (drm_output_ignored(output))
+//		return FALSE;
+
+	return 1;
+}
+EXPORT_SYMBOL(drm_output_rename);
+
+/**
+ * drm_mode_create - create a new display mode
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * None.
+ *
+ * Create a new drm_display_mode, give it an ID, and return it.
+ *
+ * RETURNS:
+ * Pointer to new mode on success, NULL on error.
+ */
+struct drm_display_mode *drm_mode_create(struct drm_device *dev)
+{
+	struct drm_display_mode *nmode;
+
+	nmode = kzalloc(sizeof(struct drm_display_mode), GFP_KERNEL);
+	if (!nmode)
+		return NULL;
+
+	nmode->mode_id = drm_idr_get(dev, nmode);
+	return nmode;
+}
+EXPORT_SYMBOL(drm_mode_create);
+
+/**
+ * drm_mode_destroy - remove a mode
+ * @dev: DRM device
+ * @mode: mode to remove
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Free @mode's unique identifier, then free it.
+ */
+void drm_mode_destroy(struct drm_device *dev, struct drm_display_mode *mode)
+{
+	drm_idr_put(dev, mode->mode_id);
+
+	kfree(mode);
+}
+EXPORT_SYMBOL(drm_mode_destroy);
+
+/**
+ * drm_mode_config_init - initialize DRM mode_configuration structure
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * None, should happen single threaded at init time.
+ *
+ * Initialize @dev's mode_config structure, used for tracking the graphics
+ * configuration of @dev.
+ */
+void drm_mode_config_init(struct drm_device *dev)
+{
+	mutex_init(&dev->mode_config.mutex);
+	INIT_LIST_HEAD(&dev->mode_config.fb_list);
+	INIT_LIST_HEAD(&dev->mode_config.crtc_list);
+	INIT_LIST_HEAD(&dev->mode_config.output_list);
+	INIT_LIST_HEAD(&dev->mode_config.property_list);
+	INIT_LIST_HEAD(&dev->mode_config.usermode_list);
+	idr_init(&dev->mode_config.crtc_idr);
+}
+EXPORT_SYMBOL(drm_mode_config_init);
+
+/**
+ * drm_get_buffer_object - find the buffer object for a given handle
+ * @dev: DRM device
+ * @bo: pointer to caller's buffer_object pointer
+ * @handle: handle to lookup
+ *
+ * LOCKING:
+ * Must take @dev's struct_mutex to protect buffer object lookup.
+ *
+ * Given @handle, lookup the buffer object in @dev and put it in the caller's
+ * @bo pointer.
+ *
+ * RETURNS:
+ * Zero on success, -EINVAL if the handle couldn't be found.
+ */
+static int drm_get_buffer_object(struct drm_device *dev, struct drm_buffer_object **bo, unsigned long handle)
+{
+	struct drm_user_object *uo;
+	struct drm_hash_item *hash;
+	int ret;
+
+	*bo = NULL;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_ht_find_item(&dev->object_hash, handle, &hash);
+	if (ret) {
+		DRM_ERROR("Couldn't find handle.\n");
+		ret = -EINVAL;
+		goto out_err;
+	}
+
+	uo = drm_hash_entry(hash, struct drm_user_object, hash);
+	if (uo->type != drm_buffer_type) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+
+	*bo = drm_user_object_entry(uo, struct drm_buffer_object, base);
+	ret = 0;
+out_err:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+char drm_init_mode[32];
+int drm_init_xres;
+int drm_init_yres;
+EXPORT_SYMBOL(drm_init_mode);
+EXPORT_SYMBOL(drm_init_xres);
+EXPORT_SYMBOL(drm_init_yres);
+
+/**
+ * drm_pick_crtcs - pick crtcs for output devices
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ */
+static void drm_pick_crtcs (struct drm_device *dev)
+{
+	int c, o, assigned;
+	struct drm_output *output, *output_equal;
+	struct drm_crtc   *crtc;
+	struct drm_display_mode *des_mode = NULL, *modes, *modes_equal;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+       		output->crtc = NULL;
+
+    		/* Don't hook up outputs that are disconnected ??
+		 *
+		 * This is debateable. Do we want fixed /dev/fbX or
+		 * dynamic on hotplug (need mode code for that though) ?
+		 *
+		 * If we don't hook up outputs now, then we only create
+		 * /dev/fbX for the output that's enabled, that's good as
+		 * the users console will be on that output.
+		 *
+		 * If we do hook up outputs that are disconnected now, then
+		 * the user may end up having to muck about with the fbcon
+		 * map flags to assign his console to the enabled output. Ugh.
+		 */
+    		if (output->status != output_status_connected)
+			continue;
+
+		des_mode = NULL;
+		list_for_each_entry(des_mode, &output->modes, head) {
+			if (/* !strcmp(des_mode->name, drm_init_mode) ||  */
+			    des_mode->hdisplay==drm_init_xres
+			    && des_mode->vdisplay==drm_init_yres) {
+				des_mode->type |= DRM_MODE_TYPE_USERPREF;
+				break;
+			}
+
+		}
+		/* No userdef mode (initial mode set from module parameter) */
+		if (!des_mode || !(des_mode->type & DRM_MODE_TYPE_USERPREF)) {
+			list_for_each_entry(des_mode, &output->modes, head) {
+				if (des_mode->type & DRM_MODE_TYPE_PREFERRED)
+					break;
+			}
+		}
+
+		/* No preferred mode, and no default mode, let's just
+		   select the first available */
+		if (!des_mode || (!(des_mode->type & DRM_MODE_TYPE_PREFERRED)
+				  && !(des_mode->type & DRM_MODE_TYPE_USERPREF))) {
+			list_for_each_entry(des_mode, &output->modes, head) {
+				if (des_mode)
+					break;
+			}
+		}
+
+		c = -1;
+		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+			assigned = 0;
+
+			c++;
+			if ((output->possible_crtcs & (1 << c)) == 0)
+		    		continue;
+
+			list_for_each_entry(output_equal, &dev->mode_config.output_list, head) {
+				if (output->id == output_equal->id)
+					continue;
+
+				/* Find out if crtc has been assigned before */
+				if (output_equal->crtc == crtc)
+					assigned = 1;
+			}
+
+#if 1 /* continue for now */
+			if (assigned)
+				continue;
+#endif
+
+			o = -1;
+			list_for_each_entry(output_equal, &dev->mode_config.output_list, head) {
+				o++;
+				if (output->id == output_equal->id)
+					continue;
+
+				list_for_each_entry(modes, &output->modes, head) {
+					list_for_each_entry(modes_equal, &output_equal->modes, head) {
+						if (drm_mode_equal (modes, modes_equal)) {
+							if ((output->possible_clones & output_equal->possible_clones) && (output_equal->crtc == crtc)) {
+								printk("Cloning %s (0x%lx) to %s (0x%lx)\n",output->name,output->possible_clones,output_equal->name,output_equal->possible_clones);
+								assigned = 0;
+								goto clone;
+							}
+						}
+					}
+				}
+			}
+
+clone:
+			/* crtc has been assigned skip it */
+			if (assigned)
+				continue;
+
+			/* Found a CRTC to attach to, do it ! */
+			output->crtc = crtc;
+			output->crtc->desired_mode = des_mode;
+			output->initial_x = 0;
+			output->initial_y = 0;
+			DRM_DEBUG("Desired mode for CRTC %d is 0x%x:%s\n",c,des_mode->mode_id, des_mode->name);
+			break;
+    		}
+	}
+}
+EXPORT_SYMBOL(drm_pick_crtcs);
+
+/**
+ * drm_initial_config - setup a sane initial output configuration
+ * @dev: DRM device
+ * @can_grow: this configuration is growable
+ *
+ * LOCKING:
+ * Called at init time, must take mode config lock.
+ *
+ * Scan the CRTCs and outputs and try to put together an initial setup.
+ * At the moment, this is a cloned configuration across all heads with
+ * a new framebuffer object as the backing store.
+ *
+ * RETURNS:
+ * Zero if everything went ok, nonzero otherwise.
+ */
+bool drm_initial_config(struct drm_device *dev, bool can_grow)
+{
+	struct drm_output *output;
+	struct drm_crtc *crtc;
+	int ret = false;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	drm_crtc_probe_output_modes(dev, 2048, 2048);
+
+	drm_pick_crtcs(dev);
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+
+		/* can't setup the crtc if there's no assigned mode */
+		if (!crtc->desired_mode)
+			continue;
+
+		/* Now setup the fbdev for attached crtcs */
+		dev->driver->fb_probe(dev, crtc);
+	}
+
+	/* This is a little screwy, as we've already walked the outputs
+	 * above, but it's a little bit of magic too. There's the potential
+	 * for things not to get setup above if an existing device gets
+	 * re-assigned thus confusing the hardware. By walking the outputs
+	 * this fixes up their crtc's.
+	 */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		/* can't setup the output if there's no assigned mode */
+		if (!output->crtc || !output->crtc->desired_mode)
+			continue;
+
+		/* and needs an attached fb */
+		if (output->crtc->fb)
+			drm_crtc_set_mode(output->crtc, output->crtc->desired_mode, 0, 0);
+	}
+
+	drm_disable_unused_functions(dev);
+
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_initial_config);
+
+/**
+ * drm_mode_config_cleanup - free up DRM mode_config info
+ * @dev: DRM device
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Free up all the outputs and CRTCs associated with this DRM device, then
+ * free up the framebuffers and associated buffer objects.
+ *
+ * FIXME: cleanup any dangling user buffer objects too
+ */
+void drm_mode_config_cleanup(struct drm_device *dev)
+{
+	struct drm_output *output, *ot;
+	struct drm_crtc *crtc, *ct;
+	struct drm_framebuffer *fb, *fbt;
+	struct drm_display_mode *mode, *mt;
+	struct drm_property *property, *pt;
+
+	list_for_each_entry_safe(output, ot, &dev->mode_config.output_list, head) {
+		drm_output_destroy(output);
+	}
+
+	list_for_each_entry_safe(property, pt, &dev->mode_config.property_list, head) {
+		drm_property_destroy(dev, property);
+	}
+
+	list_for_each_entry_safe(mode, mt, &dev->mode_config.usermode_list, head) {
+		drm_mode_destroy(dev, mode);
+	}
+
+	list_for_each_entry_safe(fb, fbt, &dev->mode_config.fb_list, head) {
+		if (fb->bo->type != drm_bo_type_kernel)
+			drm_framebuffer_destroy(fb);
+		else
+			dev->driver->fb_remove(dev, drm_crtc_from_fb(dev, fb));
+	}
+
+	list_for_each_entry_safe(crtc, ct, &dev->mode_config.crtc_list, head) {
+		drm_crtc_destroy(crtc);
+	}
+
+}
+EXPORT_SYMBOL(drm_mode_config_cleanup);
+
+/**
+ * drm_crtc_set_config - set a new config from userspace
+ * @crtc: CRTC to setup
+ * @crtc_info: user provided configuration
+ * @new_mode: new mode to set
+ * @output_set: set of outputs for the new config
+ * @fb: new framebuffer
+ *
+ * LOCKING:
+ * Caller must hold mode config lock.
+ *
+ * Setup a new configuration, provided by the user in @crtc_info, and enable
+ * it.
+ *
+ * RETURNS:
+ * Zero. (FIXME)
+ */
+int drm_crtc_set_config(struct drm_crtc *crtc, struct drm_mode_crtc *crtc_info, struct drm_display_mode *new_mode, struct drm_output **output_set, struct drm_framebuffer *fb)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_crtc **save_crtcs, *new_crtc;
+	bool save_enabled = crtc->enabled;
+	bool changed;
+	struct drm_output *output;
+	int count = 0, ro;
+
+	save_crtcs = kzalloc(dev->mode_config.num_crtc * sizeof(struct drm_crtc *), GFP_KERNEL);
+	if (!save_crtcs)
+		return -ENOMEM;
+
+	if (crtc->fb != fb)
+		changed = true;
+
+	if (crtc_info->x != crtc->x || crtc_info->y != crtc->y)
+		changed = true;
+
+	if (new_mode && (crtc->mode.mode_id != new_mode->mode_id))
+		changed = true;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		save_crtcs[count++] = output->crtc;
+
+		if (output->crtc == crtc)
+			new_crtc = NULL;
+		else
+			new_crtc = output->crtc;
+
+		for (ro = 0; ro < crtc_info->count_outputs; ro++) {
+			if (output_set[ro] == output)
+				new_crtc = crtc;
+		}
+		if (new_crtc != output->crtc) {
+			changed = true;
+			output->crtc = new_crtc;
+		}
+	}
+
+	if (changed) {
+		crtc->fb = fb;
+		crtc->enabled = (new_mode != NULL);
+		if (new_mode != NULL) {
+			DRM_DEBUG("attempting to set mode from userspace\n");
+			drm_mode_debug_printmodeline(dev, new_mode);
+			if (!drm_crtc_set_mode(crtc, new_mode, crtc_info->x,
+					       crtc_info->y)) {
+				crtc->enabled = save_enabled;
+				count = 0;
+				list_for_each_entry(output, &dev->mode_config.output_list, head)
+					output->crtc = save_crtcs[count++];
+				kfree(save_crtcs);
+				return -EINVAL;
+			}
+			crtc->desired_x = crtc_info->x;
+			crtc->desired_y = crtc_info->y;
+			crtc->desired_mode = new_mode;
+		}
+		drm_disable_unused_functions(dev);
+	}
+	kfree(save_crtcs);
+	return 0;
+}
+
+/**
+ * drm_crtc_convert_to_umode - convert a drm_display_mode into a modeinfo
+ * @out: drm_mode_modeinfo struct to return to the user
+ * @in: drm_display_mode to use
+ *
+ * LOCKING:
+ * None.
+ *
+ * Convert a drm_display_mode into a drm_mode_modeinfo structure to return to
+ * the user.
+ */
+void drm_crtc_convert_to_umode(struct drm_mode_modeinfo *out, struct drm_display_mode *in)
+{
+
+	out->id = in->mode_id;
+	out->clock = in->clock;
+	out->hdisplay = in->hdisplay;
+	out->hsync_start = in->hsync_start;
+	out->hsync_end = in->hsync_end;
+	out->htotal = in->htotal;
+	out->hskew = in->hskew;
+	out->vdisplay = in->vdisplay;
+	out->vsync_start = in->vsync_start;
+	out->vsync_end = in->vsync_end;
+	out->vtotal = in->vtotal;
+	out->vscan = in->vscan;
+	out->vrefresh = in->vrefresh;
+	out->flags = in->flags;
+	out->type = in->type;
+	strncpy(out->name, in->name, DRM_DISPLAY_MODE_LEN);
+	out->name[DRM_DISPLAY_MODE_LEN-1] = 0;
+}
+
+/**
+ * drm_crtc_convert_to_umode - convert a modeinfo into a drm_display_mode
+ * @out: drm_display_mode to return to the user
+ * @in: drm_mode_modeinfo to use
+ *
+ * LOCKING:
+ * None.
+ *
+ * Convert a drmo_mode_modeinfo into a drm_display_mode structure to return to
+ * the caller.
+ */
+void drm_crtc_convert_umode(struct drm_display_mode *out, struct drm_mode_modeinfo *in)
+{
+	out->clock = in->clock;
+	out->hdisplay = in->hdisplay;
+	out->hsync_start = in->hsync_start;
+	out->hsync_end = in->hsync_end;
+	out->htotal = in->htotal;
+	out->hskew = in->hskew;
+	out->vdisplay = in->vdisplay;
+	out->vsync_start = in->vsync_start;
+	out->vsync_end = in->vsync_end;
+	out->vtotal = in->vtotal;
+	out->vscan = in->vscan;
+	out->vrefresh = in->vrefresh;
+	out->flags = in->flags;
+	out->type = in->type;
+	strncpy(out->name, in->name, DRM_DISPLAY_MODE_LEN);
+	out->name[DRM_DISPLAY_MODE_LEN-1] = 0;
+}
+
+/**
+ * drm_mode_getresources - get graphics configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Construct a set of configuration description structures and return
+ * them to the user, including CRTC, output and framebuffer configuration.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getresources(struct drm_device *dev,
+			  void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_card_res *card_res = data;
+	struct list_head *lh;
+	struct drm_framebuffer *fb;
+	struct drm_output *output;
+	struct drm_crtc *crtc;
+	struct drm_mode_modeinfo u_mode;
+	struct drm_display_mode *mode;
+	int ret = 0;
+	int mode_count= 0;
+	int output_count = 0;
+	int crtc_count = 0;
+	int fb_count = 0;
+	int copied = 0;
+
+	memset(&u_mode, 0, sizeof(struct drm_mode_modeinfo));
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	list_for_each(lh, &dev->mode_config.fb_list)
+		fb_count++;
+
+	list_for_each(lh, &dev->mode_config.crtc_list)
+		crtc_count++;
+
+	list_for_each_entry(output, &dev->mode_config.output_list,
+			    head) {
+		output_count++;
+		list_for_each(lh, &output->modes)
+			mode_count++;
+	}
+	list_for_each(lh, &dev->mode_config.usermode_list)
+		mode_count++;
+
+	if (card_res->count_modes == 0) {
+		DRM_DEBUG("probing modes %dx%d\n", dev->mode_config.max_width, dev->mode_config.max_height);
+		drm_crtc_probe_output_modes(dev, dev->mode_config.max_width, dev->mode_config.max_height);
+		mode_count = 0;
+		list_for_each_entry(output, &dev->mode_config.output_list, head) {
+			list_for_each(lh, &output->modes)
+				mode_count++;
+		}
+		list_for_each(lh, &dev->mode_config.usermode_list)
+			mode_count++;
+	}
+
+	/* handle this in 4 parts */
+	/* FBs */
+	if (card_res->count_fbs >= fb_count) {
+		copied = 0;
+		list_for_each_entry(fb, &dev->mode_config.fb_list, head) {
+			if (put_user(fb->id, card_res->fb_id + copied))
+				return -EFAULT;
+			copied++;
+		}
+	}
+	card_res->count_fbs = fb_count;
+
+	/* CRTCs */
+	if (card_res->count_crtcs >= crtc_count) {
+		copied = 0;
+		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head){
+			DRM_DEBUG("CRTC ID is %d\n", crtc->id);
+			if (put_user(crtc->id, card_res->crtc_id + copied))
+				return -EFAULT;
+			copied++;
+		}
+	}
+	card_res->count_crtcs = crtc_count;
+
+
+	/* Outputs */
+	if (card_res->count_outputs >= output_count) {
+		copied = 0;
+		list_for_each_entry(output, &dev->mode_config.output_list,
+				    head) {
+ 			DRM_DEBUG("OUTPUT ID is %d\n", output->id);
+			if (put_user(output->id, card_res->output_id + copied))
+				return -EFAULT;
+			copied++;
+		}
+	}
+	card_res->count_outputs = output_count;
+
+	/* Modes */
+	if (card_res->count_modes >= mode_count) {
+		copied = 0;
+		list_for_each_entry(output, &dev->mode_config.output_list,
+				    head) {
+			list_for_each_entry(mode, &output->modes, head) {
+				drm_crtc_convert_to_umode(&u_mode, mode);
+				if (copy_to_user(card_res->modes + copied,
+						 &u_mode, sizeof(u_mode)))
+					return -EFAULT;
+				copied++;
+			}
+		}
+		/* add in user modes */
+		list_for_each_entry(mode, &dev->mode_config.usermode_list, head) {
+			drm_crtc_convert_to_umode(&u_mode, mode);
+			if (copy_to_user(card_res->modes + copied, &u_mode,
+					 sizeof(u_mode)))
+				return -EFAULT;
+			copied++;
+		}
+	}
+	card_res->count_modes = mode_count;
+
+	DRM_DEBUG("Counted %d %d %d\n", card_res->count_crtcs,
+		  card_res->count_outputs,
+		  card_res->count_modes);
+
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_getcrtc - get CRTC configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Construct a CRTC configuration structure to return to the user.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getcrtc(struct drm_device *dev,
+		     void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_crtc *crtc_resp = data;
+	struct drm_crtc *crtc;
+	struct drm_output *output;
+	int ocount;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+	crtc = idr_find(&dev->mode_config.crtc_idr, crtc_resp->crtc_id);
+	if (!crtc || (crtc->id != crtc_resp->crtc_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	crtc_resp->x = crtc->x;
+	crtc_resp->y = crtc->y;
+
+	if (crtc->fb)
+		crtc_resp->fb_id = crtc->fb->id;
+	else
+		crtc_resp->fb_id = 0;
+
+	crtc_resp->outputs = 0;
+	if (crtc->enabled) {
+
+		crtc_resp->mode = crtc->mode.mode_id;
+		ocount = 0;
+		list_for_each_entry(output, &dev->mode_config.output_list, head) {
+			if (output->crtc == crtc)
+				crtc_resp->outputs |= 1 << (ocount++);
+		}
+	} else {
+		crtc_resp->mode = 0;
+	}
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_getoutput - get output configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Construct a output configuration structure to return to the user.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getoutput(struct drm_device *dev,
+		       void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_get_output *out_resp = data;
+	struct drm_output *output;
+	struct drm_display_mode *mode;
+	int mode_count = 0;
+	int props_count = 0;
+	int ret = 0;
+	int copied = 0;
+	int i;
+
+	DRM_DEBUG("output id %d:\n", out_resp->output);
+
+	mutex_lock(&dev->mode_config.mutex);
+	output= idr_find(&dev->mode_config.crtc_idr, out_resp->output);
+	if (!output || (output->id != out_resp->output)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	list_for_each_entry(mode, &output->modes, head)
+		mode_count++;
+
+	for (i = 0; i < DRM_OUTPUT_MAX_UMODES; i++)
+		if (output->user_mode_ids[i] != 0)
+			mode_count++;
+
+	for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+		if (output->property_ids[i] != 0) {
+			props_count++;
+		}
+	}
+
+	strncpy(out_resp->name, output->name, DRM_OUTPUT_NAME_LEN);
+	out_resp->name[DRM_OUTPUT_NAME_LEN-1] = 0;
+
+	out_resp->mm_width = output->mm_width;
+	out_resp->mm_height = output->mm_height;
+	out_resp->subpixel = output->subpixel_order;
+	out_resp->connection = output->status;
+	if (output->crtc)
+		out_resp->crtc = output->crtc->id;
+	else
+		out_resp->crtc = 0;
+
+	out_resp->crtcs = output->possible_crtcs;
+	out_resp->clones = output->possible_clones;
+
+	if ((out_resp->count_modes >= mode_count) && mode_count) {
+		copied = 0;
+		list_for_each_entry(mode, &output->modes, head) {
+			out_resp->modes[copied++] = mode->mode_id;
+		}
+		for (i = 0; i < DRM_OUTPUT_MAX_UMODES; i++) {
+			if (output->user_mode_ids[i] != 0) {
+				if (put_user(output->user_mode_ids[i], out_resp->modes + copied))
+					return -EFAULT;
+				copied++;
+			}
+		}
+	}
+	out_resp->count_modes = mode_count;
+
+	if ((out_resp->count_props >= props_count) && props_count) {
+		copied = 0;
+		for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+			if (output->property_ids[i] != 0) {
+				if (put_user(output->property_ids[i], out_resp->props + copied)) {
+					ret = -EFAULT;
+					goto out;
+				}
+
+				if (put_user(output->property_values[i], out_resp->prop_values + copied)) {
+					ret = -EFAULT;
+					goto out;
+				}
+				copied++;
+			}
+		}
+	}
+	out_resp->count_props = props_count;
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_setcrtc - set CRTC configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Build a new CRTC configuration based on user request.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_setcrtc(struct drm_device *dev,
+		     void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_crtc *crtc_req = data;
+	struct drm_crtc *crtc;
+	struct drm_output **output_set = NULL, *output;
+	struct drm_display_mode *mode;
+	struct drm_framebuffer *fb = NULL;
+	int ret = 0;
+	int i;
+
+	mutex_lock(&dev->mode_config.mutex);
+	crtc = idr_find(&dev->mode_config.crtc_idr, crtc_req->crtc_id);
+	if (!crtc || (crtc->id != crtc_req->crtc_id)) {
+		DRM_DEBUG("Unknown CRTC ID %d\n", crtc_req->crtc_id);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (crtc_req->mode) {
+		/* if we have a mode we need a framebuffer */
+		if (crtc_req->fb_id) {
+			fb = idr_find(&dev->mode_config.crtc_idr, crtc_req->fb_id);
+			if (!fb || (fb->id != crtc_req->fb_id)) {
+				DRM_DEBUG("Unknown FB ID%d\n", crtc_req->fb_id);
+				ret = -EINVAL;
+				goto out;
+			}
+		}
+		mode = idr_find(&dev->mode_config.crtc_idr, crtc_req->mode);
+		if (!mode || (mode->mode_id != crtc_req->mode)) {
+			struct drm_output *output;
+
+			list_for_each_entry(output,
+					    &dev->mode_config.output_list,
+					    head) {
+				list_for_each_entry(mode, &output->modes,
+						    head) {
+					drm_mode_debug_printmodeline(dev,
+								     mode);
+				}
+			}
+
+			DRM_DEBUG("Unknown mode id %d, %p\n", crtc_req->mode, mode);
+			ret = -EINVAL;
+			goto out;
+		}
+	} else
+		mode = NULL;
+
+	if (crtc_req->count_outputs == 0 && mode) {
+		DRM_DEBUG("Count outputs is 0 but mode set\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (crtc_req->count_outputs > 0 && !mode && !fb) {
+		DRM_DEBUG("Count outputs is %d but no mode or fb set\n", crtc_req->count_outputs);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (crtc_req->count_outputs > 0) {
+		u32 out_id;
+		output_set = kmalloc(crtc_req->count_outputs *
+				     sizeof(struct drm_output *), GFP_KERNEL);
+		if (!output_set) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		for (i = 0; i < crtc_req->count_outputs; i++) {
+			if (get_user(out_id, &crtc_req->set_outputs[i])) {
+				ret = -EFAULT;
+				goto out;
+			}
+
+			output = idr_find(&dev->mode_config.crtc_idr, out_id);
+			if (!output || (out_id != output->id)) {
+				DRM_DEBUG("Output id %d unknown\n", out_id);
+				ret = -EINVAL;
+				goto out;
+			}
+
+			output_set[i] = output;
+		}
+	}
+
+	ret = drm_crtc_set_config(crtc, crtc_req, mode, output_set, fb);
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_addfb - add an FB to the graphics configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Add a new FB to the specified CRTC, given a user request.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_addfb(struct drm_device *dev,
+		   void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_fb_cmd *r = data;
+	struct drm_mode_config *config = &dev->mode_config;
+	struct drm_framebuffer *fb;
+	struct drm_buffer_object *bo;
+	struct drm_crtc *crtc;
+	int ret = 0;
+
+	if ((config->min_width > r->width) || (r->width > config->max_width)) {
+		DRM_ERROR("mode new framebuffer width not within limits\n");
+		return -EINVAL;
+	}
+	if ((config->min_height > r->height) || (r->height > config->max_height)) {
+		DRM_ERROR("mode new framebuffer height not within limits\n");
+		return -EINVAL;
+	}
+
+	mutex_lock(&dev->mode_config.mutex);
+	/* TODO check limits are okay */
+	ret = drm_get_buffer_object(dev, &bo, r->handle);
+	if (ret || !bo) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* TODO check buffer is sufficently large */
+	/* TODO setup destructor callback */
+
+	fb = drm_framebuffer_create(dev);
+	if (!fb) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	fb->width = r->width;
+	fb->height = r->height;
+	fb->pitch = r->pitch;
+	fb->bits_per_pixel = r->bpp;
+	fb->depth = r->depth;
+	fb->offset = bo->offset;
+	fb->bo = bo;
+
+	r->buffer_id = fb->id;
+
+	list_add(&fb->filp_head, &file_priv->fbs);
+
+	/* FIXME: bind the fb to the right crtc */
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		crtc->fb = fb;
+		dev->driver->fb_probe(dev, crtc);
+	}
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_rmfb - remove an FB from the configuration
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Remove the FB specified by the user.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_rmfb(struct drm_device *dev,
+		   void *data, struct drm_file *file_priv)
+{
+	struct drm_framebuffer *fb = 0;
+	uint32_t *id = data;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+	fb = idr_find(&dev->mode_config.crtc_idr, *id);
+	/* TODO check that we realy get a framebuffer back. */
+	if (!fb || (*id != fb->id)) {
+		DRM_ERROR("mode invalid framebuffer id\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* TODO check if we own the buffer */
+	/* TODO release all crtc connected to the framebuffer */
+	/* bind the fb to the crtc for now */
+	/* TODO unhock the destructor from the buffer object */
+
+	if (fb->bo->type != drm_bo_type_kernel)
+		drm_framebuffer_destroy(fb);
+	else
+		dev->driver->fb_remove(dev, drm_crtc_from_fb(dev, fb));
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_mode_getfb - get FB info
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * LOCKING:
+ * Caller? (FIXME)
+ *
+ * Lookup the FB given its ID and return info about it.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_getfb(struct drm_device *dev,
+		   void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_fb_cmd *r = data;
+	struct drm_framebuffer *fb;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+	fb = idr_find(&dev->mode_config.crtc_idr, r->buffer_id);
+	if (!fb || (r->buffer_id != fb->id)) {
+		DRM_ERROR("invalid framebuffer id\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	r->height = fb->height;
+	r->width = fb->width;
+	r->depth = fb->depth;
+	r->bpp = fb->bits_per_pixel;
+	r->handle = fb->bo->base.hash.key;
+	r->pitch = fb->pitch;
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_fb_release - remove and free the FBs on this file
+ * @filp: file * from the ioctl
+ *
+ * LOCKING:
+ * Takes mode config lock.
+ *
+ * Destroy all the FBs associated with @filp.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+void drm_fb_release(struct file *filp)
+{
+	struct drm_file *priv = filp->private_data;
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_framebuffer *fb, *tfb;
+
+	mutex_lock(&dev->mode_config.mutex);
+	list_for_each_entry_safe(fb, tfb, &priv->fbs, filp_head) {
+		list_del(&fb->filp_head);
+		if (fb->bo->type != drm_bo_type_kernel)
+			drm_framebuffer_destroy(fb);
+		else
+			dev->driver->fb_remove(dev, drm_crtc_from_fb(dev, fb));
+	}
+	mutex_unlock(&dev->mode_config.mutex);
+}
+
+/*
+ *
+ */
+void drm_mode_addmode(struct drm_device *dev, struct drm_display_mode *user_mode)
+{
+	user_mode->type |= DRM_MODE_TYPE_USERDEF;
+
+	user_mode->output_count = 0;
+	list_add(&user_mode->head, &dev->mode_config.usermode_list);
+}
+EXPORT_SYMBOL(drm_mode_addmode);
+
+int drm_mode_rmmode(struct drm_device *dev, struct drm_display_mode *mode)
+{
+	struct drm_display_mode *t;
+	int ret = -EINVAL;
+	list_for_each_entry(t, &dev->mode_config.usermode_list, head) {
+		if (t == mode) {
+			list_del(&mode->head);
+			drm_mode_destroy(dev, mode);
+			ret = 0;
+			break;
+		}
+	}
+	return ret;
+}
+EXPORT_SYMBOL(drm_mode_rmmode);
+
+static int drm_mode_attachmode(struct drm_device *dev,
+			       struct drm_output *output,
+			       struct drm_display_mode *mode)
+{
+	int ret = 0;
+	int i;
+
+	for (i = 0; i < DRM_OUTPUT_MAX_UMODES; i++) {
+		if (output->user_mode_ids[i] == 0) {
+			output->user_mode_ids[i] = mode->mode_id;
+			mode->output_count++;
+			break;
+		}
+	}
+
+	if (i == DRM_OUTPUT_MAX_UMODES)
+		ret = -ENOSPC;
+
+	return ret;
+}
+
+int drm_mode_attachmode_crtc(struct drm_device *dev, struct drm_crtc *crtc,
+			     struct drm_display_mode *mode)
+{
+	struct drm_output *output;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		if (output->crtc == crtc)
+			drm_mode_attachmode(dev, output, mode);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_mode_attachmode_crtc);
+
+static int drm_mode_detachmode(struct drm_device *dev,
+			       struct drm_output *output,
+			       struct drm_display_mode *mode)
+{
+	int found = 0;
+	int ret = 0, i;
+
+	for (i = 0; i < DRM_OUTPUT_MAX_UMODES; i++) {
+		if (output->user_mode_ids[i] == mode->mode_id) {
+			output->user_mode_ids[i] = 0;
+			mode->output_count--;
+			found = 1;
+		}
+	}
+
+	if (!found)
+		ret = -EINVAL;
+
+	return ret;
+}
+
+int drm_mode_detachmode_crtc(struct drm_device *dev, struct drm_display_mode *mode)
+{
+	struct drm_output *output;
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		drm_mode_detachmode(dev, output, mode);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(drm_mode_detachmode_crtc);
+
+/**
+ * drm_fb_addmode - adds a user defined mode
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * Adds a user specified mode to the kernel.
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * writes new mode id into arg.
+ * Zero on success, errno on failure.
+ */
+int drm_mode_addmode_ioctl(struct drm_device *dev,
+			   void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_modeinfo *new_mode = data;
+	struct drm_display_mode *user_mode;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+	user_mode = drm_mode_create(dev);
+	if (!user_mode) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	drm_crtc_convert_umode(user_mode, new_mode);
+
+	drm_mode_addmode(dev, user_mode);
+	new_mode->id = user_mode->mode_id;
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_fb_rmmode - removes a user defined mode
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * Remove the user defined mode specified by the user.
+ *
+ * Called by the user via ioctl
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_rmmode_ioctl(struct drm_device *dev,
+			  void *data, struct drm_file *file_priv)
+{
+	uint32_t *id = data;
+	struct drm_display_mode *mode;
+	int ret = -EINVAL;
+
+	mutex_lock(&dev->mode_config.mutex);
+	mode = idr_find(&dev->mode_config.crtc_idr, *id);
+	if (!mode || (*id != mode->mode_id)) {
+		goto out;
+	}
+
+	if (!(mode->type & DRM_MODE_TYPE_USERDEF)) {
+		goto out;
+	}
+
+	if (mode->output_count) {
+		goto out;
+	}
+
+	ret = drm_mode_rmmode(dev, mode);
+
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+/**
+ * drm_fb_attachmode - Attach a user mode to an output
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * This attaches a user specified mode to an output.
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_attachmode_ioctl(struct drm_device *dev,
+			      void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_mode_cmd *mode_cmd = data;
+	struct drm_output *output;
+	struct drm_display_mode *mode;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	mode = idr_find(&dev->mode_config.crtc_idr, mode_cmd->mode_id);
+	if (!mode || (mode->mode_id != mode_cmd->mode_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	output = idr_find(&dev->mode_config.crtc_idr, mode_cmd->output_id);
+	if (!output || (output->id != mode_cmd->output_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ret = drm_mode_attachmode(dev, output, mode);
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+
+/**
+ * drm_fb_detachmode - Detach a user specified mode from an output
+ * @inode: inode from the ioctl
+ * @filp: file * from the ioctl
+ * @cmd: cmd from ioctl
+ * @arg: arg from ioctl
+ *
+ * Called by the user via ioctl.
+ *
+ * RETURNS:
+ * Zero on success, errno on failure.
+ */
+int drm_mode_detachmode_ioctl(struct drm_device *dev,
+			      void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_mode_cmd *mode_cmd = data;
+	struct drm_output *output;
+	struct drm_display_mode *mode;
+	int ret = 0;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	mode = idr_find(&dev->mode_config.crtc_idr, mode_cmd->mode_id);
+	if (!mode || (mode->mode_id != mode_cmd->mode_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	output = idr_find(&dev->mode_config.crtc_idr, mode_cmd->output_id);
+	if (!output || (output->id != mode_cmd->output_id)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+
+	ret = drm_mode_detachmode(dev, output, mode);
+out:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
+
+struct drm_property *drm_property_create(struct drm_device *dev, int flags,
+					 const char *name, int num_values)
+{
+	struct drm_property *property = NULL;
+
+	property = kzalloc(sizeof(struct drm_output), GFP_KERNEL);
+	if (!property)
+		return NULL;
+
+	property->values = kzalloc(sizeof(uint32_t)*num_values, GFP_KERNEL);
+	if (!property->values)
+		goto fail;
+
+	property->id = drm_idr_get(dev, property);
+	property->flags = flags;
+	property->num_values = num_values;
+	INIT_LIST_HEAD(&property->enum_list);
+
+	if (name)
+		strncpy(property->name, name, DRM_PROP_NAME_LEN);
+
+	list_add_tail(&property->head, &dev->mode_config.property_list);
+	return property;
+fail:
+	kfree(property);
+	return NULL;
+}
+EXPORT_SYMBOL(drm_property_create);
+
+int drm_property_add_enum(struct drm_property *property, int index,
+			  uint32_t value, const char *name)
+{
+	struct drm_property_enum *prop_enum;
+
+	if (!(property->flags & DRM_MODE_PROP_ENUM))
+		return -EINVAL;
+
+	if (!list_empty(&property->enum_list)) {
+		list_for_each_entry(prop_enum, &property->enum_list, head) {
+			if (prop_enum->value == value) {
+				strncpy(prop_enum->name, name, DRM_PROP_NAME_LEN);
+				prop_enum->name[DRM_PROP_NAME_LEN-1] = '\0';
+				return 0;
+			}
+		}
+	}
+
+	prop_enum = kzalloc(sizeof(struct drm_property_enum), GFP_KERNEL);
+	if (!prop_enum)
+		return -ENOMEM;
+
+	strncpy(prop_enum->name, name, DRM_PROP_NAME_LEN);
+	prop_enum->name[DRM_PROP_NAME_LEN-1] = '\0';
+	prop_enum->value = value;
+
+	property->values[index] = value;
+	list_add_tail(&prop_enum->head, &property->enum_list);
+	return 0;
+}
+EXPORT_SYMBOL(drm_property_add_enum);
+
+void drm_property_destroy(struct drm_device *dev, struct drm_property *property)
+{
+	struct drm_property_enum *prop_enum, *pt;
+
+	list_for_each_entry_safe(prop_enum, pt, &property->enum_list, head) {
+		list_del(&prop_enum->head);
+		kfree(prop_enum);
+	}
+
+	kfree(property->values);
+	drm_idr_put(dev, property->id);
+	list_del(&property->head);
+	kfree(property);
+}
+EXPORT_SYMBOL(drm_property_destroy);
+
+
+int drm_output_attach_property(struct drm_output *output,
+			       struct drm_property *property, int init_val)
+{
+	int i;
+
+	for (i = 0; i < DRM_OUTPUT_MAX_PROPERTY; i++) {
+		if (output->property_ids[i] == 0) {
+			output->property_ids[i] = property->id;
+			output->property_values[i] = init_val;
+			break;
+		}
+	}
+
+	if (i == DRM_OUTPUT_MAX_PROPERTY)
+		return -EINVAL;
+	return 0;
+}
+EXPORT_SYMBOL(drm_output_attach_property);
+
+int drm_mode_getproperty_ioctl(struct drm_device *dev,
+			       void *data, struct drm_file *file_priv)
+{
+	struct drm_mode_get_property *out_resp = data;
+	struct drm_property *property;
+	int enum_count = 0;
+	int value_count = 0;
+	int ret = 0, i;
+	int copied;
+	struct drm_property_enum *prop_enum;
+
+	mutex_lock(&dev->mode_config.mutex);
+	property = idr_find(&dev->mode_config.crtc_idr, out_resp->prop_id);
+	if (!property || (property->id != out_resp->prop_id)) {
+		ret = -EINVAL;
+		goto done;
+	}
+
+
+	list_for_each_entry(prop_enum, &property->enum_list, head)
+		enum_count++;
+
+	value_count = property->num_values;
+
+	strncpy(out_resp->name, property->name, DRM_PROP_NAME_LEN);
+	out_resp->name[DRM_PROP_NAME_LEN-1] = 0;
+	out_resp->flags = property->flags;
+
+	if ((out_resp->count_values >= value_count) && value_count) {
+		for (i = 0; i < value_count; i++) {
+			if (put_user(property->values[i], out_resp->values + i)) {
+				ret = -EFAULT;
+				goto done;
+			}
+		}
+	}
+	out_resp->count_values = value_count;
+
+	if ((out_resp->count_enums >= enum_count) && enum_count) {
+		copied = 0;
+		list_for_each_entry(prop_enum, &property->enum_list, head) {
+			if (put_user(prop_enum->value, &out_resp->enums[copied].value)) {
+				ret = -EFAULT;
+				goto done;
+			}
+
+			if (copy_to_user(&out_resp->enums[copied].name,
+					 prop_enum->name, DRM_PROP_NAME_LEN)) {
+				ret = -EFAULT;
+				goto done;
+			}
+			copied++;
+		}
+	}
+	out_resp->count_enums = enum_count;
+
+done:
+	mutex_unlock(&dev->mode_config.mutex);
+	return ret;
+}
Index: linux-2.6.27/drivers/gpu/drm/drm_drv.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_drv.c	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_drv.c	2009-02-05 13:29:33.000000000 +0000
@@ -49,6 +49,9 @@
 #include "drmP.h"
 #include "drm_core.h"
 
+static void drm_cleanup(struct drm_device * dev);
+int drm_fb_loaded = 0;
+
 static int drm_version(struct drm_device *dev, void *data,
 		       struct drm_file *file_priv);
 
@@ -113,16 +116,48 @@
 
 	DRM_IOCTL_DEF(DRM_IOCTL_SG_ALLOC, drm_sg_alloc_ioctl, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
 	DRM_IOCTL_DEF(DRM_IOCTL_SG_FREE, drm_sg_free, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
-
 	DRM_IOCTL_DEF(DRM_IOCTL_WAIT_VBLANK, drm_wait_vblank, 0),
-
-	DRM_IOCTL_DEF(DRM_IOCTL_MODESET_CTL, drm_modeset_ctl, 0),
-
 	DRM_IOCTL_DEF(DRM_IOCTL_UPDATE_DRAW, drm_update_drawable_info, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
-
-	DRM_IOCTL_DEF(DRM_IOCTL_GEM_CLOSE, drm_gem_close_ioctl, 0),
-	DRM_IOCTL_DEF(DRM_IOCTL_GEM_FLINK, drm_gem_flink_ioctl, DRM_AUTH),
-	DRM_IOCTL_DEF(DRM_IOCTL_GEM_OPEN, drm_gem_open_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETRESOURCES, drm_mode_getresources, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETCRTC, drm_mode_getcrtc, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETOUTPUT, drm_mode_getoutput, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETCRTC, drm_mode_setcrtc, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ADDFB, drm_mode_addfb, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_RMFB, drm_mode_rmfb, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETFB, drm_mode_getfb, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ADDMODE, drm_mode_addmode_ioctl, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_RMMODE, drm_mode_rmmode_ioctl, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ATTACHMODE, drm_mode_attachmode_ioctl, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DETACHMODE, drm_mode_detachmode_ioctl, DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPERTY, drm_mode_getproperty_ioctl, DRM_MASTER | DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_INIT, drm_mm_init_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_TAKEDOWN, drm_mm_takedown_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_LOCK, drm_mm_lock_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MM_UNLOCK, drm_mm_unlock_ioctl,
+		      DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_CREATE, drm_fence_create_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_REFERENCE, drm_fence_reference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_UNREFERENCE, drm_fence_unreference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_SIGNALED, drm_fence_signaled_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_FLUSH, drm_fence_flush_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_WAIT, drm_fence_wait_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_EMIT, drm_fence_emit_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FENCE_BUFFERS, drm_fence_buffers_ioctl, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_CREATE, drm_bo_create_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_MAP, drm_bo_map_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_UNMAP, drm_bo_unmap_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_REFERENCE, drm_bo_reference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_UNREFERENCE, drm_bo_unreference_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_SETSTATUS, drm_bo_setstatus_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_INFO, drm_bo_info_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_WAIT_IDLE, drm_bo_wait_idle_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_BO_VERSION, drm_bo_version_ioctl, 0),
 };
 
 #define DRM_CORE_IOCTL_COUNT	ARRAY_SIZE( drm_ioctls )
@@ -164,7 +199,12 @@
 	drm_drawable_free_all(dev);
 	del_timer(&dev->timer);
 
-	/* Clear pid list */
+	if (dev->unique) {
+		drm_free(dev->unique, strlen(dev->unique) + 1, DRM_MEM_DRIVER);
+		dev->unique = NULL;
+		dev->unique_len = 0;
+	}
+
 	if (dev->magicfree.next) {
 		list_for_each_entry_safe(pt, next, &dev->magicfree, head) {
 			list_del(&pt->head);
@@ -236,12 +276,24 @@
 		dev->lock.file_priv = NULL;
 		wake_up_interruptible(&dev->lock.lock_queue);
 	}
+	dev->dev_mapping = NULL;
 	mutex_unlock(&dev->struct_mutex);
 
 	DRM_DEBUG("lastclose completed\n");
 	return 0;
 }
 
+void drm_cleanup_pci(struct pci_dev *pdev)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+
+	pci_set_drvdata(pdev, NULL);
+	pci_release_regions(pdev);
+	if (dev)
+		drm_cleanup(dev);
+}
+EXPORT_SYMBOL(drm_cleanup_pci);
+
 /**
  * Module initialization. Called via init_module at module load time, or via
  * linux/init/main.c (this is not currently supported).
@@ -255,26 +307,68 @@
  * Expands the \c DRIVER_PREINIT and \c DRIVER_POST_INIT macros before and
  * after the initialization for driver customization.
  */
-int drm_init(struct drm_driver *driver)
+int drm_init(struct drm_driver *driver,
+		       struct pci_device_id *pciidlist)
 {
-	struct pci_dev *pdev = NULL;
+	struct pci_dev *pdev;
 	struct pci_device_id *pid;
-	int i;
+	int rc, i;
 
 	DRM_DEBUG("\n");
 
-	for (i = 0; driver->pci_driver.id_table[i].vendor != 0; i++) {
-		pid = (struct pci_device_id *)&driver->pci_driver.id_table[i];
+	for (i = 0; (pciidlist[i].vendor != 0) && !drm_fb_loaded; i++) {
+		pid = &pciidlist[i];
 
 		pdev = NULL;
 		/* pass back in pdev to account for multiple identical cards */
 		while ((pdev =
 			pci_get_subsys(pid->vendor, pid->device, pid->subvendor,
-				       pid->subdevice, pdev)) != NULL) {
-			/* stealth mode requires a manual probe */
-			pci_dev_get(pdev);
-			drm_get_dev(pdev, pid, driver);
+				       pid->subdevice, pdev))) {
+			/* Are there device class requirements? */
+			if ((pid->class != 0)
+				&& ((pdev->class & pid->class_mask) != pid->class)) {
+				continue;
+			}
+			/* is there already a driver loaded, or (short circuit saves work) */
+			/* does something like VesaFB have control of the memory region? */
+			if (pci_dev_driver(pdev)
+			    || pci_request_regions(pdev, "DRM scan")) {
+				/* go into stealth mode */
+				drm_fb_loaded = 1;
+				pci_dev_put(pdev);
+				break;
+			}
+			/* no fbdev or vesadev, put things back and wait for normal probe */
+			pci_release_regions(pdev);
+		}
+	}
+
+	if (!drm_fb_loaded)
+		return pci_register_driver(&driver->pci_driver);
+	else {
+		for (i = 0; pciidlist[i].vendor != 0; i++) {
+			pid = &pciidlist[i];
+
+			pdev = NULL;
+			/* pass back in pdev to account for multiple identical cards */
+			while ((pdev =
+				pci_get_subsys(pid->vendor, pid->device,
+					       pid->subvendor, pid->subdevice,
+					       pdev))) {
+				/* Are there device class requirements? */
+				if ((pid->class != 0)
+					&& ((pdev->class & pid->class_mask) != pid->class)) {
+					continue;
+				}
+				/* stealth mode requires a manual probe */
+				pci_dev_get(pdev);
+				if ((rc = drm_get_dev(pdev, &pciidlist[i], driver))) {
+					pci_dev_put(pdev);
+					return rc;
+				}
+			}
 		}
+		DRM_INFO("Used old pci detect: framebuffer loaded\n");
 	}
 	return 0;
 }
@@ -298,6 +392,7 @@
 	}
 
 	drm_lastclose(dev);
+	drm_ctxbitmap_cleanup(dev);
 
 	if (drm_core_has_MTRR(dev) && drm_core_has_AGP(dev) &&
 	    dev->agp && dev->agp->agp_mtrr >= 0) {
@@ -308,6 +403,9 @@
 		DRM_DEBUG("mtrr_del=%d\n", retval);
 	}
 
+	drm_bo_driver_finish(dev);
+	drm_fence_manager_takedown(dev);
+
 	if (drm_core_has_AGP(dev) && dev->agp) {
 		drm_free(dev->agp, sizeof(*dev->agp), DRM_MEM_AGPLISTS);
 		dev->agp = NULL;
@@ -317,7 +415,12 @@
 		dev->driver->unload(dev);
 
 	drm_ht_remove(&dev->map_hash);
-	drm_ctxbitmap_cleanup(dev);
+	drm_mm_takedown(&dev->offset_manager);
+	drm_ht_remove(&dev->object_hash);
+
+
+	if (!drm_fb_loaded)
+		pci_disable_device(dev->pdev);
 
 	drm_put_minor(&dev->primary);
 	if (drm_put_dev(dev))
Index: linux-2.6.27/drivers/gpu/drm/drm_edid.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_edid.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,519 @@
+/*
+ * Copyright (c) 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ *
+ * DDC probing routines (drm_ddc_read & drm_do_probe_ddc_edid) originally from
+ * FB layer.
+ *   Copyright (C) 2006 Dennis Munsie <dmunsie@cecropia.com>
+ */
+#include "drmP.h"
+#include <linux/i2c-algo-bit.h>
+#include "drm_edid.h"
+
+#include <acpi/acpi_drivers.h>
+
+/* Valid EDID header has these bytes */
+static u8 edid_header[] = { 0x00, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00 };
+
+int drm_get_acpi_edid(char *method, char *edid, ssize_t length)
+{
+	int status;
+	struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };
+	union acpi_object *obj;
+	union acpi_object arg0 = { ACPI_TYPE_INTEGER };
+	struct acpi_object_list args = { 1, &arg0 };
+
+	if (length == 128)
+		arg0.integer.value = 1;
+	else if (length == 256)
+		arg0.integer.value = 2;
+	else
+		return -EINVAL;
+
+	status = acpi_evaluate_object(NULL, method, &args, &buffer);
+	if (ACPI_FAILURE(status))
+		return -ENODEV;
+
+	obj = buffer.pointer;
+
+	if (obj && obj->type == ACPI_TYPE_BUFFER)
+		memcpy(edid, obj->buffer.pointer, obj->buffer.length);
+	else {
+		printk(KERN_ERR PREFIX "Invalid _DDC data\n");
+		status = -EFAULT;
+		kfree(obj);
+	}
+
+	return status;
+}
+EXPORT_SYMBOL(drm_get_acpi_edid);
+
+/**
+ * edid_valid - sanity check EDID data
+ * @edid: EDID data
+ *
+ * Sanity check the EDID block by looking at the header, the version number
+ * and the checksum.  Return 0 if the EDID doesn't check out, or 1 if it's
+ * valid.
+ */
+static bool edid_valid(struct edid *edid)
+{
+	int i;
+	u8 csum = 0;
+	u8 *raw_edid = (u8 *)edid;
+
+	if (memcmp(edid->header, edid_header, sizeof(edid_header)))
+		goto bad;
+	if (edid->version != 1)
+		goto bad;
+	if (edid->revision <= 0 || edid->revision > 3)
+		goto bad;
+
+	for (i = 0; i < EDID_LENGTH; i++)
+		csum += raw_edid[i];
+	if (csum)
+		goto bad;
+
+	return 1;
+
+bad:
+	return 0;
+}
+
+/**
+ * drm_mode_std - convert standard mode info (width, height, refresh) into mode
+ * @t: standard timing params
+ *
+ * Take the standard timing params (in this case width, aspect, and refresh)
+ * and convert them into a real mode using CVT.
+ *
+ * Punts for now, but should eventually use the FB layer's CVT based mode
+ * generation code.
+ */
+struct drm_display_mode *drm_mode_std(struct drm_device *dev,
+				      struct std_timing *t)
+{
+//	struct fb_videomode mode;
+
+//	fb_find_mode_cvt(&mode, 0, 0);
+	/* JJJ:  convert to drm_display_mode */
+	struct drm_display_mode *mode;
+	int hsize = t->hsize * 8 + 248, vsize;
+
+	mode = drm_mode_create(dev);
+	if (!mode)
+		return NULL;
+
+	if (t->aspect_ratio == 0)
+		vsize = (hsize * 10) / 16;
+	else if (t->aspect_ratio == 1)
+		vsize = (hsize * 3) / 4;
+	else if (t->aspect_ratio == 2)
+		vsize = (hsize * 4) / 5;
+	else
+		vsize = (hsize * 9) / 16;
+
+	drm_mode_set_name(mode);
+
+	return mode;
+}
+
+/**
+ * drm_mode_detailed - create a new mode from an EDID detailed timing section
+ * @timing: EDID detailed timing info
+ * @preferred: is this a preferred mode?
+ *
+ * An EDID detailed timing block contains enough info for us to create and
+ * return a new struct drm_display_mode.  The @preferred flag will be set
+ * if this is the display's preferred timing, and we'll use it to indicate
+ * to the other layers that this mode is desired.
+ */
+struct drm_display_mode *drm_mode_detailed(struct drm_device *dev,
+					   struct detailed_timing *timing)
+{
+	struct drm_display_mode *mode;
+	struct detailed_pixel_timing *pt = &timing->data.pixel_data;
+
+	if (pt->stereo) {
+		printk(KERN_WARNING "stereo mode not supported\n");
+		return NULL;
+	}
+	if (!pt->separate_sync) {
+		printk(KERN_WARNING "integrated sync not supported\n");
+		return NULL;
+	}
+
+	mode = drm_mode_create(dev);
+	if (!mode)
+		return NULL;
+
+	mode->type = DRM_MODE_TYPE_DRIVER;
+	mode->clock = timing->pixel_clock * 10;
+
+	mode->hdisplay = (pt->hactive_hi << 8) | pt->hactive_lo;
+	mode->hsync_start = mode->hdisplay + ((pt->hsync_offset_hi << 8) |
+					      pt->hsync_offset_lo);
+	mode->hsync_end = mode->hsync_start +
+		((pt->hsync_pulse_width_hi << 8) |
+		 pt->hsync_pulse_width_lo);
+	mode->htotal = mode->hdisplay + ((pt->hblank_hi << 8) | pt->hblank_lo);
+
+	mode->vdisplay = (pt->vactive_hi << 8) | pt->vactive_lo;
+	mode->vsync_start = mode->vdisplay + ((pt->vsync_offset_hi << 8) |
+					      pt->vsync_offset_lo);
+	mode->vsync_end = mode->vsync_start +
+		((pt->vsync_pulse_width_hi << 8) |
+		 pt->vsync_pulse_width_lo);
+	mode->vtotal = mode->vdisplay + ((pt->vblank_hi << 8) | pt->vblank_lo);
+
+	drm_mode_set_name(mode);
+
+	if (pt->interlaced)
+		mode->flags |= V_INTERLACE;
+
+	mode->flags |= pt->hsync_positive ? V_PHSYNC : V_NHSYNC;
+	mode->flags |= pt->vsync_positive ? V_PVSYNC : V_NVSYNC;
+
+	return mode;
+}
+
+/*
+ * Detailed mode info for the EDID "established modes" data to use.
+ */
+static struct drm_display_mode edid_est_modes[] = {
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 40000, 800, 840,
+		   968, 1056, 0, 600, 601, 605, 628, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@60Hz */
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 36000, 800, 824,
+		   896, 1024, 0, 600, 601, 603,  625, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@56Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 31500, 640, 656,
+		   720, 840, 0, 480, 481, 484, 500, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@75Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 31500, 640, 664,
+		   704,  832, 0, 480, 489, 491, 520, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@72Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 30240, 640, 704,
+		   768,  864, 0, 480, 483, 486, 525, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@67Hz */
+	{ DRM_MODE("640x480", DRM_MODE_TYPE_DRIVER, 25200, 640, 656,
+		   752, 800, 0, 480, 490, 492, 525, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 640x480@60Hz */
+	{ DRM_MODE("720x400", DRM_MODE_TYPE_DRIVER, 35500, 720, 738,
+		   846, 900, 0, 400, 421, 423,  449, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 720x400@88Hz */
+	{ DRM_MODE("720x400", DRM_MODE_TYPE_DRIVER, 28320, 720, 738,
+		   846,  900, 0, 400, 412, 414, 449, 0,
+		   V_NHSYNC | V_PVSYNC) }, /* 720x400@70Hz */
+	{ DRM_MODE("1280x1024", DRM_MODE_TYPE_DRIVER, 135000, 1280, 1296,
+		   1440, 1688, 0, 1024, 1025, 1028, 1066, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 1280x1024@75Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER, 78800, 1024, 1040,
+		   1136, 1312, 0,  768, 769, 772, 800, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 1024x768@75Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER, 75000, 1024, 1048,
+		   1184, 1328, 0,  768, 771, 777, 806, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 1024x768@70Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER, 65000, 1024, 1048,
+		   1184, 1344, 0,  768, 771, 777, 806, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 1024x768@60Hz */
+	{ DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER,44900, 1024, 1032,
+		   1208, 1264, 0, 768, 768, 776, 817, 0,
+		   V_PHSYNC | V_PVSYNC | V_INTERLACE) }, /* 1024x768@43Hz */
+	{ DRM_MODE("832x624", DRM_MODE_TYPE_DRIVER, 57284, 832, 864,
+		   928, 1152, 0, 624, 625, 628, 667, 0,
+		   V_NHSYNC | V_NVSYNC) }, /* 832x624@75Hz */
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 49500, 800, 816,
+		   896, 1056, 0, 600, 601, 604,  625, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@75Hz */
+	{ DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER, 50000, 800, 856,
+		   976, 1040, 0, 600, 637, 643, 666, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 800x600@72Hz */
+	{ DRM_MODE("1152x864", DRM_MODE_TYPE_DRIVER, 108000, 1152, 1216,
+		   1344, 1600, 0,  864, 865, 868, 900, 0,
+		   V_PHSYNC | V_PVSYNC) }, /* 1152x864@75Hz */
+};
+
+#define EDID_EST_TIMINGS 16
+#define EDID_STD_TIMINGS 8
+#define EDID_DETAILED_TIMINGS 4
+
+/**
+ * add_established_modes - get est. modes from EDID and add them
+ * @edid: EDID block to scan
+ *
+ * Each EDID block contains a bitmap of the supported "established modes" list
+ * (defined above).  Tease them out and add them to the global modes list.
+ */
+static int add_established_modes(struct drm_output *output, struct edid *edid)
+{
+	struct drm_device *dev = output->dev;
+	unsigned long est_bits = edid->established_timings.t1 |
+		(edid->established_timings.t2 << 8) |
+		((edid->established_timings.mfg_rsvd & 0x80) << 9);
+	int i, modes = 0;
+
+	for (i = 0; i <= EDID_EST_TIMINGS; i++)
+		if (est_bits & (1<<i)) {
+			struct drm_display_mode *newmode;
+			newmode = drm_mode_duplicate(dev, &edid_est_modes[i]);
+			drm_mode_probed_add(output, newmode);
+			modes++;
+		}
+
+	return modes;
+}
+
+/**
+ * add_standard_modes - get std. modes from EDID and add them
+ * @edid: EDID block to scan
+ *
+ * Standard modes can be calculated using the CVT standard.  Grab them from
+ * @edid, calculate them, and add them to the list.
+ */
+static int add_standard_modes(struct drm_output *output, struct edid *edid)
+{
+	struct drm_device *dev = output->dev;
+	int i, modes = 0;
+
+	for (i = 0; i < EDID_STD_TIMINGS; i++) {
+		struct std_timing *t = &edid->standard_timings[i];
+		struct drm_display_mode *newmode;
+
+		/* If std timings bytes are 1, 1 it's empty */
+		if (t->hsize == 1 && (t->aspect_ratio | t->vfreq) == 1)
+			continue;
+
+		newmode = drm_mode_std(dev, &edid->standard_timings[i]);
+		drm_mode_probed_add(output, newmode);
+		modes++;
+	}
+
+	return modes;
+}
+
+/**
+ * add_detailed_modes - get detailed mode info from EDID data
+ * @edid: EDID block to scan
+ *
+ * Some of the detailed timing sections may contain mode information.  Grab
+ * it and add it to the list.
+ */
+static int add_detailed_info(struct drm_output *output, struct edid *edid)
+{
+	struct drm_device *dev = output->dev;
+	int i, j, modes = 0;
+
+	for (i = 0; i < EDID_DETAILED_TIMINGS; i++) {
+		struct detailed_timing *timing = &edid->detailed_timings[i];
+		struct detailed_non_pixel *data = &timing->data.other_data;
+		struct drm_display_mode *newmode;
+
+		/* EDID up to and including 1.2 may put monitor info here */
+		if (edid->version == 1 && edid->revision < 3)
+			continue;
+
+		/* Detailed mode timing */
+		if (timing->pixel_clock) {
+			newmode = drm_mode_detailed(dev, timing);
+			/* First detailed mode is preferred */
+			if (i == 0 && edid->preferred_timing)
+				newmode->type |= DRM_MODE_TYPE_PREFERRED;
+			drm_mode_probed_add(output, newmode);
+
+			modes++;
+			continue;
+		}
+
+		/* Other timing or info */
+		switch (data->type) {
+		case EDID_DETAIL_MONITOR_SERIAL:
+			break;
+		case EDID_DETAIL_MONITOR_STRING:
+			break;
+		case EDID_DETAIL_MONITOR_RANGE:
+			/* Get monitor range data */
+			break;
+		case EDID_DETAIL_MONITOR_NAME:
+			break;
+		case EDID_DETAIL_MONITOR_CPDATA:
+			break;
+		case EDID_DETAIL_STD_MODES:
+			/* Five modes per detailed section */
+			for (j = 0; j < 5; i++) {
+				struct std_timing *std;
+				struct drm_display_mode *newmode;
+
+				std = &data->data.timings[j];
+				newmode = drm_mode_std(dev, std);
+				drm_mode_probed_add(output, newmode);
+				modes++;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+
+	return modes;
+}
+
+#define DDC_ADDR 0x50
+
+static unsigned char *drm_do_probe_ddc_edid(struct i2c_adapter *adapter)
+{
+	unsigned char start = 0x0;
+	unsigned char *buf = kmalloc(EDID_LENGTH, GFP_KERNEL);
+	struct i2c_msg msgs[] = {
+		{
+			.addr	= DDC_ADDR,
+			.flags	= 0,
+			.len	= 1,
+			.buf	= &start,
+		}, {
+			.addr	= DDC_ADDR,
+			.flags	= I2C_M_RD,
+			.len	= EDID_LENGTH,
+			.buf	= buf,
+		}
+	};
+
+	if (!buf) {
+		DRM_ERROR("unable to allocate memory for EDID block.\n");
+		return NULL;
+	}
+
+	if (i2c_transfer(adapter, msgs, 2) == 2)
+		return buf;
+
+	DRM_INFO("unable to read EDID block.\n");
+	kfree(buf);
+	return NULL;
+}
+
+unsigned char *drm_ddc_read(struct i2c_adapter *adapter)
+{
+	struct i2c_algo_bit_data *algo_data = adapter->algo_data;
+	unsigned char *edid = NULL;
+	int i, j;
+
+	/*
+	 * Startup the bus:
+	 *   Set clock line high (but give it time to come up)
+	 *   Then set clock & data low
+	 */
+	algo_data->setscl(algo_data->data, 1);
+	udelay(550); /* startup delay */
+	algo_data->setscl(algo_data->data, 0);
+	algo_data->setsda(algo_data->data, 0);
+
+	for (i = 0; i < 3; i++) {
+		/* For some old monitors we need the
+		 * following process to initialize/stop DDC
+		 */
+		algo_data->setsda(algo_data->data, 0);
+		msleep(13);
+
+		algo_data->setscl(algo_data->data, 1);
+		for (j = 0; j < 5; j++) {
+			msleep(10);
+			if (algo_data->getscl(algo_data->data))
+				break;
+		}
+		if (j == 5)
+			continue;
+
+		algo_data->setsda(algo_data->data, 0);
+		msleep(15);
+		algo_data->setscl(algo_data->data, 0);
+		msleep(15);
+		algo_data->setsda(algo_data->data, 1);
+		msleep(15);
+
+		/* Do the real work */
+		edid = drm_do_probe_ddc_edid(adapter);
+		algo_data->setsda(algo_data->data, 0);
+		algo_data->setscl(algo_data->data, 0);
+		msleep(15);
+
+		algo_data->setscl(algo_data->data, 1);
+		for (j = 0; j < 10; j++) {
+			msleep(10);
+			if (algo_data->getscl(algo_data->data))
+				break;
+		}
+
+		algo_data->setsda(algo_data->data, 1);
+		msleep(15);
+		algo_data->setscl(algo_data->data, 0);
+		if (edid)
+			break;
+	}
+	/* Release the DDC lines when done or the Apple Cinema HD display
+	 * will switch off
+	 */
+	algo_data->setsda(algo_data->data, 0);
+	algo_data->setscl(algo_data->data, 0);
+	algo_data->setscl(algo_data->data, 1);
+
+	return edid;
+}
+EXPORT_SYMBOL(drm_ddc_read);
+
+/**
+ * drm_get_edid - get EDID data, if available
+ * @output: output we're probing
+ * @adapter: i2c adapter to use for DDC
+ *
+ * Poke the given output's i2c channel to grab EDID data if possible.
+ *
+ * Return edid data or NULL if we couldn't find any.
+ */
+struct edid *drm_get_edid(struct drm_output *output,
+			  struct i2c_adapter *adapter)
+{
+	struct edid *edid;
+
+	edid = (struct edid *)drm_ddc_read(adapter);
+	if (!edid) {
+		dev_warn(&output->dev->pdev->dev, "%s: no EDID data\n",
+			 output->name);
+		return NULL;
+	}
+	if (!edid_valid(edid)) {
+		dev_warn(&output->dev->pdev->dev, "%s: EDID invalid.\n",
+			 output->name);
+		kfree(edid);
+		return NULL;
+	}
+	return edid;
+}
+EXPORT_SYMBOL(drm_get_edid);
+
+/**
+ * drm_add_edid_modes - add modes from EDID data, if available
+ * @output: output we're probing
+ * @edid: edid data
+ *
+ * Add the specified modes to the output's mode list.
+ *
+ * Return number of modes added or 0 if we couldn't find any.
+ */
+int drm_add_edid_modes(struct drm_output *output, struct edid *edid)
+{
+	int num_modes = 0;
+
+	if (edid == NULL) {
+		return 0;
+	}
+	if (!edid_valid(edid)) {
+		dev_warn(&output->dev->pdev->dev, "%s: EDID invalid.\n",
+			 output->name);
+		return 0;
+	}
+	num_modes += add_established_modes(output, edid);
+	num_modes += add_standard_modes(output, edid);
+	num_modes += add_detailed_info(output, edid);
+	return num_modes;
+}
+EXPORT_SYMBOL(drm_add_edid_modes);
Index: linux-2.6.27/drivers/gpu/drm/drm_fence.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_fence.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,829 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+
+/*
+ * Convenience function to be called by fence::wait methods that
+ * need polling.
+ */
+
+int drm_fence_wait_polling(struct drm_fence_object *fence, int lazy,
+			   int interruptible, uint32_t mask,
+			   unsigned long end_jiffies)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	uint32_t count = 0;
+	int ret;
+
+	DECLARE_WAITQUEUE(entry, current);
+	add_wait_queue(&fc->fence_queue, &entry);
+
+	ret = 0;
+
+	for (;;) {
+		__set_current_state((interruptible) ?
+				    TASK_INTERRUPTIBLE :
+				    TASK_UNINTERRUPTIBLE);
+		if (drm_fence_object_signaled(fence, mask))
+			break;
+		if (time_after_eq(jiffies, end_jiffies)) {
+			ret = -EBUSY;
+			break;
+		}
+		if (lazy)
+			schedule_timeout(1);
+		else if ((++count & 0x0F) == 0){
+			__set_current_state(TASK_RUNNING);
+			schedule();
+			__set_current_state((interruptible) ?
+					    TASK_INTERRUPTIBLE :
+					    TASK_UNINTERRUPTIBLE);
+		}
+		if (interruptible && signal_pending(current)) {
+			ret = -EAGAIN;
+			break;
+		}
+	}
+	__set_current_state(TASK_RUNNING);
+	remove_wait_queue(&fc->fence_queue, &entry);
+	return ret;
+}
+EXPORT_SYMBOL(drm_fence_wait_polling);
+
+/*
+ * Typically called by the IRQ handler.
+ */
+
+void drm_fence_handler(struct drm_device *dev, uint32_t fence_class,
+		       uint32_t sequence, uint32_t type, uint32_t error)
+{
+	int wake = 0;
+	uint32_t diff;
+	uint32_t relevant_type;
+	uint32_t new_type;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence_class];
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	struct list_head *head;
+	struct drm_fence_object *fence, *next;
+	int found = 0;
+
+	if (list_empty(&fc->ring))
+		return;
+
+	list_for_each_entry(fence, &fc->ring, ring) {
+		diff = (sequence - fence->sequence) & driver->sequence_mask;
+		if (diff > driver->wrap_diff) {
+			found = 1;
+			break;
+		}
+	}
+
+	fc->waiting_types &= ~type;
+	head = (found) ? &fence->ring : &fc->ring;
+
+	list_for_each_entry_safe_reverse(fence, next, head, ring) {
+		if (&fence->ring == &fc->ring)
+			break;
+
+		if (error) {
+			fence->error = error;
+			fence->signaled_types = fence->type;
+			list_del_init(&fence->ring);
+			wake = 1;
+			break;
+		}
+
+		if (type & DRM_FENCE_TYPE_EXE)
+			type |= fence->native_types;
+
+		relevant_type = type & fence->type;
+		new_type = (fence->signaled_types | relevant_type) ^
+			fence->signaled_types;
+
+		if (new_type) {
+			fence->signaled_types |= new_type;
+			DRM_DEBUG("Fence 0x%08lx signaled 0x%08x\n",
+				  fence->base.hash.key, fence->signaled_types);
+
+			if (driver->needed_flush)
+				fc->pending_flush |= driver->needed_flush(fence);
+
+			if (new_type & fence->waiting_types)
+				wake = 1;
+		}
+
+		fc->waiting_types |= fence->waiting_types & ~fence->signaled_types;
+
+		if (!(fence->type & ~fence->signaled_types)) {
+			DRM_DEBUG("Fence completely signaled 0x%08lx\n",
+				  fence->base.hash.key);
+			list_del_init(&fence->ring);
+		}
+	}
+
+	/*
+	 * Reinstate lost waiting types.
+	 */
+
+	if ((fc->waiting_types & type) != type) {
+		head = head->prev;
+		list_for_each_entry(fence, head, ring) {
+			if (&fence->ring == &fc->ring)
+				break;
+			diff = (fc->highest_waiting_sequence - fence->sequence) &
+				driver->sequence_mask;
+			if (diff > driver->wrap_diff)
+				break;
+
+			fc->waiting_types |= fence->waiting_types & ~fence->signaled_types;
+		}
+	}
+
+	if (wake)
+		wake_up_all(&fc->fence_queue);
+}
+EXPORT_SYMBOL(drm_fence_handler);
+
+static void drm_fence_unring(struct drm_device *dev, struct list_head *ring)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	unsigned long flags;
+
+	write_lock_irqsave(&fm->lock, flags);
+	list_del_init(ring);
+	write_unlock_irqrestore(&fm->lock, flags);
+}
+
+void drm_fence_usage_deref_locked(struct drm_fence_object **fence)
+{
+	struct drm_fence_object *tmp_fence = *fence;
+	struct drm_device *dev = tmp_fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	*fence = NULL;
+	if (atomic_dec_and_test(&tmp_fence->usage)) {
+		drm_fence_unring(dev, &tmp_fence->ring);
+		DRM_DEBUG("Destroyed a fence object 0x%08lx\n",
+			  tmp_fence->base.hash.key);
+		atomic_dec(&fm->count);
+		BUG_ON(!list_empty(&tmp_fence->base.list));
+		drm_free(tmp_fence, sizeof(*tmp_fence), DRM_MEM_FENCE);
+	}
+}
+EXPORT_SYMBOL(drm_fence_usage_deref_locked);
+
+void drm_fence_usage_deref_unlocked(struct drm_fence_object **fence)
+{
+	struct drm_fence_object *tmp_fence = *fence;
+	struct drm_device *dev = tmp_fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	*fence = NULL;
+	if (atomic_dec_and_test(&tmp_fence->usage)) {
+		mutex_lock(&dev->struct_mutex);
+		if (atomic_read(&tmp_fence->usage) == 0) {
+			drm_fence_unring(dev, &tmp_fence->ring);
+			atomic_dec(&fm->count);
+			BUG_ON(!list_empty(&tmp_fence->base.list));
+			drm_free(tmp_fence, sizeof(*tmp_fence), DRM_MEM_FENCE);
+		}
+		mutex_unlock(&dev->struct_mutex);
+	}
+}
+EXPORT_SYMBOL(drm_fence_usage_deref_unlocked);
+
+struct drm_fence_object
+*drm_fence_reference_locked(struct drm_fence_object *src)
+{
+	DRM_ASSERT_LOCKED(&src->dev->struct_mutex);
+
+	atomic_inc(&src->usage);
+	return src;
+}
+
+void drm_fence_reference_unlocked(struct drm_fence_object **dst,
+				  struct drm_fence_object *src)
+{
+	mutex_lock(&src->dev->struct_mutex);
+	*dst = src;
+	atomic_inc(&src->usage);
+	mutex_unlock(&src->dev->struct_mutex);
+}
+EXPORT_SYMBOL(drm_fence_reference_unlocked);
+
+static void drm_fence_object_destroy(struct drm_file *priv,
+				     struct drm_user_object *base)
+{
+	struct drm_fence_object *fence =
+	    drm_user_object_entry(base, struct drm_fence_object, base);
+
+	drm_fence_usage_deref_locked(&fence);
+}
+
+int drm_fence_object_signaled(struct drm_fence_object *fence, uint32_t mask)
+{
+	unsigned long flags;
+	int signaled;
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+
+	mask &= fence->type;
+	read_lock_irqsave(&fm->lock, flags);
+	signaled = (mask & fence->signaled_types) == mask;
+	read_unlock_irqrestore(&fm->lock, flags);
+	if (!signaled && driver->poll) {
+		write_lock_irqsave(&fm->lock, flags);
+		driver->poll(dev, fence->fence_class, mask);
+		signaled = (mask & fence->signaled_types) == mask;
+		write_unlock_irqrestore(&fm->lock, flags);
+	}
+	return signaled;
+}
+EXPORT_SYMBOL(drm_fence_object_signaled);
+
+
+int drm_fence_object_flush(struct drm_fence_object *fence,
+			   uint32_t type)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	unsigned long irq_flags;
+	uint32_t saved_pending_flush;
+	uint32_t diff;
+	int call_flush;
+
+	if (type & ~fence->type) {
+		DRM_ERROR("Flush trying to extend fence type, "
+			  "0x%x, 0x%x\n", type, fence->type);
+		return -EINVAL;
+	}
+
+	write_lock_irqsave(&fm->lock, irq_flags);
+	fence->waiting_types |= type;
+	fc->waiting_types |= fence->waiting_types;
+	diff = (fence->sequence - fc->highest_waiting_sequence) &
+		driver->sequence_mask;
+
+	if (diff < driver->wrap_diff)
+		fc->highest_waiting_sequence = fence->sequence;
+
+	/*
+	 * fence->waiting_types has changed. Determine whether
+	 * we need to initiate some kind of flush as a result of this.
+	 */
+
+	saved_pending_flush = fc->pending_flush;
+	if (driver->needed_flush)
+		fc->pending_flush |= driver->needed_flush(fence);
+
+	if (driver->poll)
+		driver->poll(dev, fence->fence_class, fence->waiting_types);
+
+	call_flush = fc->pending_flush;
+	write_unlock_irqrestore(&fm->lock, irq_flags);
+
+	if (call_flush && driver->flush)
+		driver->flush(dev, fence->fence_class);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_fence_object_flush);
+
+/*
+ * Make sure old fence objects are signaled before their fence sequences are
+ * wrapped around and reused.
+ */
+
+void drm_fence_flush_old(struct drm_device *dev, uint32_t fence_class,
+			 uint32_t sequence)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence_class];
+	struct drm_fence_object *fence;
+	unsigned long irq_flags;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	int call_flush;
+
+	uint32_t diff;
+
+	write_lock_irqsave(&fm->lock, irq_flags);
+
+	list_for_each_entry_reverse(fence, &fc->ring, ring) {
+		diff = (sequence - fence->sequence) & driver->sequence_mask;
+		if (diff <= driver->flush_diff)
+			break;
+
+		fence->waiting_types = fence->type;
+		fc->waiting_types |= fence->type;
+
+		if (driver->needed_flush)
+			fc->pending_flush |= driver->needed_flush(fence);
+	}
+
+	if (driver->poll)
+		driver->poll(dev, fence_class, fc->waiting_types);
+
+	call_flush = fc->pending_flush;
+	write_unlock_irqrestore(&fm->lock, irq_flags);
+
+	if (call_flush && driver->flush)
+		driver->flush(dev, fence->fence_class);
+
+	/*
+	 * FIXME: Shold we implement a wait here for really old fences?
+	 */
+
+}
+EXPORT_SYMBOL(drm_fence_flush_old);
+
+int drm_fence_object_wait(struct drm_fence_object *fence,
+			  int lazy, int ignore_signals, uint32_t mask)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	int ret = 0;
+	unsigned long _end = 3 * DRM_HZ;
+
+	if (mask & ~fence->type) {
+		DRM_ERROR("Wait trying to extend fence type"
+			  " 0x%08x 0x%08x\n", mask, fence->type);
+		BUG();
+		return -EINVAL;
+	}
+
+	if (driver->wait)
+		return driver->wait(fence, lazy, !ignore_signals, mask);
+
+
+	drm_fence_object_flush(fence, mask);
+	if (driver->has_irq(dev, fence->fence_class, mask)) {
+		if (!ignore_signals)
+			ret = wait_event_interruptible_timeout
+				(fc->fence_queue,
+				 drm_fence_object_signaled(fence, mask),
+				 3 * DRM_HZ);
+		else
+			ret = wait_event_timeout
+				(fc->fence_queue,
+				 drm_fence_object_signaled(fence, mask),
+				 3 * DRM_HZ);
+
+		if (unlikely(ret == -ERESTARTSYS))
+			return -EAGAIN;
+
+		if (unlikely(ret == 0))
+			return -EBUSY;
+
+		return 0;
+	}
+
+	return drm_fence_wait_polling(fence, lazy, !ignore_signals, mask,
+				      _end);
+}
+EXPORT_SYMBOL(drm_fence_object_wait);
+
+
+
+int drm_fence_object_emit(struct drm_fence_object *fence, uint32_t fence_flags,
+			  uint32_t fence_class, uint32_t type)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence->fence_class];
+	unsigned long flags;
+	uint32_t sequence;
+	uint32_t native_types;
+	int ret;
+
+	drm_fence_unring(dev, &fence->ring);
+	ret = driver->emit(dev, fence_class, fence_flags, &sequence,
+			   &native_types);
+	if (ret)
+		return ret;
+
+	write_lock_irqsave(&fm->lock, flags);
+	fence->fence_class = fence_class;
+	fence->type = type;
+	fence->waiting_types = 0;
+	fence->signaled_types = 0;
+	fence->error = 0;
+	fence->sequence = sequence;
+	fence->native_types = native_types;
+	if (list_empty(&fc->ring))
+		fc->highest_waiting_sequence = sequence - 1;
+	list_add_tail(&fence->ring, &fc->ring);
+	fc->latest_queued_sequence = sequence;
+	write_unlock_irqrestore(&fm->lock, flags);
+	return 0;
+}
+EXPORT_SYMBOL(drm_fence_object_emit);
+
+static int drm_fence_object_init(struct drm_device *dev, uint32_t fence_class,
+				 uint32_t type,
+				 uint32_t fence_flags,
+				 struct drm_fence_object *fence)
+{
+	int ret = 0;
+	unsigned long flags;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	mutex_lock(&dev->struct_mutex);
+	atomic_set(&fence->usage, 1);
+	mutex_unlock(&dev->struct_mutex);
+
+	write_lock_irqsave(&fm->lock, flags);
+	INIT_LIST_HEAD(&fence->ring);
+
+	/*
+	 *  Avoid hitting BUG() for kernel-only fence objects.
+	 */
+
+	INIT_LIST_HEAD(&fence->base.list);
+	fence->fence_class = fence_class;
+	fence->type = type;
+	fence->signaled_types = 0;
+	fence->waiting_types = 0;
+	fence->sequence = 0;
+	fence->error = 0;
+	fence->dev = dev;
+	write_unlock_irqrestore(&fm->lock, flags);
+	if (fence_flags & DRM_FENCE_FLAG_EMIT) {
+		ret = drm_fence_object_emit(fence, fence_flags,
+					    fence->fence_class, type);
+	}
+	return ret;
+}
+
+int drm_fence_add_user_object(struct drm_file *priv,
+			      struct drm_fence_object *fence, int shareable)
+{
+	struct drm_device *dev = priv->minor->dev;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_user_object(priv, &fence->base, shareable);
+	if (ret)
+		goto out;
+	atomic_inc(&fence->usage);
+	fence->base.type = drm_fence_type;
+	fence->base.remove = &drm_fence_object_destroy;
+	DRM_DEBUG("Fence 0x%08lx created\n", fence->base.hash.key);
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+EXPORT_SYMBOL(drm_fence_add_user_object);
+
+int drm_fence_object_create(struct drm_device *dev, uint32_t fence_class,
+			    uint32_t type, unsigned flags,
+			    struct drm_fence_object **c_fence)
+{
+	struct drm_fence_object *fence;
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+
+	fence = drm_calloc(1, sizeof(*fence), DRM_MEM_FENCE);
+	if (!fence) {
+		DRM_INFO("Out of memory creating fence object.\n");
+		return -ENOMEM;
+	}
+	ret = drm_fence_object_init(dev, fence_class, type, flags, fence);
+	if (ret) {
+		drm_fence_usage_deref_unlocked(&fence);
+		return ret;
+	}
+	*c_fence = fence;
+	atomic_inc(&fm->count);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_fence_object_create);
+
+void drm_fence_manager_init(struct drm_device *dev)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fence_class;
+	struct drm_fence_driver *fed = dev->driver->fence_driver;
+	int i;
+	unsigned long flags;
+
+	rwlock_init(&fm->lock);
+	write_lock_irqsave(&fm->lock, flags);
+	fm->initialized = 0;
+	if (!fed)
+	    goto out_unlock;
+
+	fm->initialized = 1;
+	fm->num_classes = fed->num_classes;
+	BUG_ON(fm->num_classes > _DRM_FENCE_CLASSES);
+
+	for (i = 0; i < fm->num_classes; ++i) {
+	    fence_class = &fm->fence_class[i];
+
+	    memset(fence_class, 0, sizeof(*fence_class));
+	    INIT_LIST_HEAD(&fence_class->ring);
+	    DRM_INIT_WAITQUEUE(&fence_class->fence_queue);
+	}
+
+	atomic_set(&fm->count, 0);
+ out_unlock:
+	write_unlock_irqrestore(&fm->lock, flags);
+}
+
+void drm_fence_fill_arg(struct drm_fence_object *fence,
+			struct drm_fence_arg *arg)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_manager *fm = &dev->fm;
+	unsigned long irq_flags;
+
+	read_lock_irqsave(&fm->lock, irq_flags);
+	arg->handle = fence->base.hash.key;
+	arg->fence_class = fence->fence_class;
+	arg->type = fence->type;
+	arg->signaled = fence->signaled_types;
+	arg->error = fence->error;
+	arg->sequence = fence->sequence;
+	read_unlock_irqrestore(&fm->lock, irq_flags);
+}
+EXPORT_SYMBOL(drm_fence_fill_arg);
+
+void drm_fence_manager_takedown(struct drm_device *dev)
+{
+}
+
+struct drm_fence_object *drm_lookup_fence_object(struct drm_file *priv,
+						 uint32_t handle)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_user_object *uo;
+	struct drm_fence_object *fence;
+
+	mutex_lock(&dev->struct_mutex);
+	uo = drm_lookup_user_object(priv, handle);
+	if (!uo || (uo->type != drm_fence_type)) {
+		mutex_unlock(&dev->struct_mutex);
+		return NULL;
+	}
+	fence = drm_fence_reference_locked(drm_user_object_entry(uo, struct drm_fence_object, base));
+	mutex_unlock(&dev->struct_mutex);
+	return fence;
+}
+
+int drm_fence_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	if (arg->flags & DRM_FENCE_FLAG_EMIT)
+		LOCK_TEST_WITH_RETURN(dev, file_priv);
+	ret = drm_fence_object_create(dev, arg->fence_class,
+				      arg->type, arg->flags, &fence);
+	if (ret)
+		return ret;
+	ret = drm_fence_add_user_object(file_priv, fence,
+					arg->flags &
+					DRM_FENCE_FLAG_SHAREABLE);
+	if (ret) {
+		drm_fence_usage_deref_unlocked(&fence);
+		return ret;
+	}
+
+	/*
+	 * usage > 0. No need to lock dev->struct_mutex;
+	 */
+
+	arg->handle = fence->base.hash.key;
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+int drm_fence_reference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	struct drm_user_object *uo;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_user_object_ref(file_priv, arg->handle, drm_fence_type, &uo);
+	if (ret)
+		return ret;
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+
+int drm_fence_unreference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	return drm_user_object_unref(file_priv, arg->handle, drm_fence_type);
+}
+
+int drm_fence_signaled_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+int drm_fence_flush_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+	ret = drm_fence_object_flush(fence, arg->type);
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+
+int drm_fence_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+	ret = drm_fence_object_wait(fence,
+				    arg->flags & DRM_FENCE_FLAG_WAIT_LAZY,
+				    0, arg->type);
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+
+int drm_fence_emit_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	LOCK_TEST_WITH_RETURN(dev, file_priv);
+	fence = drm_lookup_fence_object(file_priv, arg->handle);
+	if (!fence)
+		return -EINVAL;
+	ret = drm_fence_object_emit(fence, arg->flags, arg->fence_class,
+				    arg->type);
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
+
+int drm_fence_buffers_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_arg *arg = data;
+	struct drm_fence_object *fence;
+	ret = 0;
+
+	if (!fm->initialized) {
+		DRM_ERROR("The DRM driver does not support fencing.\n");
+		return -EINVAL;
+	}
+
+	if (!dev->bm.initialized) {
+		DRM_ERROR("Buffer object manager is not initialized\n");
+		return -EINVAL;
+	}
+	LOCK_TEST_WITH_RETURN(dev, file_priv);
+	ret = drm_fence_buffer_objects(dev, NULL, arg->flags,
+				       NULL, &fence);
+	if (ret)
+		return ret;
+
+	if (!(arg->flags & DRM_FENCE_FLAG_NO_USER)) {
+		ret = drm_fence_add_user_object(file_priv, fence,
+						arg->flags &
+						DRM_FENCE_FLAG_SHAREABLE);
+		if (ret)
+			return ret;
+	}
+
+	arg->handle = fence->base.hash.key;
+
+	drm_fence_fill_arg(fence, arg);
+	drm_fence_usage_deref_unlocked(&fence);
+
+	return ret;
+}
Index: linux-2.6.27/drivers/gpu/drm/drm_fops.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_fops.c	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_fops.c	2009-02-05 13:29:33.000000000 +0000
@@ -231,6 +231,7 @@
 	int minor_id = iminor(inode);
 	struct drm_file *priv;
 	int ret;
+	int i, j;
 
 	if (filp->f_flags & O_EXCL)
 		return -EBUSY;	/* No exclusive opens */
@@ -255,9 +256,21 @@
 	priv->lock_count = 0;
 
 	INIT_LIST_HEAD(&priv->lhead);
+	INIT_LIST_HEAD(&priv->refd_objects);
+	INIT_LIST_HEAD(&priv->fbs);
 
-	if (dev->driver->driver_features & DRIVER_GEM)
-		drm_gem_open(dev, priv);
+	for (i = 0; i < _DRM_NO_REF_TYPES; ++i) {
+		ret = drm_ht_create(&priv->refd_object_hash[i],
+				    DRM_FILE_HASH_ORDER);
+		if (ret)
+			break;
+	}
+
+	if (ret) {
+		for (j = 0; j < i; ++j)
+			drm_ht_remove(&priv->refd_object_hash[j]);
+		goto out_free;
+	}
 
 	if (dev->driver->open) {
 		ret = dev->driver->open(dev, priv);
@@ -314,6 +327,33 @@
 }
 EXPORT_SYMBOL(drm_fasync);
 
+static void drm_object_release(struct file *filp)
+{
+	struct drm_file *priv = filp->private_data;
+	struct list_head *head;
+	struct drm_ref_object *ref_object;
+	int i;
+
+	/*
+	 * Free leftover ref objects created by me. Note that we cannot use
+	 * list_for_each() here, as the struct_mutex may be temporarily
+	 * released by the remove_() functions, and thus the lists may be
+	 * altered.
+	 * Also, a drm_remove_ref_object() will not remove it
+	 * from the list unless its refcount is 1.
+	 */
+
+	head = &priv->refd_objects;
+	while (head->next != head) {
+		ref_object = list_entry(head->next, struct drm_ref_object, list);
+		drm_remove_ref_object(priv, ref_object);
+		head = &priv->refd_objects;
+	}
+
+	for (i = 0; i < _DRM_NO_REF_TYPES; ++i)
+		drm_ht_remove(&priv->refd_object_hash[i]);
+}
+
 /**
  * Release file.
  *
@@ -403,9 +443,6 @@
 		dev->driver->reclaim_buffers(dev, file_priv);
 	}
 
-	if (dev->driver->driver_features & DRIVER_GEM)
-		drm_gem_release(dev, file_priv);
-
 	drm_fasync(-1, filp, 0);
 
 	mutex_lock(&dev->ctxlist_mutex);
@@ -430,6 +467,8 @@
 	mutex_unlock(&dev->ctxlist_mutex);
 
 	mutex_lock(&dev->struct_mutex);
+	drm_fb_release(filp);
+	drm_object_release(filp);
 	if (file_priv->remove_auth_on_close == 1) {
 		struct drm_file *temp;
 
Index: linux-2.6.27/drivers/gpu/drm/drm_hashtab.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_hashtab.c	2008-10-09 23:13:53.000000000 +0100
+++ linux-2.6.27/drivers/gpu/drm/drm_hashtab.c	2009-02-05 13:29:33.000000000 +0000
@@ -29,7 +29,7 @@
  * Simple open hash tab implementation.
  *
  * Authors:
- * Thomas HellstrÃ¶m <thomas-at-tungstengraphics-dot-com>
+ * Thomas Hellström <thomas-at-tungstengraphics-dot-com>
  */
 
 #include "drmP.h"
Index: linux-2.6.27/drivers/gpu/drm/drm_irq.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_irq.c	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_irq.c	2009-02-05 13:29:33.000000000 +0000
@@ -70,6 +70,7 @@
 
 	return 0;
 }
+#if 0
 
 static void vblank_disable_fn(unsigned long arg)
 {
@@ -184,6 +185,8 @@
 }
 EXPORT_SYMBOL(drm_vblank_init);
 
+#endif
+
 /**
  * Install IRQ handler.
  *
@@ -221,6 +224,17 @@
 
 	DRM_DEBUG("irq=%d\n", dev->pdev->irq);
 
+	if (drm_core_check_feature(dev, DRIVER_IRQ_VBL)) {
+		init_waitqueue_head(&dev->vbl_queue);
+
+		spin_lock_init(&dev->vbl_lock);
+
+		INIT_LIST_HEAD(&dev->vbl_sigs);
+		INIT_LIST_HEAD(&dev->vbl_sigs2);
+
+		dev->vbl_pending = 0;
+	}
+
 	/* Before installing handler */
 	dev->driver->irq_preinstall(dev);
 
@@ -281,8 +295,6 @@
 
 	free_irq(dev->pdev->irq, dev);
 
-	drm_vblank_cleanup(dev);
-
 	dev->locked_tasklet_func = NULL;
 
 	return 0;
@@ -326,174 +338,6 @@
 }
 
 /**
- * drm_vblank_count - retrieve "cooked" vblank counter value
- * @dev: DRM device
- * @crtc: which counter to retrieve
- *
- * Fetches the "cooked" vblank count value that represents the number of
- * vblank events since the system was booted, including lost events due to
- * modesetting activity.
- */
-u32 drm_vblank_count(struct drm_device *dev, int crtc)
-{
-	return atomic_read(&dev->_vblank_count[crtc]);
-}
-EXPORT_SYMBOL(drm_vblank_count);
-
-/**
- * drm_update_vblank_count - update the master vblank counter
- * @dev: DRM device
- * @crtc: counter to update
- *
- * Call back into the driver to update the appropriate vblank counter
- * (specified by @crtc).  Deal with wraparound, if it occurred, and
- * update the last read value so we can deal with wraparound on the next
- * call if necessary.
- *
- * Only necessary when going from off->on, to account for frames we
- * didn't get an interrupt for.
- *
- * Note: caller must hold dev->vbl_lock since this reads & writes
- * device vblank fields.
- */
-static void drm_update_vblank_count(struct drm_device *dev, int crtc)
-{
-	u32 cur_vblank, diff;
-
-	/*
-	 * Interrupts were disabled prior to this call, so deal with counter
-	 * wrap if needed.
-	 * NOTE!  It's possible we lost a full dev->max_vblank_count events
-	 * here if the register is small or we had vblank interrupts off for
-	 * a long time.
-	 */
-	cur_vblank = dev->driver->get_vblank_counter(dev, crtc);
-	diff = cur_vblank - dev->last_vblank[crtc];
-	if (cur_vblank < dev->last_vblank[crtc]) {
-		diff += dev->max_vblank_count;
-
-		DRM_DEBUG("last_vblank[%d]=0x%x, cur_vblank=0x%x => diff=0x%x\n",
-			  crtc, dev->last_vblank[crtc], cur_vblank, diff);
-	}
-
-	DRM_DEBUG("enabling vblank interrupts on crtc %d, missed %d\n",
-		  crtc, diff);
-
-	atomic_add(diff, &dev->_vblank_count[crtc]);
-}
-
-/**
- * drm_vblank_get - get a reference count on vblank events
- * @dev: DRM device
- * @crtc: which CRTC to own
- *
- * Acquire a reference count on vblank events to avoid having them disabled
- * while in use.
- *
- * RETURNS
- * Zero on success, nonzero on failure.
- */
-int drm_vblank_get(struct drm_device *dev, int crtc)
-{
-	unsigned long irqflags;
-	int ret = 0;
-
-	spin_lock_irqsave(&dev->vbl_lock, irqflags);
-	/* Going from 0->1 means we have to enable interrupts again */
-	if (atomic_add_return(1, &dev->vblank_refcount[crtc]) == 1 &&
-	    !dev->vblank_enabled[crtc]) {
-		ret = dev->driver->enable_vblank(dev, crtc);
-		DRM_DEBUG("enabling vblank on crtc %d, ret: %d\n", crtc, ret);
-		if (ret)
-			atomic_dec(&dev->vblank_refcount[crtc]);
-		else {
-			dev->vblank_enabled[crtc] = 1;
-			drm_update_vblank_count(dev, crtc);
-		}
-	}
-	spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
-
-	return ret;
-}
-EXPORT_SYMBOL(drm_vblank_get);
-
-/**
- * drm_vblank_put - give up ownership of vblank events
- * @dev: DRM device
- * @crtc: which counter to give up
- *
- * Release ownership of a given vblank counter, turning off interrupts
- * if possible.
- */
-void drm_vblank_put(struct drm_device *dev, int crtc)
-{
-	/* Last user schedules interrupt disable */
-	if (atomic_dec_and_test(&dev->vblank_refcount[crtc]))
-		mod_timer(&dev->vblank_disable_timer, jiffies + 5*DRM_HZ);
-}
-EXPORT_SYMBOL(drm_vblank_put);
-
-/**
- * drm_modeset_ctl - handle vblank event counter changes across mode switch
- * @DRM_IOCTL_ARGS: standard ioctl arguments
- *
- * Applications should call the %_DRM_PRE_MODESET and %_DRM_POST_MODESET
- * ioctls around modesetting so that any lost vblank events are accounted for.
- *
- * Generally the counter will reset across mode sets.  If interrupts are
- * enabled around this call, we don't have to do anything since the counter
- * will have already been incremented.
- */
-int drm_modeset_ctl(struct drm_device *dev, void *data,
-		    struct drm_file *file_priv)
-{
-	struct drm_modeset_ctl *modeset = data;
-	unsigned long irqflags;
-	int crtc, ret = 0;
-
-	/* If drm_vblank_init() hasn't been called yet, just no-op */
-	if (!dev->num_crtcs)
-		goto out;
-
-	crtc = modeset->crtc;
-	if (crtc >= dev->num_crtcs) {
-		ret = -EINVAL;
-		goto out;
-	}
-
-	/*
-	 * To avoid all the problems that might happen if interrupts
-	 * were enabled/disabled around or between these calls, we just
-	 * have the kernel take a reference on the CRTC (just once though
-	 * to avoid corrupting the count if multiple, mismatch calls occur),
-	 * so that interrupts remain enabled in the interim.
-	 */
-	switch (modeset->cmd) {
-	case _DRM_PRE_MODESET:
-		if (!dev->vblank_inmodeset[crtc]) {
-			dev->vblank_inmodeset[crtc] = 1;
-			drm_vblank_get(dev, crtc);
-		}
-		break;
-	case _DRM_POST_MODESET:
-		if (dev->vblank_inmodeset[crtc]) {
-			spin_lock_irqsave(&dev->vbl_lock, irqflags);
-			dev->vblank_disable_allowed = 1;
-			dev->vblank_inmodeset[crtc] = 0;
-			spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
-			drm_vblank_put(dev, crtc);
-		}
-		break;
-	default:
-		ret = -EINVAL;
-		break;
-	}
-
-out:
-	return ret;
-}
-
-/**
  * Wait for VBLANK.
  *
  * \param inode device inode.
@@ -512,14 +356,14 @@
  *
  * If a signal is not requested, then calls vblank_wait().
  */
-int drm_wait_vblank(struct drm_device *dev, void *data,
-		    struct drm_file *file_priv)
+int drm_wait_vblank(struct drm_device *dev, void *data, struct drm_file *file_priv)
 {
 	union drm_wait_vblank *vblwait = data;
+	struct timeval now;
 	int ret = 0;
-	unsigned int flags, seq, crtc;
+	unsigned int flags, seq;
 
-	if ((!dev->pdev->irq) || (!dev->irq_enabled))
+	if ((!dev->irq) || (!dev->irq_enabled))
 		return -EINVAL;
 
 	if (vblwait->request.type &
@@ -531,17 +375,13 @@
 	}
 
 	flags = vblwait->request.type & _DRM_VBLANK_FLAGS_MASK;
-	crtc = flags & _DRM_VBLANK_SECONDARY ? 1 : 0;
 
-	if (crtc >= dev->num_crtcs)
+	if (!drm_core_check_feature(dev, (flags & _DRM_VBLANK_SECONDARY) ?
+				    DRIVER_IRQ_VBL2 : DRIVER_IRQ_VBL))
 		return -EINVAL;
 
-	ret = drm_vblank_get(dev, crtc);
-	if (ret) {
-		DRM_ERROR("failed to acquire vblank counter, %d\n", ret);
-		return ret;
-	}
-	seq = drm_vblank_count(dev, crtc);
+	seq = atomic_read((flags & _DRM_VBLANK_SECONDARY) ? &dev->vbl_received2
+			  : &dev->vbl_received);
 
 	switch (vblwait->request.type & _DRM_VBLANK_TYPES_MASK) {
 	case _DRM_VBLANK_RELATIVE:
@@ -550,8 +390,7 @@
 	case _DRM_VBLANK_ABSOLUTE:
 		break;
 	default:
-		ret = -EINVAL;
-		goto done;
+		return -EINVAL;
 	}
 
 	if ((flags & _DRM_VBLANK_NEXTONMISS) &&
@@ -561,7 +400,8 @@
 
 	if (flags & _DRM_VBLANK_SIGNAL) {
 		unsigned long irqflags;
-		struct list_head *vbl_sigs = &dev->vbl_sigs[crtc];
+		struct list_head *vbl_sigs = (flags & _DRM_VBLANK_SECONDARY)
+				      ? &dev->vbl_sigs2 : &dev->vbl_sigs;
 		struct drm_vbl_sig *vbl_sig;
 
 		spin_lock_irqsave(&dev->vbl_lock, irqflags);
@@ -582,29 +422,22 @@
 			}
 		}
 
-		if (atomic_read(&dev->vbl_signal_pending) >= 100) {
+		if (dev->vbl_pending >= 100) {
 			spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
-			ret = -EBUSY;
-			goto done;
+			return -EBUSY;
 		}
 
-		spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
+		dev->vbl_pending++;
 
-		vbl_sig = drm_calloc(1, sizeof(struct drm_vbl_sig),
-				     DRM_MEM_DRIVER);
-		if (!vbl_sig) {
-			ret = -ENOMEM;
-			goto done;
-		}
+		spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
 
-		ret = drm_vblank_get(dev, crtc);
-		if (ret) {
-			drm_free(vbl_sig, sizeof(struct drm_vbl_sig),
-				 DRM_MEM_DRIVER);
-			return ret;
+		if (!
+		    (vbl_sig =
+		     drm_alloc(sizeof(struct drm_vbl_sig), DRM_MEM_DRIVER))) {
+			return -ENOMEM;
 		}
 
-		atomic_inc(&dev->vbl_signal_pending);
+		memset((void *)vbl_sig, 0, sizeof(*vbl_sig));
 
 		vbl_sig->sequence = vblwait->request.sequence;
 		vbl_sig->info.si_signo = vblwait->request.signal;
@@ -618,29 +451,20 @@
 
 		vblwait->reply.sequence = seq;
 	} else {
-		DRM_DEBUG("waiting on vblank count %d, crtc %d\n",
-			  vblwait->request.sequence, crtc);
-		DRM_WAIT_ON(ret, dev->vbl_queue[crtc], 3 * DRM_HZ,
-			    ((drm_vblank_count(dev, crtc)
-			      - vblwait->request.sequence) <= (1 << 23)));
-
-		if (ret != -EINTR) {
-			struct timeval now;
-
-			do_gettimeofday(&now);
-
-			vblwait->reply.tval_sec = now.tv_sec;
-			vblwait->reply.tval_usec = now.tv_usec;
-			vblwait->reply.sequence = drm_vblank_count(dev, crtc);
-			DRM_DEBUG("returning %d to client\n",
-				  vblwait->reply.sequence);
-		} else {
-			DRM_DEBUG("vblank wait interrupted by signal\n");
-		}
+		if (flags & _DRM_VBLANK_SECONDARY) {
+			if (dev->driver->vblank_wait2)
+				ret = dev->driver->vblank_wait2(dev, &vblwait->request.sequence);
+		} else if (dev->driver->vblank_wait)
+			ret =
+			    dev->driver->vblank_wait(dev,
+						     &vblwait->request.sequence);
+
+		do_gettimeofday(&now);
+		vblwait->reply.tval_sec = now.tv_sec;
+		vblwait->reply.tval_usec = now.tv_usec;
 	}
 
-done:
-	drm_vblank_put(dev, crtc);
+      done:
 	return ret;
 }
 
@@ -648,57 +472,43 @@
  * Send the VBLANK signals.
  *
  * \param dev DRM device.
- * \param crtc CRTC where the vblank event occurred
  *
  * Sends a signal for each task in drm_device::vbl_sigs and empties the list.
  *
  * If a signal is not requested, then calls vblank_wait().
  */
-static void drm_vbl_send_signals(struct drm_device *dev, int crtc)
+void drm_vbl_send_signals(struct drm_device * dev)
 {
-	struct drm_vbl_sig *vbl_sig, *tmp;
-	struct list_head *vbl_sigs;
-	unsigned int vbl_seq;
 	unsigned long flags;
+	int i;
 
 	spin_lock_irqsave(&dev->vbl_lock, flags);
 
-	vbl_sigs = &dev->vbl_sigs[crtc];
-	vbl_seq = drm_vblank_count(dev, crtc);
+	for (i = 0; i < 2; i++) {
+		struct drm_vbl_sig *vbl_sig, *tmp;
+		struct list_head *vbl_sigs = i ? &dev->vbl_sigs2 : &dev->vbl_sigs;
+		unsigned int vbl_seq = atomic_read(i ? &dev->vbl_received2 :
+						   &dev->vbl_received);
+
+		list_for_each_entry_safe(vbl_sig, tmp, vbl_sigs, head) {
+			if ((vbl_seq - vbl_sig->sequence) <= (1 << 23)) {
+				vbl_sig->info.si_code = vbl_seq;
+				send_sig_info(vbl_sig->info.si_signo,
+					      &vbl_sig->info, vbl_sig->task);
+
+				list_del(&vbl_sig->head);
 
-	list_for_each_entry_safe(vbl_sig, tmp, vbl_sigs, head) {
-	    if ((vbl_seq - vbl_sig->sequence) <= (1 << 23)) {
-		vbl_sig->info.si_code = vbl_seq;
-		send_sig_info(vbl_sig->info.si_signo,
-			      &vbl_sig->info, vbl_sig->task);
-
-		list_del(&vbl_sig->head);
-
-		drm_free(vbl_sig, sizeof(*vbl_sig),
-			 DRM_MEM_DRIVER);
-		atomic_dec(&dev->vbl_signal_pending);
-		drm_vblank_put(dev, crtc);
-	    }
+				drm_free(vbl_sig, sizeof(*vbl_sig),
+					 DRM_MEM_DRIVER);
+
+				dev->vbl_pending--;
+			}
+		}
 	}
 
 	spin_unlock_irqrestore(&dev->vbl_lock, flags);
 }
-
-/**
- * drm_handle_vblank - handle a vblank event
- * @dev: DRM device
- * @crtc: where this event occurred
- *
- * Drivers should call this routine in their vblank interrupt handlers to
- * update the vblank counter and send any signals that may be pending.
- */
-void drm_handle_vblank(struct drm_device *dev, int crtc)
-{
-	atomic_inc(&dev->_vblank_count[crtc]);
-	DRM_WAKEUP(&dev->vbl_queue[crtc]);
-	drm_vbl_send_signals(dev, crtc);
-}
-EXPORT_SYMBOL(drm_handle_vblank);
+EXPORT_SYMBOL(drm_vbl_send_signals);
 
 /**
  * Tasklet wrapper function.
Index: linux-2.6.27/drivers/gpu/drm/drm_mm.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_mm.c	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_mm.c	2009-02-05 13:29:33.000000000 +0000
@@ -38,7 +38,7 @@
  * Aligned allocations can also see improvement.
  *
  * Authors:
- * Thomas HellstrÃ¶m <thomas-at-tungstengraphics-dot-com>
+ * Thomas Hellström <thomas-at-tungstengraphics-dot-com>
  */
 
 #include "drmP.h"
Index: linux-2.6.27/drivers/gpu/drm/drm_modes.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_modes.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,560 @@
+/*
+ * Copyright Â© 1997-2003 by The XFree86 Project, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Except as contained in this notice, the name of the copyright holder(s)
+ * and author(s) shall not be used in advertising or otherwise to promote
+ * the sale, use or other dealings in this Software without prior written
+ * authorization from the copyright holder(s) and author(s).
+ */
+/*
+ * Copyright Â© 2007 Dave Airlie
+ */
+
+#include <linux/list.h>
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+
+/**
+ * drm_mode_debug_printmodeline - debug print a mode
+ * @dev: DRM device
+ * @mode: mode to print
+ *
+ * LOCKING:
+ * None.
+ *
+ * Describe @mode using DRM_DEBUG.
+ */
+void drm_mode_debug_printmodeline(struct drm_device *dev,
+				  struct drm_display_mode *mode)
+{
+	DRM_DEBUG("Modeline %d:\"%s\" %d %d %d %d %d %d %d %d %d %d 0x%x\n",
+		  mode->mode_id, mode->name, mode->vrefresh, mode->clock,
+		  mode->hdisplay, mode->hsync_start,
+		  mode->hsync_end, mode->htotal,
+		  mode->vdisplay, mode->vsync_start,
+		  mode->vsync_end, mode->vtotal, mode->type);
+}
+EXPORT_SYMBOL(drm_mode_debug_printmodeline);
+
+/**
+ * drm_mode_set_name - set the name on a mode
+ * @mode: name will be set in this mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Set the name of @mode to a standard format.
+ */
+void drm_mode_set_name(struct drm_display_mode *mode)
+{
+	snprintf(mode->name, DRM_DISPLAY_MODE_LEN, "%dx%d", mode->hdisplay,
+		 mode->vdisplay);
+}
+EXPORT_SYMBOL(drm_mode_set_name);
+
+/**
+ * drm_mode_list_concat - move modes from one list to another
+ * @head: source list
+ * @new: dst list
+ *
+ * LOCKING:
+ * Caller must ensure both lists are locked.
+ *
+ * Move all the modes from @head to @new.
+ */
+void drm_mode_list_concat(struct list_head *head, struct list_head *new)
+{
+
+	struct list_head *entry, *tmp;
+
+	list_for_each_safe(entry, tmp, head) {
+		list_move_tail(entry, new);
+	}
+}
+
+/**
+ * drm_mode_width - get the width of a mode
+ * @mode: mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Return @mode's width (hdisplay) value.
+ *
+ * FIXME: is this needed?
+ *
+ * RETURNS:
+ * @mode->hdisplay
+ */
+int drm_mode_width(struct drm_display_mode *mode)
+{
+	return mode->hdisplay;
+
+}
+EXPORT_SYMBOL(drm_mode_width);
+
+/**
+ * drm_mode_height - get the height of a mode
+ * @mode: mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Return @mode's height (vdisplay) value.
+ *
+ * FIXME: is this needed?
+ *
+ * RETURNS:
+ * @mode->vdisplay
+ */
+int drm_mode_height(struct drm_display_mode *mode)
+{
+	return mode->vdisplay;
+}
+EXPORT_SYMBOL(drm_mode_height);
+
+/**
+ * drm_mode_vrefresh - get the vrefresh of a mode
+ * @mode: mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Return @mode's vrefresh rate or calculate it if necessary.
+ *
+ * FIXME: why is this needed?  shouldn't vrefresh be set already?
+ *
+ * RETURNS:
+ * Vertical refresh rate of @mode x 1000. For precision reasons.
+ */
+int drm_mode_vrefresh(struct drm_display_mode *mode)
+{
+	int refresh = 0;
+	unsigned int calc_val;
+
+	if (mode->vrefresh > 0)
+		refresh = mode->vrefresh;
+	else if (mode->htotal > 0 && mode->vtotal > 0) {
+		/* work out vrefresh the value will be x1000 */
+		calc_val = (mode->clock * 1000);
+
+		calc_val /= mode->htotal;
+		calc_val *= 1000;
+		calc_val /= mode->vtotal;
+
+		refresh = calc_val;
+		if (mode->flags & V_INTERLACE)
+			refresh *= 2;
+		if (mode->flags & V_DBLSCAN)
+			refresh /= 2;
+		if (mode->vscan > 1)
+			refresh /= mode->vscan;
+	}
+	return refresh;
+}
+EXPORT_SYMBOL(drm_mode_vrefresh);
+
+/**
+ * drm_mode_set_crtcinfo - set CRTC modesetting parameters
+ * @p: mode
+ * @adjust_flags: unused? (FIXME)
+ *
+ * LOCKING:
+ * None.
+ *
+ * Setup the CRTC modesetting parameters for @p, adjusting if necessary.
+ */
+void drm_mode_set_crtcinfo(struct drm_display_mode *p, int adjust_flags)
+{
+	if ((p == NULL) || ((p->type & DRM_MODE_TYPE_CRTC_C) == DRM_MODE_TYPE_BUILTIN))
+		return;
+
+	p->crtc_hdisplay = p->hdisplay;
+	p->crtc_hsync_start = p->hsync_start;
+	p->crtc_hsync_end = p->hsync_end;
+	p->crtc_htotal = p->htotal;
+	p->crtc_hskew = p->hskew;
+	p->crtc_vdisplay = p->vdisplay;
+	p->crtc_vsync_start = p->vsync_start;
+	p->crtc_vsync_end = p->vsync_end;
+	p->crtc_vtotal = p->vtotal;
+
+	if (p->flags & V_INTERLACE) {
+		if (adjust_flags & CRTC_INTERLACE_HALVE_V) {
+			p->crtc_vdisplay /= 2;
+			p->crtc_vsync_start /= 2;
+			p->crtc_vsync_end /= 2;
+			p->crtc_vtotal /= 2;
+		}
+
+		p->crtc_vtotal |= 1;
+	}
+
+	if (p->flags & V_DBLSCAN) {
+		p->crtc_vdisplay *= 2;
+		p->crtc_vsync_start *= 2;
+		p->crtc_vsync_end *= 2;
+		p->crtc_vtotal *= 2;
+	}
+
+	if (p->vscan > 1) {
+		p->crtc_vdisplay *= p->vscan;
+		p->crtc_vsync_start *= p->vscan;
+		p->crtc_vsync_end *= p->vscan;
+		p->crtc_vtotal *= p->vscan;
+	}
+
+	p->crtc_vblank_start = min(p->crtc_vsync_start, p->crtc_vdisplay);
+	p->crtc_vblank_end = max(p->crtc_vsync_end, p->crtc_vtotal);
+	p->crtc_hblank_start = min(p->crtc_hsync_start, p->crtc_hdisplay);
+	p->crtc_hblank_end = max(p->crtc_hsync_end, p->crtc_htotal);
+
+	p->crtc_hadjusted = false;
+	p->crtc_vadjusted = false;
+}
+EXPORT_SYMBOL(drm_mode_set_crtcinfo);
+
+
+/**
+ * drm_mode_duplicate - allocate and duplicate an existing mode
+ * @m: mode to duplicate
+ *
+ * LOCKING:
+ * None.
+ *
+ * Just allocate a new mode, copy the existing mode into it, and return
+ * a pointer to it.  Used to create new instances of established modes.
+ */
+struct drm_display_mode *drm_mode_duplicate(struct drm_device *dev,
+					    struct drm_display_mode *mode)
+{
+	struct drm_display_mode *nmode;
+	int new_id;
+
+	nmode = drm_mode_create(dev);
+	if (!nmode)
+		return NULL;
+
+	new_id = nmode->mode_id;
+	*nmode = *mode;
+	nmode->mode_id = new_id;
+	INIT_LIST_HEAD(&nmode->head);
+	return nmode;
+}
+EXPORT_SYMBOL(drm_mode_duplicate);
+
+/**
+ * drm_mode_equal - test modes for equality
+ * @mode1: first mode
+ * @mode2: second mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Check to see if @mode1 and @mode2 are equivalent.
+ *
+ * RETURNS:
+ * True if the modes are equal, false otherwise.
+ */
+bool drm_mode_equal(struct drm_display_mode *mode1, struct drm_display_mode *mode2)
+{
+	if (mode1->clock == mode2->clock &&
+	    mode1->hdisplay == mode2->hdisplay &&
+	    mode1->hsync_start == mode2->hsync_start &&
+	    mode1->hsync_end == mode2->hsync_end &&
+	    mode1->htotal == mode2->htotal &&
+	    mode1->hskew == mode2->hskew &&
+	    mode1->vdisplay == mode2->vdisplay &&
+	    mode1->vsync_start == mode2->vsync_start &&
+	    mode1->vsync_end == mode2->vsync_end &&
+	    mode1->vtotal == mode2->vtotal &&
+	    mode1->vscan == mode2->vscan &&
+	    mode1->flags == mode2->flags)
+		return true;
+
+	return false;
+}
+EXPORT_SYMBOL(drm_mode_equal);
+
+/**
+ * drm_mode_validate_size - make sure modes adhere to size constraints
+ * @dev: DRM device
+ * @mode_list: list of modes to check
+ * @maxX: maximum width
+ * @maxY: maximum height
+ * @maxPitch: max pitch
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * The DRM device (@dev) has size and pitch limits.  Here we validate the
+ * modes we probed for @dev against those limits and set their status as
+ * necessary.
+ */
+void drm_mode_validate_size(struct drm_device *dev,
+			    struct list_head *mode_list,
+			    int maxX, int maxY, int maxPitch)
+{
+	struct drm_display_mode *mode;
+
+	list_for_each_entry(mode, mode_list, head) {
+		if (maxPitch > 0 && mode->hdisplay > maxPitch)
+			mode->status = MODE_BAD_WIDTH;
+
+		if (maxX > 0 && mode->hdisplay > maxX)
+			mode->status = MODE_VIRTUAL_X;
+
+		if (maxY > 0 && mode->vdisplay > maxY)
+			mode->status = MODE_VIRTUAL_Y;
+	}
+}
+EXPORT_SYMBOL(drm_mode_validate_size);
+
+/**
+ * drm_mode_validate_clocks - validate modes against clock limits
+ * @dev: DRM device
+ * @mode_list: list of modes to check
+ * @min: minimum clock rate array
+ * @max: maximum clock rate array
+ * @n_ranges: number of clock ranges (size of arrays)
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * Some code may need to check a mode list against the clock limits of the
+ * device in question.  This function walks the mode list, testing to make
+ * sure each mode falls within a given range (defined by @min and @max
+ * arrays) and sets @mode->status as needed.
+ */
+void drm_mode_validate_clocks(struct drm_device *dev,
+			      struct list_head *mode_list,
+			      int *min, int *max, int n_ranges)
+{
+	struct drm_display_mode *mode;
+	int i;
+
+	list_for_each_entry(mode, mode_list, head) {
+		bool good = false;
+		for (i = 0; i < n_ranges; i++) {
+			if (mode->clock >= min[i] && mode->clock <= max[i]) {
+				good = true;
+				break;
+			}
+		}
+		if (!good)
+			mode->status = MODE_CLOCK_RANGE;
+	}
+}
+EXPORT_SYMBOL(drm_mode_validate_clocks);
+
+/**
+ * drm_mode_prune_invalid - remove invalid modes from mode list
+ * @dev: DRM device
+ * @mode_list: list of modes to check
+ * @verbose: be verbose about it
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * Once mode list generation is complete, a caller can use this routine to
+ * remove invalid modes from a mode list.  If any of the modes have a
+ * status other than %MODE_OK, they are removed from @mode_list and freed.
+ */
+void drm_mode_prune_invalid(struct drm_device *dev,
+			    struct list_head *mode_list, bool verbose)
+{
+	struct drm_display_mode *mode, *t;
+
+	list_for_each_entry_safe(mode, t, mode_list, head) {
+		if (mode->status != MODE_OK) {
+			list_del(&mode->head);
+			if (verbose) {
+				drm_mode_debug_printmodeline(dev, mode);
+				DRM_DEBUG("Not using %s mode %d\n", mode->name, mode->status);
+			}
+			kfree(mode);
+		}
+	}
+}
+
+/**
+ * drm_mode_compare - compare modes for favorability
+ * @lh_a: list_head for first mode
+ * @lh_b: list_head for second mode
+ *
+ * LOCKING:
+ * None.
+ *
+ * Compare two modes, given by @lh_a and @lh_b, returning a value indicating
+ * which is better.
+ *
+ * RETURNS:
+ * Negative if @lh_a is better than @lh_b, zero if they're equivalent, or
+ * positive if @lh_b is better than @lh_a.
+ */
+static int drm_mode_compare(struct list_head *lh_a, struct list_head *lh_b)
+{
+	struct drm_display_mode *a = list_entry(lh_a, struct drm_display_mode, head);
+	struct drm_display_mode *b = list_entry(lh_b, struct drm_display_mode, head);
+	int diff;
+
+	diff = ((b->type & DRM_MODE_TYPE_PREFERRED) != 0) -
+		((a->type & DRM_MODE_TYPE_PREFERRED) != 0);
+	if (diff)
+		return diff;
+	diff = b->hdisplay * b->vdisplay - a->hdisplay * a->vdisplay;
+	if (diff)
+		return diff;
+	diff = b->clock - a->clock;
+	return diff;
+}
+
+/* FIXME: what we don't have a list sort function? */
+/* list sort from Mark J Roberts (mjr@znex.org) */
+void list_sort(struct list_head *head, int (*cmp)(struct list_head *a, struct list_head *b))
+{
+	struct list_head *p, *q, *e, *list, *tail, *oldhead;
+	int insize, nmerges, psize, qsize, i;
+
+	list = head->next;
+	list_del(head);
+	insize = 1;
+	for (;;) {
+		p = oldhead = list;
+		list = tail = NULL;
+		nmerges = 0;
+
+		while (p) {
+			nmerges++;
+			q = p;
+			psize = 0;
+			for (i = 0; i < insize; i++) {
+				psize++;
+				q = q->next == oldhead ? NULL : q->next;
+				if (!q)
+					break;
+			}
+
+			qsize = insize;
+			while (psize > 0 || (qsize > 0 && q)) {
+				if (!psize) {
+					e = q;
+					q = q->next;
+					qsize--;
+					if (q == oldhead)
+						q = NULL;
+				} else if (!qsize || !q) {
+					e = p;
+					p = p->next;
+					psize--;
+					if (p == oldhead)
+						p = NULL;
+				} else if (cmp(p, q) <= 0) {
+					e = p;
+					p = p->next;
+					psize--;
+					if (p == oldhead)
+						p = NULL;
+				} else {
+					e = q;
+					q = q->next;
+					qsize--;
+					if (q == oldhead)
+						q = NULL;
+				}
+				if (tail)
+					tail->next = e;
+				else
+					list = e;
+				e->prev = tail;
+				tail = e;
+			}
+			p = q;
+		}
+
+		tail->next = list;
+		list->prev = tail;
+
+		if (nmerges <= 1)
+			break;
+
+		insize *= 2;
+	}
+
+	head->next = list;
+	head->prev = list->prev;
+	list->prev->next = head;
+	list->prev = head;
+}
+
+/**
+ * drm_mode_sort - sort mode list
+ * @mode_list: list to sort
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * Sort @mode_list by favorability, putting good modes first.
+ */
+void drm_mode_sort(struct list_head *mode_list)
+{
+	list_sort(mode_list, drm_mode_compare);
+}
+
+
+/**
+ * drm_mode_output_list_update - update the mode list for the output
+ * @output: the output to update
+ *
+ * LOCKING:
+ * Caller must hold a lock protecting @mode_list.
+ *
+ * This moves the modes from the @output probed_modes list
+ * to the actual mode list. It compares the probed mode against the current
+ * list and only adds different modes. All modes unverified after this point
+ * will be removed by the prune invalid modes.
+ */
+void drm_mode_output_list_update(struct drm_output *output)
+{
+	struct drm_display_mode *mode;
+	struct drm_display_mode *pmode, *pt;
+	int found_it;
+	list_for_each_entry_safe(pmode, pt, &output->probed_modes,
+				 head) {
+		found_it = 0;
+		/* go through current modes checking for the new probed mode */
+		list_for_each_entry(mode, &output->modes, head) {
+			if (drm_mode_equal(pmode, mode)) {
+				found_it = 1;
+				/* if equal delete the probed mode */
+				mode->status = pmode->status;
+				list_del(&pmode->head);
+				kfree(pmode);
+				break;
+			}
+		}
+
+		if (!found_it) {
+			list_move_tail(&pmode->head, &output->modes);
+		}
+	}
+}
Index: linux-2.6.27/drivers/gpu/drm/drm_object.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_object.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,294 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+
+int drm_add_user_object(struct drm_file *priv, struct drm_user_object *item,
+			int shareable)
+{
+	struct drm_device *dev = priv->minor->dev;
+	int ret;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	/* The refcount will be bumped to 1 when we add the ref object below. */
+	atomic_set(&item->refcount, 0);
+	item->shareable = shareable;
+	item->owner = priv;
+
+	ret = drm_ht_just_insert_please(&dev->object_hash, &item->hash,
+					(unsigned long)item, 32, 0, 0);
+	if (ret)
+		return ret;
+
+	ret = drm_add_ref_object(priv, item, _DRM_REF_USE);
+	if (ret)
+		ret = drm_ht_remove_item(&dev->object_hash, &item->hash);
+
+	return ret;
+}
+EXPORT_SYMBOL(drm_add_user_object);
+
+struct drm_user_object *drm_lookup_user_object(struct drm_file *priv, uint32_t key)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_hash_item *hash;
+	int ret;
+	struct drm_user_object *item;
+
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+
+	ret = drm_ht_find_item(&dev->object_hash, key, &hash);
+	if (ret)
+		return NULL;
+
+	item = drm_hash_entry(hash, struct drm_user_object, hash);
+
+	if (priv != item->owner) {
+		struct drm_open_hash *ht = &priv->refd_object_hash[_DRM_REF_USE];
+		ret = drm_ht_find_item(ht, (unsigned long)item, &hash);
+		if (ret) {
+			DRM_ERROR("Object not registered for usage\n");
+			return NULL;
+		}
+	}
+	return item;
+}
+EXPORT_SYMBOL(drm_lookup_user_object);
+
+static void drm_deref_user_object(struct drm_file *priv, struct drm_user_object *item)
+{
+	struct drm_device *dev = priv->minor->dev;
+	int ret;
+
+	if (atomic_dec_and_test(&item->refcount)) {
+		ret = drm_ht_remove_item(&dev->object_hash, &item->hash);
+		BUG_ON(ret);
+		item->remove(priv, item);
+	}
+}
+
+static int drm_object_ref_action(struct drm_file *priv, struct drm_user_object *ro,
+				 enum drm_ref_type action)
+{
+	int ret = 0;
+
+	switch (action) {
+	case _DRM_REF_USE:
+		atomic_inc(&ro->refcount);
+		break;
+	default:
+		if (!ro->ref_struct_locked) {
+			break;
+		} else {
+			ro->ref_struct_locked(priv, ro, action);
+		}
+	}
+	return ret;
+}
+
+int drm_add_ref_object(struct drm_file *priv, struct drm_user_object *referenced_object,
+		       enum drm_ref_type ref_action)
+{
+	int ret = 0;
+	struct drm_ref_object *item;
+	struct drm_open_hash *ht = &priv->refd_object_hash[ref_action];
+
+	DRM_ASSERT_LOCKED(&priv->minor->dev->struct_mutex);
+	if (!referenced_object->shareable && priv != referenced_object->owner) {
+		DRM_ERROR("Not allowed to reference this object\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * If this is not a usage reference, Check that usage has been registered
+	 * first. Otherwise strange things may happen on destruction.
+	 */
+
+	if ((ref_action != _DRM_REF_USE) && priv != referenced_object->owner) {
+		item =
+		    drm_lookup_ref_object(priv, referenced_object,
+					  _DRM_REF_USE);
+		if (!item) {
+			DRM_ERROR
+			    ("Object not registered for usage by this client\n");
+			return -EINVAL;
+		}
+	}
+
+	if (NULL !=
+	    (item =
+	     drm_lookup_ref_object(priv, referenced_object, ref_action))) {
+		atomic_inc(&item->refcount);
+		return drm_object_ref_action(priv, referenced_object,
+					     ref_action);
+	}
+
+	item = drm_calloc(1, sizeof(*item), DRM_MEM_OBJECTS);
+	if (item == NULL) {
+		DRM_ERROR("Could not allocate reference object\n");
+		return -ENOMEM;
+	}
+
+	atomic_set(&item->refcount, 1);
+	item->hash.key = (unsigned long)referenced_object;
+	ret = drm_ht_insert_item(ht, &item->hash);
+	item->unref_action = ref_action;
+
+	if (ret)
+		goto out;
+
+	list_add(&item->list, &priv->refd_objects);
+	ret = drm_object_ref_action(priv, referenced_object, ref_action);
+out:
+	return ret;
+}
+
+struct drm_ref_object *drm_lookup_ref_object(struct drm_file *priv,
+					struct drm_user_object *referenced_object,
+					enum drm_ref_type ref_action)
+{
+	struct drm_hash_item *hash;
+	int ret;
+
+	DRM_ASSERT_LOCKED(&priv->minor->dev->struct_mutex);
+	ret = drm_ht_find_item(&priv->refd_object_hash[ref_action],
+			       (unsigned long)referenced_object, &hash);
+	if (ret)
+		return NULL;
+
+	return drm_hash_entry(hash, struct drm_ref_object, hash);
+}
+EXPORT_SYMBOL(drm_lookup_ref_object);
+
+static void drm_remove_other_references(struct drm_file *priv,
+					struct drm_user_object *ro)
+{
+	int i;
+	struct drm_open_hash *ht;
+	struct drm_hash_item *hash;
+	struct drm_ref_object *item;
+
+	for (i = _DRM_REF_USE + 1; i < _DRM_NO_REF_TYPES; ++i) {
+		ht = &priv->refd_object_hash[i];
+		while (!drm_ht_find_item(ht, (unsigned long)ro, &hash)) {
+			item = drm_hash_entry(hash, struct drm_ref_object, hash);
+			drm_remove_ref_object(priv, item);
+		}
+	}
+}
+
+void drm_remove_ref_object(struct drm_file *priv, struct drm_ref_object *item)
+{
+	int ret;
+	struct drm_user_object *user_object = (struct drm_user_object *) item->hash.key;
+	struct drm_open_hash *ht = &priv->refd_object_hash[item->unref_action];
+	enum drm_ref_type unref_action;
+
+	DRM_ASSERT_LOCKED(&priv->minor->dev->struct_mutex);
+	unref_action = item->unref_action;
+	if (atomic_dec_and_test(&item->refcount)) {
+		ret = drm_ht_remove_item(ht, &item->hash);
+		BUG_ON(ret);
+		list_del_init(&item->list);
+		if (unref_action == _DRM_REF_USE)
+			drm_remove_other_references(priv, user_object);
+		drm_free(item, sizeof(*item), DRM_MEM_OBJECTS);
+	}
+
+	switch (unref_action) {
+	case _DRM_REF_USE:
+		drm_deref_user_object(priv, user_object);
+		break;
+	default:
+		BUG_ON(!user_object->unref);
+		user_object->unref(priv, user_object, unref_action);
+		break;
+	}
+
+}
+EXPORT_SYMBOL(drm_remove_ref_object);
+
+int drm_user_object_ref(struct drm_file *priv, uint32_t user_token,
+			enum drm_object_type type, struct drm_user_object **object)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_user_object *uo;
+	struct drm_hash_item *hash;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_ht_find_item(&dev->object_hash, user_token, &hash);
+	if (ret) {
+		DRM_ERROR("Could not find user object to reference.\n");
+		goto out_err;
+	}
+	uo = drm_hash_entry(hash, struct drm_user_object, hash);
+	if (uo->type != type) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+	ret = drm_add_ref_object(priv, uo, _DRM_REF_USE);
+	if (ret)
+		goto out_err;
+	mutex_unlock(&dev->struct_mutex);
+	*object = uo;
+	return 0;
+out_err:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+int drm_user_object_unref(struct drm_file *priv, uint32_t user_token,
+			  enum drm_object_type type)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_user_object *uo;
+	struct drm_ref_object *ro;
+	int ret;
+
+	mutex_lock(&dev->struct_mutex);
+	uo = drm_lookup_user_object(priv, user_token);
+	if (!uo || (uo->type != type)) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+	ro = drm_lookup_ref_object(priv, uo, _DRM_REF_USE);
+	if (!ro) {
+		ret = -EINVAL;
+		goto out_err;
+	}
+	drm_remove_ref_object(priv, ro);
+	mutex_unlock(&dev->struct_mutex);
+	return 0;
+out_err:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
Index: linux-2.6.27/drivers/gpu/drm/drm_regman.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_regman.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,200 @@
+/**************************************************************************
+ * Copyright (c) 2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * An allocate-fence manager implementation intended for sets of base-registers
+ * or tiling-registers.
+ */
+
+#include "drmP.h"
+
+/*
+ * Allocate a compatible register and put it on the unfenced list.
+ */
+
+int drm_regs_alloc(struct drm_reg_manager *manager,
+		   const void *data,
+		   uint32_t fence_class,
+		   uint32_t fence_type,
+		   int interruptible, int no_wait, struct drm_reg **reg)
+{
+	struct drm_reg *entry, *next_entry;
+	int ret;
+
+	*reg = NULL;
+
+	/*
+	 * Search the unfenced list.
+	 */
+
+	list_for_each_entry(entry, &manager->unfenced, head) {
+		if (manager->reg_reusable(entry, data)) {
+			entry->new_fence_type |= fence_type;
+			goto out;
+		}
+	}
+
+	/*
+	 * Search the lru list.
+	 */
+
+	list_for_each_entry_safe(entry, next_entry, &manager->lru, head) {
+		struct drm_fence_object *fence = entry->fence;
+		if (fence->fence_class == fence_class &&
+		    (entry->fence_type & fence_type) == entry->fence_type &&
+		    manager->reg_reusable(entry, data)) {
+			list_del(&entry->head);
+			entry->new_fence_type = fence_type;
+			list_add_tail(&entry->head, &manager->unfenced);
+			goto out;
+		}
+	}
+
+	/*
+	 * Search the free list.
+	 */
+
+	list_for_each_entry(entry, &manager->free, head) {
+		list_del(&entry->head);
+		entry->new_fence_type = fence_type;
+		list_add_tail(&entry->head, &manager->unfenced);
+		goto out;
+	}
+
+	if (no_wait)
+		return -EBUSY;
+
+	/*
+	 * Go back to the lru list and try to expire fences.
+	 */
+
+	list_for_each_entry_safe(entry, next_entry, &manager->lru, head) {
+		BUG_ON(!entry->fence);
+		ret = drm_fence_object_wait(entry->fence, 0, !interruptible,
+					    entry->fence_type);
+		if (ret)
+			return ret;
+
+		drm_fence_usage_deref_unlocked(&entry->fence);
+		list_del(&entry->head);
+		entry->new_fence_type = fence_type;
+		list_add_tail(&entry->head, &manager->unfenced);
+		goto out;
+	}
+
+	/*
+	 * Oops. All registers are used up :(.
+	 */
+
+	return -EBUSY;
+out:
+	*reg = entry;
+	return 0;
+}
+EXPORT_SYMBOL(drm_regs_alloc);
+
+void drm_regs_fence(struct drm_reg_manager *manager,
+		    struct drm_fence_object *fence)
+{
+	struct drm_reg *entry;
+	struct drm_reg *next_entry;
+
+	if (!fence) {
+
+		/*
+		 * Old fence (if any) is still valid.
+		 * Put back on free and lru lists.
+		 */
+
+		list_for_each_entry_safe_reverse(entry, next_entry,
+						 &manager->unfenced, head) {
+			list_del(&entry->head);
+			list_add(&entry->head, (entry->fence) ?
+				 &manager->lru : &manager->free);
+		}
+	} else {
+
+		/*
+		 * Fence with a new fence and put on lru list.
+		 */
+
+		list_for_each_entry_safe(entry, next_entry, &manager->unfenced,
+					 head) {
+			list_del(&entry->head);
+			if (entry->fence)
+				drm_fence_usage_deref_unlocked(&entry->fence);
+			drm_fence_reference_unlocked(&entry->fence, fence);
+
+			entry->fence_type = entry->new_fence_type;
+			BUG_ON((entry->fence_type & fence->type) !=
+			       entry->fence_type);
+
+			list_add_tail(&entry->head, &manager->lru);
+		}
+	}
+}
+EXPORT_SYMBOL(drm_regs_fence);
+
+void drm_regs_free(struct drm_reg_manager *manager)
+{
+	struct drm_reg *entry;
+	struct drm_reg *next_entry;
+
+	drm_regs_fence(manager, NULL);
+
+	list_for_each_entry_safe(entry, next_entry, &manager->free, head) {
+		list_del(&entry->head);
+		manager->reg_destroy(entry);
+	}
+
+	list_for_each_entry_safe(entry, next_entry, &manager->lru, head) {
+
+		(void)drm_fence_object_wait(entry->fence, 1, 1,
+					    entry->fence_type);
+		list_del(&entry->head);
+		drm_fence_usage_deref_unlocked(&entry->fence);
+		manager->reg_destroy(entry);
+	}
+}
+EXPORT_SYMBOL(drm_regs_free);
+
+void drm_regs_add(struct drm_reg_manager *manager, struct drm_reg *reg)
+{
+	reg->fence = NULL;
+	list_add_tail(&reg->head, &manager->free);
+}
+EXPORT_SYMBOL(drm_regs_add);
+
+void drm_regs_init(struct drm_reg_manager *manager,
+		   int (*reg_reusable) (const struct drm_reg *, const void *),
+		   void (*reg_destroy) (struct drm_reg *))
+{
+	INIT_LIST_HEAD(&manager->free);
+	INIT_LIST_HEAD(&manager->lru);
+	INIT_LIST_HEAD(&manager->unfenced);
+	manager->reg_reusable = reg_reusable;
+	manager->reg_destroy = reg_destroy;
+}
+EXPORT_SYMBOL(drm_regs_init);
Index: linux-2.6.27/drivers/gpu/drm/drm_sman.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_sman.c	2008-10-09 23:13:53.000000000 +0100
+++ linux-2.6.27/drivers/gpu/drm/drm_sman.c	2009-02-05 13:29:33.000000000 +0000
@@ -33,7 +33,7 @@
  * struct or a context identifier.
  *
  * Authors:
- * Thomas HellstrÃ¶m <thomas-at-tungstengraphics-dot-com>
+ * Thomas Hellström <thomas-at-tungstengraphics-dot-com>
  */
 
 #include "drm_sman.h"
Index: linux-2.6.27/drivers/gpu/drm/drm_stub.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_stub.c	2009-02-05 13:29:29.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_stub.c	2009-02-05 13:29:33.000000000 +0000
@@ -97,6 +97,7 @@
 	init_timer(&dev->timer);
 	mutex_init(&dev->struct_mutex);
 	mutex_init(&dev->ctxlist_mutex);
+	mutex_init(&dev->bm.evict_mutex);
 
 	idr_init(&dev->drw_idr);
 
@@ -113,6 +114,18 @@
 		return -ENOMEM;
 	}
 
+	if (drm_mm_init(&dev->offset_manager, DRM_FILE_PAGE_OFFSET_START,
+			DRM_FILE_PAGE_OFFSET_SIZE)) {
+		drm_ht_remove(&dev->map_hash);
+		return -ENOMEM;
+	}
+
+	if (drm_ht_create(&dev->object_hash, DRM_OBJECT_HASH_ORDER)) {
+		drm_ht_remove(&dev->map_hash);
+		drm_mm_takedown(&dev->offset_manager);
+		return -ENOMEM;
+	}
+
 	/* the DRM has 6 basic counters */
 	dev->counters = 6;
 	dev->types[0] = _DRM_STAT_LOCK;
@@ -152,15 +165,7 @@
 		goto error_out_unreg;
 	}
 
-	if (driver->driver_features & DRIVER_GEM) {
-		retcode = drm_gem_init(dev);
-		if (retcode) {
-			DRM_ERROR("Cannot initialize graphics execution "
-				  "manager (GEM)\n");
-			goto error_out_unreg;
-		}
-	}
-
+	drm_fence_manager_init(dev);
 	return 0;
 
       error_out_unreg:
@@ -284,6 +289,8 @@
 	drm_free(dev, sizeof(*dev), DRM_MEM_STUB);
 	return ret;
 }
+EXPORT_SYMBOL(drm_get_dev);
+
 
 /**
  * Put a device minor number.
Index: linux-2.6.27/drivers/gpu/drm/drm_ttm.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/drm_ttm.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,430 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+#include <asm/agp.h>
+
+static void drm_ttm_ipi_handler(void *null)
+{
+	flush_agp_cache();
+}
+
+void drm_ttm_cache_flush(void)
+{
+	if (on_each_cpu(drm_ttm_ipi_handler, NULL, 1) != 0)
+		DRM_ERROR("Timed out waiting for drm cache flush.\n");
+}
+EXPORT_SYMBOL(drm_ttm_cache_flush);
+
+/*
+ * Use kmalloc if possible. Otherwise fall back to vmalloc.
+ */
+
+static void ttm_alloc_pages(struct drm_ttm *ttm)
+{
+	unsigned long size = ttm->num_pages * sizeof(*ttm->pages);
+	ttm->pages = NULL;
+
+	if (size <= PAGE_SIZE)
+		ttm->pages = drm_calloc(1, size, DRM_MEM_TTM);
+
+	if (!ttm->pages) {
+		ttm->pages = vmalloc_user(size);
+		if (ttm->pages)
+			ttm->page_flags |= DRM_TTM_PAGE_VMALLOC;
+	}
+}
+
+static void ttm_free_pages(struct drm_ttm *ttm)
+{
+	unsigned long size = ttm->num_pages * sizeof(*ttm->pages);
+
+	if (ttm->page_flags & DRM_TTM_PAGE_VMALLOC) {
+		vfree(ttm->pages);
+		ttm->page_flags &= ~DRM_TTM_PAGE_VMALLOC;
+	} else {
+		drm_free(ttm->pages, size, DRM_MEM_TTM);
+	}
+	ttm->pages = NULL;
+}
+
+static struct page *drm_ttm_alloc_page(void)
+{
+	struct page *page;
+
+	page = alloc_page(GFP_KERNEL | __GFP_ZERO | GFP_DMA32);
+	if (!page)
+		return NULL;
+	return page;
+}
+
+/*
+ * Change caching policy for the linear kernel map
+ * for range of pages in a ttm.
+ */
+
+static int drm_set_caching(struct drm_ttm *ttm, int noncached)
+{
+	int i;
+	struct page **cur_page;
+	int do_tlbflush = 0;
+
+	if ((ttm->page_flags & DRM_TTM_PAGE_UNCACHED) == noncached)
+		return 0;
+
+	if (noncached)
+		drm_ttm_cache_flush();
+
+	for (i = 0; i < ttm->num_pages; ++i) {
+		cur_page = ttm->pages + i;
+		if (*cur_page) {
+			if (!PageHighMem(*cur_page)) {
+				if (noncached) {
+					map_page_into_agp(*cur_page);
+				} else {
+					unmap_page_from_agp(*cur_page);
+				}
+				do_tlbflush = 1;
+			}
+		}
+	}
+	//if (do_tlbflush)
+	//	flush_agp_mappings();
+
+	DRM_FLAG_MASKED(ttm->page_flags, noncached, DRM_TTM_PAGE_UNCACHED);
+
+	return 0;
+}
+
+
+static void drm_ttm_free_user_pages(struct drm_ttm *ttm)
+{
+	int write;
+	int dirty;
+	struct page *page;
+	int i;
+
+	BUG_ON(!(ttm->page_flags & DRM_TTM_PAGE_USER));
+	write = ((ttm->page_flags & DRM_TTM_PAGE_USER_WRITE) != 0);
+	dirty = ((ttm->page_flags & DRM_TTM_PAGE_USER_DIRTY) != 0);
+
+	for (i = 0; i < ttm->num_pages; ++i) {
+		page = ttm->pages[i];
+		if (page == NULL)
+			continue;
+
+		if (page == ttm->dummy_read_page) {
+			BUG_ON(write);
+			continue;
+		}
+
+		if (write && dirty && !PageReserved(page))
+			set_page_dirty_lock(page);
+
+		ttm->pages[i] = NULL;
+		put_page(page);
+	}
+}
+
+static void drm_ttm_free_alloced_pages(struct drm_ttm *ttm)
+{
+	int i;
+	struct drm_buffer_manager *bm = &ttm->dev->bm;
+	struct page **cur_page;
+
+	for (i = 0; i < ttm->num_pages; ++i) {
+		cur_page = ttm->pages + i;
+		if (*cur_page) {
+			if (page_count(*cur_page) != 1)
+				DRM_ERROR("Erroneous page count. Leaking pages.\n");
+			if (page_mapped(*cur_page))
+				DRM_ERROR("Erroneous map count. Leaking page mappings.\n");
+			__free_page(*cur_page);
+			--bm->cur_pages;
+		}
+	}
+}
+
+/*
+ * Free all resources associated with a ttm.
+ */
+
+int drm_destroy_ttm(struct drm_ttm *ttm)
+{
+	struct drm_ttm_backend *be;
+
+	if (!ttm)
+		return 0;
+
+	be = ttm->be;
+	if (be) {
+		be->func->destroy(be);
+		ttm->be = NULL;
+	}
+
+	if (ttm->pages) {
+		if (ttm->page_flags & DRM_TTM_PAGE_UNCACHED)
+			drm_set_caching(ttm, 0);
+
+		if (ttm->page_flags & DRM_TTM_PAGE_USER)
+			drm_ttm_free_user_pages(ttm);
+		else
+			drm_ttm_free_alloced_pages(ttm);
+
+		ttm_free_pages(ttm);
+	}
+
+	return 0;
+}
+
+struct page *drm_ttm_get_page(struct drm_ttm *ttm, int index)
+{
+	struct page *p;
+	struct drm_buffer_manager *bm = &ttm->dev->bm;
+
+	p = ttm->pages[index];
+	if (!p) {
+		p = drm_ttm_alloc_page();
+		if (!p)
+			return NULL;
+		ttm->pages[index] = p;
+		++bm->cur_pages;
+	}
+	return p;
+}
+EXPORT_SYMBOL(drm_ttm_get_page);
+
+int drm_ttm_set_user(struct drm_ttm *ttm,
+		     struct task_struct *tsk,
+		     int write,
+		     unsigned long start,
+		     unsigned long num_pages,
+		     struct page *dummy_read_page)
+{
+	struct mm_struct *mm = tsk->mm;
+	int ret;
+	int i;
+
+	BUG_ON(num_pages != ttm->num_pages);
+
+	ttm->dummy_read_page = dummy_read_page;
+	ttm->page_flags |= DRM_TTM_PAGE_USER |
+		((write) ? DRM_TTM_PAGE_USER_WRITE : 0);
+
+
+	down_read(&mm->mmap_sem);
+	ret = get_user_pages(tsk, mm, start, num_pages,
+			     write, 0, ttm->pages, NULL);
+	up_read(&mm->mmap_sem);
+
+	if (ret != num_pages && write) {
+		drm_ttm_free_user_pages(ttm);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < num_pages; ++i) {
+		if (ttm->pages[i] == NULL)
+			ttm->pages[i] = ttm->dummy_read_page;
+	}
+
+	return 0;
+}
+
+int drm_ttm_populate(struct drm_ttm *ttm)
+{
+	struct page *page;
+	unsigned long i;
+	struct drm_ttm_backend *be;
+
+	if (ttm->state != ttm_unpopulated)
+		return 0;
+
+	be = ttm->be;
+	for (i = 0; i < ttm->num_pages; ++i) {
+		page = drm_ttm_get_page(ttm, i);
+		if (!page)
+			return -ENOMEM;
+	}
+	be->func->populate(be, ttm->num_pages, ttm->pages);
+	ttm->state = ttm_unbound;
+	return 0;
+}
+
+static inline size_t drm_size_align(size_t size)
+{
+	size_t tmpSize = 4;
+	if (size > PAGE_SIZE)
+		return PAGE_ALIGN(size);
+	while (tmpSize < size)
+		tmpSize <<= 1;
+
+	return (size_t) tmpSize;
+}
+
+/*
+ * Calculate the estimated pinned memory usage of a ttm.
+ */
+
+unsigned long drm_ttm_size(struct drm_device *dev,
+			   unsigned long num_pages,
+			   int user_bo)
+{
+	struct drm_bo_driver *bo_driver = dev->driver->bo_driver;
+	unsigned long tmp;
+
+	tmp = drm_size_align(sizeof(struct drm_ttm)) +
+		drm_size_align(num_pages * sizeof(struct page *)) +
+		((user_bo) ? 0 : drm_size_align(num_pages * PAGE_SIZE));
+
+	if (bo_driver->backend_size)
+		tmp += bo_driver->backend_size(dev, num_pages);
+	else
+		tmp += drm_size_align(num_pages * sizeof(struct page *)) +
+			3*drm_size_align(sizeof(struct drm_ttm_backend));
+	return tmp;
+}
+
+
+/*
+ * Initialize a ttm.
+ */
+
+struct drm_ttm *drm_ttm_init(struct drm_device *dev, unsigned long size)
+{
+	struct drm_bo_driver *bo_driver = dev->driver->bo_driver;
+	struct drm_ttm *ttm;
+
+	if (!bo_driver)
+		return NULL;
+
+	ttm = drm_calloc(1, sizeof(*ttm), DRM_MEM_TTM);
+	if (!ttm)
+		return NULL;
+
+	ttm->dev = dev;
+	atomic_set(&ttm->vma_count, 0);
+
+	ttm->destroy = 0;
+	ttm->num_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+
+	ttm->page_flags = 0;
+
+	/*
+	 * Account also for AGP module memory usage.
+	 */
+
+	ttm_alloc_pages(ttm);
+	if (!ttm->pages) {
+		drm_destroy_ttm(ttm);
+		DRM_ERROR("Failed allocating page table\n");
+		return NULL;
+	}
+	ttm->be = bo_driver->create_ttm_backend_entry(dev);
+	if (!ttm->be) {
+		drm_destroy_ttm(ttm);
+		DRM_ERROR("Failed creating ttm backend entry\n");
+		return NULL;
+	}
+	ttm->state = ttm_unpopulated;
+	return ttm;
+}
+
+/*
+ * Unbind a ttm region from the aperture.
+ */
+
+void drm_ttm_evict(struct drm_ttm *ttm)
+{
+	struct drm_ttm_backend *be = ttm->be;
+	int ret;
+
+	if (ttm->state == ttm_bound) {
+		ret = be->func->unbind(be);
+		BUG_ON(ret);
+	}
+
+	ttm->state = ttm_evicted;
+}
+
+void drm_ttm_fixup_caching(struct drm_ttm *ttm)
+{
+
+	if (ttm->state == ttm_evicted) {
+		struct drm_ttm_backend *be = ttm->be;
+		if (be->func->needs_ub_cache_adjust(be))
+			drm_set_caching(ttm, 0);
+		ttm->state = ttm_unbound;
+	}
+}
+
+void drm_ttm_unbind(struct drm_ttm *ttm)
+{
+	if (ttm->state == ttm_bound)
+		drm_ttm_evict(ttm);
+
+	drm_ttm_fixup_caching(ttm);
+}
+
+int drm_bind_ttm(struct drm_ttm *ttm, struct drm_bo_mem_reg *bo_mem)
+{
+	struct drm_bo_driver *bo_driver = ttm->dev->driver->bo_driver;
+	int ret = 0;
+	struct drm_ttm_backend *be;
+
+	if (!ttm)
+		return -EINVAL;
+	if (ttm->state == ttm_bound)
+		return 0;
+
+	be = ttm->be;
+
+	ret = drm_ttm_populate(ttm);
+	if (ret)
+		return ret;
+
+	if (ttm->state == ttm_unbound && !(bo_mem->flags & DRM_BO_FLAG_CACHED))
+		drm_set_caching(ttm, DRM_TTM_PAGE_UNCACHED);
+	else if ((bo_mem->flags & DRM_BO_FLAG_CACHED_MAPPED) &&
+		   bo_driver->ttm_cache_flush)
+		bo_driver->ttm_cache_flush(ttm);
+
+	ret = be->func->bind(be, bo_mem);
+	if (ret) {
+		ttm->state = ttm_evicted;
+		DRM_ERROR("Couldn't bind backend.\n");
+		return ret;
+	}
+
+	ttm->state = ttm_bound;
+	if (ttm->page_flags & DRM_TTM_PAGE_USER)
+		ttm->page_flags |= DRM_TTM_PAGE_USER_DIRTY;
+	return 0;
+}
+EXPORT_SYMBOL(drm_bind_ttm);
Index: linux-2.6.27/drivers/gpu/drm/drm_vm.c
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/drm_vm.c	2008-10-09 23:13:53.000000000 +0100
+++ linux-2.6.27/drivers/gpu/drm/drm_vm.c	2009-02-05 13:29:33.000000000 +0000
@@ -40,6 +40,10 @@
 
 static void drm_vm_open(struct vm_area_struct *vma);
 static void drm_vm_close(struct vm_area_struct *vma);
+static int drm_bo_mmap_locked(struct vm_area_struct *vma,
+			      struct file *filp,
+			      drm_local_map_t *map);
+
 
 static pgprot_t drm_io_prot(uint32_t map_type, struct vm_area_struct *vma)
 {
@@ -267,6 +271,9 @@
 				dmah.size = map->size;
 				__drm_pci_free(dev, &dmah);
 				break;
+			case _DRM_TTM:
+				BUG_ON(1);
+				break;
 			}
 			drm_free(map, sizeof(*map), DRM_MEM_MAPS);
 		}
@@ -647,6 +654,8 @@
 		vma->vm_flags |= VM_RESERVED;
 		vma->vm_page_prot = drm_dma_prot(map->type, vma);
 		break;
+	case _DRM_TTM:
+		return drm_bo_mmap_locked(vma, filp, map);
 	default:
 		return -EINVAL;	/* This should never happen. */
 	}
@@ -671,3 +680,213 @@
 	return ret;
 }
 EXPORT_SYMBOL(drm_mmap);
+
+/**
+ * buffer object vm functions.
+ */
+
+/**
+ * \c Pagefault method for buffer objects.
+ *
+ * \param vma Virtual memory area.
+ * \param address File offset.
+ * \return Error or refault. The pfn is manually inserted.
+ *
+ * It's important that pfns are inserted while holding the bo->mutex lock.
+ * otherwise we might race with unmap_mapping_range() which is always
+ * called with the bo->mutex lock held.
+ *
+ * We're modifying the page attribute bits of the vma->vm_page_prot field,
+ * without holding the mmap_sem in write mode. Only in read mode.
+ * These bits are not used by the mm subsystem code, and we consider them
+ * protected by the bo->mutex lock.
+ */
+
+#define DRM_NOPFN_EXTRA 15 /* Fault 16 pages at a time in */
+
+int drm_bo_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+	unsigned long page_offset;
+	struct page *page = NULL;
+	struct drm_ttm *ttm = NULL;
+	struct drm_device *dev;
+	unsigned long pfn;
+	int err;
+	unsigned long bus_base;
+	unsigned long bus_offset;
+	unsigned long bus_size;
+	int i;
+	unsigned long ret = VM_FAULT_NOPAGE;
+	unsigned long address = (unsigned long)vmf->virtual_address;
+
+	if (address > vma->vm_end)
+		return VM_FAULT_SIGBUS;
+
+	dev = bo->dev;
+	err = drm_bo_read_lock(&dev->bm.bm_lock);
+	if (err)
+		return VM_FAULT_NOPAGE;
+
+	err = mutex_lock_interruptible(&bo->mutex);
+	if (err) {
+		drm_bo_read_unlock(&dev->bm.bm_lock);
+		return VM_FAULT_NOPAGE;
+	}
+
+	err = drm_bo_wait(bo, 0, 0, 0);
+	if (err) {
+		ret = (err != -EAGAIN) ? VM_FAULT_SIGBUS : VM_FAULT_NOPAGE;
+		goto out_unlock;
+	}
+
+	/*
+	 * If buffer happens to be in a non-mappable location,
+	 * move it to a mappable.
+	 */
+
+	if (!(bo->mem.flags & DRM_BO_FLAG_MAPPABLE)) {
+		uint32_t new_mask = bo->mem.mask |
+			DRM_BO_FLAG_MAPPABLE |
+			DRM_BO_FLAG_FORCE_MAPPABLE;
+		err = drm_bo_move_buffer(bo, new_mask, 0, 0);
+		if (err) {
+			ret = (err != -EAGAIN) ? VM_FAULT_SIGBUS : VM_FAULT_NOPAGE;
+			goto out_unlock;
+		}
+	}
+
+	err = drm_bo_pci_offset(dev, &bo->mem, &bus_base, &bus_offset,
+				&bus_size);
+
+	if (err) {
+		ret = VM_FAULT_SIGBUS;
+		goto out_unlock;
+	}
+
+	page_offset = (address - vma->vm_start) >> PAGE_SHIFT;
+
+	if (bus_size) {
+		struct drm_mem_type_manager *man = &dev->bm.man[bo->mem.mem_type];
+
+		pfn = ((bus_base + bus_offset) >> PAGE_SHIFT) + page_offset;
+		vma->vm_page_prot = drm_io_prot(man->drm_bus_maptype, vma);
+	} else {
+		ttm = bo->ttm;
+
+		drm_ttm_fixup_caching(ttm);
+		page = drm_ttm_get_page(ttm, page_offset);
+		if (!page) {
+			ret = VM_FAULT_OOM;
+			goto out_unlock;
+		}
+		pfn = page_to_pfn(page);
+		vma->vm_page_prot = (bo->mem.flags & DRM_BO_FLAG_CACHED) ?
+			vm_get_page_prot(vma->vm_flags) :
+			drm_io_prot(_DRM_TTM, vma);
+	}
+
+	err = vm_insert_pfn(vma, address, pfn);
+	if (err) {
+		ret = (err != -EAGAIN) ? VM_FAULT_OOM : VM_FAULT_NOPAGE;
+		goto out_unlock;
+	}
+
+	for (i=0; i<DRM_NOPFN_EXTRA; ++i) {
+
+		if (++page_offset == bo->mem.num_pages)
+			break;
+		address = vma->vm_start + (page_offset << PAGE_SHIFT);
+		if (address >= vma->vm_end)
+			break;
+		if (bus_size) {
+			pfn = ((bus_base + bus_offset) >> PAGE_SHIFT)
+				+ page_offset;
+		} else {
+			page = drm_ttm_get_page(ttm, page_offset);
+			if (!page)
+				break;
+			pfn = page_to_pfn(page);
+		}
+		if (vm_insert_pfn(vma, address, pfn))
+			break;
+	}
+out_unlock:
+	mutex_unlock(&bo->mutex);
+	drm_bo_read_unlock(&dev->bm.bm_lock);
+	return ret;
+}
+EXPORT_SYMBOL(drm_bo_vm_fault);
+
+static void drm_bo_vm_open_locked(struct vm_area_struct *vma)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+
+	drm_vm_open_locked(vma);
+	atomic_inc(&bo->usage);
+}
+
+/**
+ * \c vma open method for buffer objects.
+ *
+ * \param vma virtual memory area.
+ */
+
+static void drm_bo_vm_open(struct vm_area_struct *vma)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+	struct drm_device *dev = bo->dev;
+
+	mutex_lock(&dev->struct_mutex);
+	drm_bo_vm_open_locked(vma);
+	mutex_unlock(&dev->struct_mutex);
+}
+
+/**
+ * \c vma close method for buffer objects.
+ *
+ * \param vma virtual memory area.
+ */
+
+static void drm_bo_vm_close(struct vm_area_struct *vma)
+{
+	struct drm_buffer_object *bo = (struct drm_buffer_object *) vma->vm_private_data;
+	struct drm_device *dev = bo->dev;
+
+	drm_vm_close(vma);
+	if (bo) {
+		mutex_lock(&dev->struct_mutex);
+		drm_bo_usage_deref_locked((struct drm_buffer_object **)
+					  &vma->vm_private_data);
+		mutex_unlock(&dev->struct_mutex);
+	}
+	return;
+}
+
+static struct vm_operations_struct drm_bo_vm_ops = {
+	.fault = drm_bo_vm_fault,
+	.open = drm_bo_vm_open,
+	.close = drm_bo_vm_close,
+};
+
+/**
+ * mmap buffer object memory.
+ *
+ * \param vma virtual memory area.
+ * \param file_priv DRM file private.
+ * \param map The buffer object drm map.
+ * \return zero on success or a negative number on failure.
+ */
+
+int drm_bo_mmap_locked(struct vm_area_struct *vma,
+		       struct file *filp,
+		       drm_local_map_t *map)
+{
+	vma->vm_ops = &drm_bo_vm_ops;
+	vma->vm_private_data = map->handle;
+	vma->vm_file = filp;
+	vma->vm_flags |= VM_RESERVED | VM_IO;
+	vma->vm_flags |= VM_PFNMAP;
+	drm_bo_vm_open_locked(vma);
+	return 0;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/Makefile
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/Makefile	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,13 @@
+#
+# Makefile for the drm device driver.  This driver provides support for the
+# Direct Rendering Infrastructure (DRI) in XFree86 4.1.0 and higher.
+
+ccflags-y := -Iinclude/drm
+
+psb-y := psb_drv.o psb_mmu.o psb_sgx.o psb_irq.o psb_fence.o psb_buffer.o \
+		psb_gtt.o psb_setup.o psb_i2c.o psb_fb.o psb_msvdx.o \
+		psb_msvdxinit.o psb_regman.o psb_reset.o psb_scene.o \
+		psb_schedule.o psb_xhw.o
+
+
+obj-$(CONFIG_DRM_PSB)	+= psb.o
Index: linux-2.6.27/drivers/gpu/drm/psb/i915_drv.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/i915_drv.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,795 @@
+/* i915_drv.h -- Private header for the I915 driver -*- linux-c -*-
+ */
+/*
+ *
+ * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef _I915_DRV_H_
+#define _I915_DRV_H_
+
+#include "i915_reg.h"
+
+/* General customization:
+ */
+
+#define DRIVER_AUTHOR		"Tungsten Graphics, Inc."
+
+#define DRIVER_NAME		"i915"
+#define DRIVER_DESC		"Intel Graphics"
+#define DRIVER_DATE		"20070209"
+
+#if defined(__linux__)
+#define I915_HAVE_FENCE
+#define I915_HAVE_BUFFER
+#endif
+
+/* Interface history:
+ *
+ * 1.1: Original.
+ * 1.2: Add Power Management
+ * 1.3: Add vblank support
+ * 1.4: Fix cmdbuffer path, add heap destroy
+ * 1.5: Add vblank pipe configuration
+ * 1.6: - New ioctl for scheduling buffer swaps on vertical blank
+ *      - Support vertical blank on secondary display pipe
+ * 1.8: New ioctl for ARB_Occlusion_Query
+ * 1.9: Usable page flipping and triple buffering
+ * 1.10: Plane/pipe disentangling
+ * 1.11: TTM superioctl
+ */
+#define DRIVER_MAJOR		1
+#if defined(I915_HAVE_FENCE) && defined(I915_HAVE_BUFFER)
+#define DRIVER_MINOR		11
+#else
+#define DRIVER_MINOR		6
+#endif
+#define DRIVER_PATCHLEVEL	0
+
+#define DRM_DRIVER_PRIVATE_T struct drm_i915_private
+
+#ifdef I915_HAVE_BUFFER
+#define I915_MAX_VALIDATE_BUFFERS 4096
+#endif
+
+struct drm_i915_ring_buffer {
+	int tail_mask;
+	unsigned long Start;
+	unsigned long End;
+	unsigned long Size;
+	u8 *virtual_start;
+	int head;
+	int tail;
+	int space;
+	drm_local_map_t map;
+};
+
+struct mem_block {
+	struct mem_block *next;
+	struct mem_block *prev;
+	int start;
+	int size;
+	struct drm_file *file_priv; /* NULL: free, -1: heap, other: real files */
+};
+
+struct drm_i915_vbl_swap {
+	struct list_head head;
+	drm_drawable_t drw_id;
+	unsigned int plane;
+	unsigned int sequence;
+	int flip;
+};
+
+struct drm_i915_private {
+	struct drm_buffer_object *ring_buffer;
+	drm_local_map_t *sarea;
+	drm_local_map_t *mmio_map;
+
+	unsigned long mmiobase;
+	unsigned long mmiolen;
+
+	struct drm_i915_sarea *sarea_priv;
+	struct drm_i915_ring_buffer ring;
+
+	struct drm_dma_handle *status_page_dmah;
+	void *hw_status_page;
+	dma_addr_t dma_status_page;
+	uint32_t counter;
+	unsigned int status_gfx_addr;
+	drm_local_map_t hws_map;
+
+	unsigned int cpp;
+	int use_mi_batchbuffer_start;
+
+	wait_queue_head_t irq_queue;
+	atomic_t irq_received;
+	atomic_t irq_emitted;
+
+	int tex_lru_log_granularity;
+	int allow_batchbuffer;
+	struct mem_block *agp_heap;
+	unsigned int sr01, adpa, ppcr, dvob, dvoc, lvds;
+	int vblank_pipe;
+	DRM_SPINTYPE user_irq_lock;
+	int user_irq_refcount;
+	int fence_irq_on;
+	uint32_t irq_enable_reg;
+	int irq_enabled;
+
+#ifdef I915_HAVE_FENCE
+	uint32_t flush_sequence;
+	uint32_t flush_flags;
+	uint32_t flush_pending;
+	uint32_t saved_flush_status;
+	uint32_t reported_sequence;
+	int reported_sequence_valid;
+#endif
+#ifdef I915_HAVE_BUFFER
+	void *agp_iomap;
+	unsigned int max_validate_buffers;
+	struct mutex cmdbuf_mutex;
+#endif
+
+	DRM_SPINTYPE swaps_lock;
+	struct drm_i915_vbl_swap vbl_swaps;
+	unsigned int swaps_pending;
+
+	/* LVDS info */
+	int backlight_duty_cycle;  /* restore backlight to this value */
+	bool panel_wants_dither;
+	struct drm_display_mode *panel_fixed_mode;
+
+ 	/* Register state */
+	u8 saveLBB;
+	u32 saveDSPACNTR;
+	u32 saveDSPBCNTR;
+	u32 savePIPEACONF;
+	u32 savePIPEBCONF;
+	u32 savePIPEASRC;
+	u32 savePIPEBSRC;
+	u32 saveFPA0;
+	u32 saveFPA1;
+	u32 saveDPLL_A;
+	u32 saveDPLL_A_MD;
+	u32 saveHTOTAL_A;
+	u32 saveHBLANK_A;
+	u32 saveHSYNC_A;
+	u32 saveVTOTAL_A;
+	u32 saveVBLANK_A;
+	u32 saveVSYNC_A;
+	u32 saveBCLRPAT_A;
+	u32 saveDSPASTRIDE;
+	u32 saveDSPASIZE;
+	u32 saveDSPAPOS;
+	u32 saveDSPABASE;
+	u32 saveDSPASURF;
+	u32 saveDSPATILEOFF;
+	u32 savePFIT_PGM_RATIOS;
+	u32 saveBLC_PWM_CTL;
+	u32 saveBLC_PWM_CTL2;
+	u32 saveFPB0;
+	u32 saveFPB1;
+	u32 saveDPLL_B;
+	u32 saveDPLL_B_MD;
+	u32 saveHTOTAL_B;
+	u32 saveHBLANK_B;
+	u32 saveHSYNC_B;
+	u32 saveVTOTAL_B;
+	u32 saveVBLANK_B;
+	u32 saveVSYNC_B;
+	u32 saveBCLRPAT_B;
+	u32 saveDSPBSTRIDE;
+	u32 saveDSPBSIZE;
+	u32 saveDSPBPOS;
+	u32 saveDSPBBASE;
+	u32 saveDSPBSURF;
+	u32 saveDSPBTILEOFF;
+	u32 saveVCLK_DIVISOR_VGA0;
+	u32 saveVCLK_DIVISOR_VGA1;
+	u32 saveVCLK_POST_DIV;
+	u32 saveVGACNTRL;
+	u32 saveADPA;
+	u32 saveLVDS;
+	u32 saveLVDSPP_ON;
+	u32 saveLVDSPP_OFF;
+	u32 saveDVOA;
+	u32 saveDVOB;
+	u32 saveDVOC;
+	u32 savePP_ON;
+	u32 savePP_OFF;
+	u32 savePP_CONTROL;
+	u32 savePP_CYCLE;
+	u32 savePFIT_CONTROL;
+	u32 save_palette_a[256];
+	u32 save_palette_b[256];
+	u32 saveFBC_CFB_BASE;
+	u32 saveFBC_LL_BASE;
+	u32 saveFBC_CONTROL;
+	u32 saveFBC_CONTROL2;
+	u32 saveSWF0[16];
+	u32 saveSWF1[16];
+	u32 saveSWF2[3];
+	u8 saveMSR;
+	u8 saveSR[8];
+	u8 saveGR[24];
+	u8 saveAR_INDEX;
+	u8 saveAR[20];
+	u8 saveDACMASK;
+	u8 saveDACDATA[256*3]; /* 256 3-byte colors */
+	u8 saveCR[36];
+};
+
+enum intel_chip_family {
+	CHIP_I8XX = 0x01,
+	CHIP_I9XX = 0x02,
+	CHIP_I915 = 0x04,
+	CHIP_I965 = 0x08,
+	CHIP_POULSBO = 0x10,
+};
+
+extern struct drm_ioctl_desc i915_ioctls[];
+extern int i915_max_ioctl;
+
+				/* i915_dma.c */
+extern void i915_kernel_lost_context(struct drm_device * dev);
+extern int i915_driver_load(struct drm_device *, unsigned long flags);
+extern int i915_driver_unload(struct drm_device *dev);
+extern void i915_driver_lastclose(struct drm_device * dev);
+extern void i915_driver_preclose(struct drm_device *dev,
+				 struct drm_file *file_priv);
+extern int i915_driver_device_is_agp(struct drm_device * dev);
+extern long i915_compat_ioctl(struct file *filp, unsigned int cmd,
+			      unsigned long arg);
+extern void i915_emit_breadcrumb(struct drm_device *dev);
+extern void i915_dispatch_flip(struct drm_device * dev, int pipes, int sync);
+extern int i915_emit_mi_flush(struct drm_device *dev, uint32_t flush);
+extern int i915_driver_firstopen(struct drm_device *dev);
+extern int i915_do_cleanup_pageflip(struct drm_device *dev);
+extern int i915_dma_cleanup(struct drm_device *dev);
+
+/* i915_irq.c */
+extern int i915_irq_emit(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv);
+extern int i915_irq_wait(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv);
+
+extern void i915_driver_wait_next_vblank(struct drm_device *dev, int pipe);
+extern int i915_driver_vblank_wait(struct drm_device *dev, unsigned int *sequence);
+extern int i915_driver_vblank_wait2(struct drm_device *dev, unsigned int *sequence);
+extern irqreturn_t i915_driver_irq_handler(DRM_IRQ_ARGS);
+extern void i915_driver_irq_preinstall(struct drm_device * dev);
+extern void i915_driver_irq_postinstall(struct drm_device * dev);
+extern void i915_driver_irq_uninstall(struct drm_device * dev);
+extern int i915_vblank_pipe_set(struct drm_device *dev, void *data,
+				struct drm_file *file_priv);
+extern int i915_vblank_pipe_get(struct drm_device *dev, void *data,
+				struct drm_file *file_priv);
+extern int i915_emit_irq(struct drm_device * dev);
+extern void i915_user_irq_on(struct drm_i915_private *dev_priv);
+extern void i915_user_irq_off(struct drm_i915_private *dev_priv);
+extern void i915_enable_interrupt (struct drm_device *dev);
+extern int i915_vblank_swap(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv);
+
+/* i915_mem.c */
+extern int i915_mem_alloc(struct drm_device *dev, void *data,
+			  struct drm_file *file_priv);
+extern int i915_mem_free(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv);
+extern int i915_mem_init_heap(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv);
+extern int i915_mem_destroy_heap(struct drm_device *dev, void *data,
+				 struct drm_file *file_priv);
+extern void i915_mem_takedown(struct mem_block **heap);
+extern void i915_mem_release(struct drm_device * dev,
+			     struct drm_file *file_priv,
+			     struct mem_block *heap);
+#ifdef I915_HAVE_FENCE
+/* i915_fence.c */
+extern void i915_fence_handler(struct drm_device *dev);
+extern void i915_invalidate_reported_sequence(struct drm_device *dev);
+
+#endif
+
+#ifdef I915_HAVE_BUFFER
+/* i915_buffer.c */
+extern struct drm_ttm_backend *i915_create_ttm_backend_entry(struct drm_device *dev);
+extern int i915_fence_types(struct drm_buffer_object *bo, uint32_t *fclass,
+			    uint32_t *type);
+extern int i915_invalidate_caches(struct drm_device *dev, uint64_t buffer_flags);
+extern int i915_init_mem_type(struct drm_device *dev, uint32_t type,
+			       struct drm_mem_type_manager *man);
+extern uint32_t i915_evict_mask(struct drm_buffer_object *bo);
+extern int i915_move(struct drm_buffer_object *bo, int evict,
+		int no_wait, struct drm_bo_mem_reg *new_mem);
+void i915_flush_ttm(struct drm_ttm *ttm);
+#endif
+
+#ifdef __linux__
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+extern void intel_init_chipset_flush_compat(struct drm_device *dev);
+extern void intel_fini_chipset_flush_compat(struct drm_device *dev);
+#endif
+#endif
+
+
+/* modesetting */
+extern void intel_modeset_init(struct drm_device *dev);
+extern void intel_modeset_cleanup(struct drm_device *dev);
+
+
+#define I915_READ(reg)          DRM_READ32(dev_priv->mmio_map, (reg))
+#define I915_WRITE(reg,val)     DRM_WRITE32(dev_priv->mmio_map, (reg), (val))
+#define I915_READ16(reg)	DRM_READ16(dev_priv->mmio_map, (reg))
+#define I915_WRITE16(reg,val)	DRM_WRITE16(dev_priv->mmio_map, (reg), (val))
+
+#define I915_VERBOSE 0
+
+#define RING_LOCALS	unsigned int outring, ringmask, outcount; \
+			volatile char *virt;
+
+#define BEGIN_LP_RING(n) do {				\
+	if (I915_VERBOSE)				\
+		DRM_DEBUG("BEGIN_LP_RING(%d) in %s\n",	\
+	                         (n), __FUNCTION__);           \
+	if (dev_priv->ring.space < (n)*4)                      \
+		i915_wait_ring(dev, (n)*4, __FUNCTION__);      \
+	outcount = 0;					\
+	outring = dev_priv->ring.tail;			\
+	ringmask = dev_priv->ring.tail_mask;		\
+	virt = dev_priv->ring.virtual_start;		\
+} while (0)
+
+#define OUT_RING(n) do {					\
+	if (I915_VERBOSE) DRM_DEBUG("   OUT_RING %x\n", (int)(n));	\
+	*(volatile unsigned int *)(virt + outring) = (n);		\
+	outcount++;						\
+	outring += 4;						\
+	outring &= ringmask;					\
+} while (0)
+
+#define ADVANCE_LP_RING() do {						\
+	if (I915_VERBOSE) DRM_DEBUG("ADVANCE_LP_RING %x\n", outring);	\
+	dev_priv->ring.tail = outring;					\
+	dev_priv->ring.space -= outcount * 4;				\
+	I915_WRITE(LP_RING + RING_TAIL, outring);			\
+} while(0)
+
+#define MI_NOOP	(0x00 << 23)
+
+extern int i915_wait_ring(struct drm_device * dev, int n, const char *caller);
+
+/*
+ * The Bridge device's PCI config space has information about the
+ * fb aperture size and the amount of pre-reserved memory.
+ */
+#define INTEL_GMCH_CTRL		0x52
+#define INTEL_GMCH_ENABLED	0x4
+#define INTEL_GMCH_MEM_MASK	0x1
+#define INTEL_GMCH_MEM_64M	0x1
+#define INTEL_GMCH_MEM_128M	0
+
+#define INTEL_855_GMCH_GMS_MASK		(0x7 << 4)
+#define INTEL_855_GMCH_GMS_DISABLED	(0x0 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_1M	(0x1 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_4M	(0x2 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_8M	(0x3 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_16M	(0x4 << 4)
+#define INTEL_855_GMCH_GMS_STOLEN_32M	(0x5 << 4)
+
+#define INTEL_915G_GMCH_GMS_STOLEN_48M	(0x6 << 4)
+#define INTEL_915G_GMCH_GMS_STOLEN_64M	(0x7 << 4)
+
+/* Extended config space */
+#define LBB 0xf4
+
+/* VGA stuff */
+
+#define VGA_ST01_MDA 0x3ba
+#define VGA_ST01_CGA 0x3da
+
+#define VGA_MSR_WRITE 0x3c2
+#define VGA_MSR_READ 0x3cc
+#define   VGA_MSR_MEM_EN (1<<1)
+#define   VGA_MSR_CGA_MODE (1<<0)
+
+#define VGA_SR_INDEX 0x3c4
+#define VGA_SR_DATA 0x3c5
+
+#define VGA_AR_INDEX 0x3c0
+#define   VGA_AR_VID_EN (1<<5)
+#define VGA_AR_DATA_WRITE 0x3c0
+#define VGA_AR_DATA_READ 0x3c1
+
+#define VGA_GR_INDEX 0x3ce
+#define VGA_GR_DATA 0x3cf
+/* GR05 */
+#define   VGA_GR_MEM_READ_MODE_SHIFT 3
+#define     VGA_GR_MEM_READ_MODE_PLANE 1
+/* GR06 */
+#define   VGA_GR_MEM_MODE_MASK 0xc
+#define   VGA_GR_MEM_MODE_SHIFT 2
+#define   VGA_GR_MEM_A0000_AFFFF 0
+#define   VGA_GR_MEM_A0000_BFFFF 1
+#define   VGA_GR_MEM_B0000_B7FFF 2
+#define   VGA_GR_MEM_B0000_BFFFF 3
+
+#define VGA_DACMASK 0x3c6
+#define VGA_DACRX 0x3c7
+#define VGA_DACWX 0x3c8
+#define VGA_DACDATA 0x3c9
+
+#define VGA_CR_INDEX_MDA 0x3b4
+#define VGA_CR_DATA_MDA 0x3b5
+#define VGA_CR_INDEX_CGA 0x3d4
+#define VGA_CR_DATA_CGA 0x3d5
+
+#define GFX_OP_USER_INTERRUPT		((0<<29)|(2<<23))
+#define GFX_OP_BREAKPOINT_INTERRUPT	((0<<29)|(1<<23))
+#define CMD_REPORT_HEAD			(7<<23)
+#define CMD_STORE_DWORD_IDX		((0x21<<23) | 0x1)
+#define CMD_OP_BATCH_BUFFER  ((0x0<<29)|(0x30<<23)|0x1)
+
+#define CMD_MI_FLUSH         (0x04 << 23)
+#define MI_NO_WRITE_FLUSH    (1 << 2)
+#define MI_READ_FLUSH        (1 << 0)
+#define MI_EXE_FLUSH         (1 << 1)
+#define MI_END_SCENE         (1 << 4) /* flush binner and incr scene count */
+#define MI_SCENE_COUNT       (1 << 3) /* just increment scene count */
+
+/* Packet to load a register value from the ring/batch command stream:
+ */
+#define CMD_MI_LOAD_REGISTER_IMM	((0x22 << 23)|0x1)
+
+#define BB1_START_ADDR_MASK   (~0x7)
+#define BB1_PROTECTED         (1<<0)
+#define BB1_UNPROTECTED       (0<<0)
+#define BB2_END_ADDR_MASK     (~0x7)
+
+#define I915REG_HWS_PGA		0x02080
+
+/* Framebuffer compression */
+#define FBC_CFB_BASE		0x03200 /* 4k page aligned */
+#define FBC_LL_BASE		0x03204 /* 4k page aligned */
+#define FBC_CONTROL		0x03208
+#define   FBC_CTL_EN		(1<<31)
+#define   FBC_CTL_PERIODIC	(1<<30)
+#define   FBC_CTL_INTERVAL_SHIFT (16)
+#define   FBC_CTL_UNCOMPRESSIBLE (1<<14)
+#define   FBC_CTL_STRIDE_SHIFT	(5)
+#define   FBC_CTL_FENCENO	(1<<0)
+#define FBC_COMMAND		0x0320c
+#define   FBC_CMD_COMPRESS	(1<<0)
+#define FBC_STATUS		0x03210
+#define   FBC_STAT_COMPRESSING	(1<<31)
+#define   FBC_STAT_COMPRESSED	(1<<30)
+#define   FBC_STAT_MODIFIED	(1<<29)
+#define   FBC_STAT_CURRENT_LINE	(1<<0)
+#define FBC_CONTROL2		0x03214
+#define   FBC_CTL_FENCE_DBL	(0<<4)
+#define   FBC_CTL_IDLE_IMM	(0<<2)
+#define   FBC_CTL_IDLE_FULL	(1<<2)
+#define   FBC_CTL_IDLE_LINE	(2<<2)
+#define   FBC_CTL_IDLE_DEBUG	(3<<2)
+#define   FBC_CTL_CPU_FENCE	(1<<1)
+#define   FBC_CTL_PLANEA	(0<<0)
+#define   FBC_CTL_PLANEB	(1<<0)
+#define FBC_FENCE_OFF		0x0321b
+
+#define FBC_LL_SIZE		(1536)
+#define FBC_LL_PAD		(32)
+
+/* Interrupt bits:
+ */
+#define USER_INT_FLAG    (1<<1)
+#define VSYNC_PIPEB_FLAG (1<<5)
+#define VSYNC_PIPEA_FLAG (1<<7)
+#define HWB_OOM_FLAG     (1<<13) /* binner out of memory */
+
+#define I915REG_HWSTAM		0x02098
+#define I915REG_INT_IDENTITY_R	0x020a4
+#define I915REG_INT_MASK_R	0x020a8
+#define I915REG_INT_ENABLE_R	0x020a0
+#define I915REG_INSTPM	        0x020c0
+
+#define I915REG_PIPEASTAT	0x70024
+#define I915REG_PIPEBSTAT	0x71024
+
+#define I915_VBLANK_INTERRUPT_ENABLE	(1UL<<17)
+#define I915_VBLANK_CLEAR		(1UL<<1)
+
+#define GPIOA			0x5010
+#define GPIOB			0x5014
+#define GPIOC			0x5018
+#define GPIOD			0x501c
+#define GPIOE			0x5020
+#define GPIOF			0x5024
+#define GPIOG			0x5028
+#define GPIOH			0x502c
+# define GPIO_CLOCK_DIR_MASK		(1 << 0)
+# define GPIO_CLOCK_DIR_IN		(0 << 1)
+# define GPIO_CLOCK_DIR_OUT		(1 << 1)
+# define GPIO_CLOCK_VAL_MASK		(1 << 2)
+# define GPIO_CLOCK_VAL_OUT		(1 << 3)
+# define GPIO_CLOCK_VAL_IN		(1 << 4)
+# define GPIO_CLOCK_PULLUP_DISABLE	(1 << 5)
+# define GPIO_DATA_DIR_MASK		(1 << 8)
+# define GPIO_DATA_DIR_IN		(0 << 9)
+# define GPIO_DATA_DIR_OUT		(1 << 9)
+# define GPIO_DATA_VAL_MASK		(1 << 10)
+# define GPIO_DATA_VAL_OUT		(1 << 11)
+# define GPIO_DATA_VAL_IN		(1 << 12)
+# define GPIO_DATA_PULLUP_DISABLE	(1 << 13)
+
+/* p317, 319
+ */
+#define VCLK2_VCO_M        0x6008 /* treat as 16 bit? (includes msbs) */
+#define VCLK2_VCO_N        0x600a
+#define VCLK2_VCO_DIV_SEL  0x6012
+
+#define VCLK_DIVISOR_VGA0   0x6000
+#define VCLK_DIVISOR_VGA1   0x6004
+#define VCLK_POST_DIV	    0x6010
+/** Selects a post divisor of 4 instead of 2. */
+# define VGA1_PD_P2_DIV_4	(1 << 15)
+/** Overrides the p2 post divisor field */
+# define VGA1_PD_P1_DIV_2	(1 << 13)
+# define VGA1_PD_P1_SHIFT	8
+/** P1 value is 2 greater than this field */
+# define VGA1_PD_P1_MASK	(0x1f << 8)
+/** Selects a post divisor of 4 instead of 2. */
+# define VGA0_PD_P2_DIV_4	(1 << 7)
+/** Overrides the p2 post divisor field */
+# define VGA0_PD_P1_DIV_2	(1 << 5)
+# define VGA0_PD_P1_SHIFT	0
+/** P1 value is 2 greater than this field */
+# define VGA0_PD_P1_MASK	(0x1f << 0)
+
+#define POST_DIV_SELECT        0x70
+#define POST_DIV_1             0x00
+#define POST_DIV_2             0x10
+#define POST_DIV_4             0x20
+#define POST_DIV_8             0x30
+#define POST_DIV_16            0x40
+#define POST_DIV_32            0x50
+#define VCO_LOOP_DIV_BY_4M     0x00
+#define VCO_LOOP_DIV_BY_16M    0x04
+
+#define SRX_INDEX		0x3c4
+#define SRX_DATA		0x3c5
+#define SR01			1
+#define SR01_SCREEN_OFF		(1<<5)
+
+#define PPCR			0x61204
+#define PPCR_ON			(1<<0)
+
+#define DVOA			0x61120
+#define DVOA_ON			(1<<31)
+#define DVOB			0x61140
+#define DVOB_ON			(1<<31)
+#define DVOC			0x61160
+#define DVOC_ON			(1<<31)
+#define LVDS			0x61180
+#define LVDS_ON			(1<<31)
+
+#define ADPA			0x61100
+#define ADPA_DPMS_MASK		(~(3<<10))
+#define ADPA_DPMS_ON		(0<<10)
+#define ADPA_DPMS_SUSPEND	(1<<10)
+#define ADPA_DPMS_STANDBY	(2<<10)
+#define ADPA_DPMS_OFF		(3<<10)
+
+#define NOPID                   0x2094
+#define LP_RING			0x2030
+#define HP_RING			0x2040
+/* The binner has its own ring buffer:
+ */
+#define HWB_RING		0x2400
+
+#define RING_TAIL		0x00
+#define TAIL_ADDR		0x001FFFF8
+#define RING_HEAD		0x04
+#define HEAD_WRAP_COUNT		0xFFE00000
+#define HEAD_WRAP_ONE		0x00200000
+#define HEAD_ADDR		0x001FFFFC
+#define RING_START		0x08
+#define START_ADDR		0x0xFFFFF000
+#define RING_LEN		0x0C
+#define RING_NR_PAGES		0x001FF000
+#define RING_REPORT_MASK	0x00000006
+#define RING_REPORT_64K		0x00000002
+#define RING_REPORT_128K	0x00000004
+#define RING_NO_REPORT		0x00000000
+#define RING_VALID_MASK		0x00000001
+#define RING_VALID		0x00000001
+#define RING_INVALID		0x00000000
+
+/* Instruction parser error reg:
+ */
+#define IPEIR			0x2088
+
+/* Scratch pad debug 0 reg:
+ */
+#define SCPD0			0x209c
+
+/* Error status reg:
+ */
+#define ESR			0x20b8
+
+/* Secondary DMA fetch address debug reg:
+ */
+#define DMA_FADD_S		0x20d4
+
+/* Cache mode 0 reg.
+ *  - Manipulating render cache behaviour is central
+ *    to the concept of zone rendering, tuning this reg can help avoid
+ *    unnecessary render cache reads and even writes (for z/stencil)
+ *    at beginning and end of scene.
+ *
+ * - To change a bit, write to this reg with a mask bit set and the
+ * bit of interest either set or cleared.  EG: (BIT<<16) | BIT to set.
+ */
+#define Cache_Mode_0		0x2120
+#define CM0_MASK_SHIFT          16
+#define CM0_IZ_OPT_DISABLE      (1<<6)
+#define CM0_ZR_OPT_DISABLE      (1<<5)
+#define CM0_DEPTH_EVICT_DISABLE (1<<4)
+#define CM0_COLOR_EVICT_DISABLE (1<<3)
+#define CM0_DEPTH_WRITE_DISABLE (1<<1)
+#define CM0_RC_OP_FLUSH_DISABLE (1<<0)
+
+
+/* Graphics flush control.  A CPU write flushes the GWB of all writes.
+ * The data is discarded.
+ */
+#define GFX_FLSH_CNTL		0x2170
+
+/* Binner control.  Defines the location of the bin pointer list:
+ */
+#define BINCTL			0x2420
+#define BC_MASK			(1 << 9)
+
+/* Binned scene info.
+ */
+#define BINSCENE		0x2428
+#define BS_OP_LOAD		(1 << 8)
+#define BS_MASK			(1 << 22)
+
+/* Bin command parser debug reg:
+ */
+#define BCPD			0x2480
+
+/* Bin memory control debug reg:
+ */
+#define BMCD			0x2484
+
+/* Bin data cache debug reg:
+ */
+#define BDCD			0x2488
+
+/* Binner pointer cache debug reg:
+ */
+#define BPCD			0x248c
+
+/* Binner scratch pad debug reg:
+ */
+#define BINSKPD			0x24f0
+
+/* HWB scratch pad debug reg:
+ */
+#define HWBSKPD			0x24f4
+
+/* Binner memory pool reg:
+ */
+#define BMP_BUFFER		0x2430
+#define BMP_PAGE_SIZE_4K	(0 << 10)
+#define BMP_BUFFER_SIZE_SHIFT	1
+#define BMP_ENABLE		(1 << 0)
+
+/* Get/put memory from the binner memory pool:
+ */
+#define BMP_GET			0x2438
+#define BMP_PUT			0x2440
+#define BMP_OFFSET_SHIFT	5
+
+/* 3D state packets:
+ */
+#define GFX_OP_RASTER_RULES    ((0x3<<29)|(0x7<<24))
+
+#define GFX_OP_SCISSOR         ((0x3<<29)|(0x1c<<24)|(0x10<<19))
+#define SC_UPDATE_SCISSOR       (0x1<<1)
+#define SC_ENABLE_MASK          (0x1<<0)
+#define SC_ENABLE               (0x1<<0)
+
+#define GFX_OP_LOAD_INDIRECT   ((0x3<<29)|(0x1d<<24)|(0x7<<16))
+
+#define GFX_OP_SCISSOR_INFO    ((0x3<<29)|(0x1d<<24)|(0x81<<16)|(0x1))
+#define SCI_YMIN_MASK      (0xffff<<16)
+#define SCI_XMIN_MASK      (0xffff<<0)
+#define SCI_YMAX_MASK      (0xffff<<16)
+#define SCI_XMAX_MASK      (0xffff<<0)
+
+#define GFX_OP_SCISSOR_ENABLE	 ((0x3<<29)|(0x1c<<24)|(0x10<<19))
+#define GFX_OP_SCISSOR_RECT	 ((0x3<<29)|(0x1d<<24)|(0x81<<16)|1)
+#define GFX_OP_COLOR_FACTOR      ((0x3<<29)|(0x1d<<24)|(0x1<<16)|0x0)
+#define GFX_OP_STIPPLE           ((0x3<<29)|(0x1d<<24)|(0x83<<16))
+#define GFX_OP_MAP_INFO          ((0x3<<29)|(0x1d<<24)|0x4)
+#define GFX_OP_DESTBUFFER_VARS   ((0x3<<29)|(0x1d<<24)|(0x85<<16)|0x0)
+#define GFX_OP_DRAWRECT_INFO     ((0x3<<29)|(0x1d<<24)|(0x80<<16)|(0x3))
+
+#define GFX_OP_DRAWRECT_INFO_I965  ((0x7900<<16)|0x2)
+
+#define SRC_COPY_BLT_CMD                ((2<<29)|(0x43<<22)|4)
+#define XY_SRC_COPY_BLT_CMD		((2<<29)|(0x53<<22)|6)
+#define XY_MONO_SRC_COPY_IMM_BLT	((2<<29)|(0x71<<22)|5)
+#define XY_SRC_COPY_BLT_WRITE_ALPHA	(1<<21)
+#define XY_SRC_COPY_BLT_WRITE_RGB	(1<<20)
+#define   BLT_DEPTH_8			(0<<24)
+#define   BLT_DEPTH_16_565		(1<<24)
+#define   BLT_DEPTH_16_1555		(2<<24)
+#define   BLT_DEPTH_32			(3<<24)
+#define   BLT_ROP_GXCOPY		(0xcc<<16)
+
+#define MI_BATCH_BUFFER		((0x30<<23)|1)
+#define MI_BATCH_BUFFER_START	(0x31<<23)
+#define MI_BATCH_BUFFER_END	(0xA<<23)
+#define MI_BATCH_NON_SECURE	(1)
+
+#define MI_BATCH_NON_SECURE_I965 (1<<8)
+
+#define MI_WAIT_FOR_EVENT       ((0x3<<23))
+#define MI_WAIT_FOR_PLANE_B_FLIP      (1<<6)
+#define MI_WAIT_FOR_PLANE_A_FLIP      (1<<2)
+#define MI_WAIT_FOR_PLANE_A_SCANLINES (1<<1)
+
+#define MI_LOAD_SCAN_LINES_INCL  ((0x12<<23))
+
+#define CMD_OP_DISPLAYBUFFER_INFO ((0x0<<29)|(0x14<<23)|2)
+#define ASYNC_FLIP                (1<<22)
+#define DISPLAY_PLANE_A           (0<<20)
+#define DISPLAY_PLANE_B           (1<<20)
+
+/* Display regs */
+#define DSPACNTR                0x70180
+#define DSPBCNTR                0x71180
+#define DISPPLANE_SEL_PIPE_MASK                 (1<<24)
+
+/* Define the region of interest for the binner:
+ */
+#define CMD_OP_BIN_CONTROL	 ((0x3<<29)|(0x1d<<24)|(0x84<<16)|4)
+
+#define CMD_OP_DESTBUFFER_INFO	 ((0x3<<29)|(0x1d<<24)|(0x8e<<16)|1)
+
+#define BREADCRUMB_BITS 31
+#define BREADCRUMB_MASK ((1U << BREADCRUMB_BITS) - 1)
+
+#define READ_BREADCRUMB(dev_priv)  (((volatile u32*)(dev_priv->hw_status_page))[5])
+#define READ_HWSP(dev_priv, reg)  (((volatile u32*)(dev_priv->hw_status_page))[reg])
+
+#define PRIMARY_RINGBUFFER_SIZE		(128*1024)
+
+#define BLC_PWM_CTL2		0x61250
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/i915_reg.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/i915_reg.h	2009-02-05 18:29:58.000000000 +0000
@@ -0,0 +1,98 @@
+/* Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
+ * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
+ * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "../i915/i915_reg.h"
+
+#define I915_GCFGC			0xf0
+#define I915_LOW_FREQUENCY_ENABLE		(1 << 7)
+#define I915_DISPLAY_CLOCK_190_200_MHZ		(0 << 4)
+#define I915_DISPLAY_CLOCK_333_MHZ		(4 << 4)
+#define I915_DISPLAY_CLOCK_MASK			(7 << 4)
+
+#define I855_HPLLCC			0xc0
+#define I855_CLOCK_CONTROL_MASK			(3 << 0)
+#define I855_CLOCK_133_200			(0 << 0)
+#define I855_CLOCK_100_200			(1 << 0)
+#define I855_CLOCK_100_133			(2 << 0)
+#define I855_CLOCK_166_250			(3 << 0)
+
+#define LVDSPP_ON       0x61208
+#define LVDSPP_OFF      0x6120c
+#define PP_CYCLE        0x61210
+
+
+
+#define IS_I830(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82830_CGC)
+#define IS_845G(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82845G_IG)
+#define IS_I85X(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82855GM_IG)
+#define IS_I855(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82855GM_IG)
+#define IS_I865G(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82865_IG)
+
+#define IS_I915G(dev) (dev->pci_device == PCI_DEVICE_ID_INTEL_82915G_IG)/* || dev->pci_device == PCI_DEVICE_ID_INTELPCI_CHIP_E7221_G)*/
+#define IS_I915GM(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82915GM_IG)
+#define IS_I945G(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82945G_IG)
+#define IS_I945GM(dev) ((dev)->pci_device == PCI_DEVICE_ID_INTEL_82945GM_IG)
+
+#define IS_I965G(dev) ((dev)->pci_device == 0x2972 || \
+		       (dev)->pci_device == 0x2982 || \
+		       (dev)->pci_device == 0x2992 || \
+		       (dev)->pci_device == 0x29A2 || \
+		       (dev)->pci_device == 0x2A02 || \
+		       (dev)->pci_device == 0x2A12)
+
+#define IS_I965GM(dev) ((dev)->pci_device == 0x2A02)
+
+#define IS_G33(dev)    ((dev)->pci_device == 0x29C2 ||	\
+		   	(dev)->pci_device == 0x29B2 ||	\
+			(dev)->pci_device == 0x29D2)
+
+#define IS_I9XX(dev) (IS_I915G(dev) || IS_I915GM(dev) || IS_I945G(dev) || \
+		      IS_I945GM(dev) || IS_I965G(dev) || IS_POULSBO(dev))
+
+#define IS_MOBILE(dev) (IS_I830(dev) || IS_I85X(dev) || IS_I915GM(dev) || \
+			IS_I945GM(dev) || IS_I965GM(dev) || IS_POULSBO(dev))
+
+#define IS_POULSBO(dev) (((dev)->pci_device == 0x8108) || \
+			 ((dev)->pci_device == 0x8109))
+
+#define FPA0		0x06040
+#define FPA1		0x06044
+#define FPB0		0x06048
+#define FPB1		0x0604c
+#define FP_N_DIV_MASK				0x003f0000
+#define FP_N_DIV_SHIFT				16
+#define FP_M1_DIV_MASK				0x00003f00
+#define FP_M1_DIV_SHIFT			8
+#define FP_M2_DIV_MASK				0x0000003f
+#define FP_M2_DIV_SHIFT			0
+
+#define DSPABASE		0x70184
+#define DSPBBASE       		0x71184
+#define DSPAKEYVAL		0x70194
+#define DSPAKEYMASK		0x70198
+
+#define VSYNCSHIFT_A	0x60028
+#define VSYNCSHIFT_B	0x61028
+#define DPLL_B_MD		0x06020
+
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_crt.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_crt.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,242 @@
+/*
+ * Copyright Â© 2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+
+#include <linux/i2c.h>
+
+static void intel_crt_dpms(struct drm_output *output, int mode)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	u32 temp;
+
+	temp = I915_READ(ADPA);
+	temp &= ~(ADPA_HSYNC_CNTL_DISABLE | ADPA_VSYNC_CNTL_DISABLE);
+	temp &= ~ADPA_DAC_ENABLE;
+
+	switch(mode) {
+	case DPMSModeOn:
+		temp |= ADPA_DAC_ENABLE;
+		break;
+	case DPMSModeStandby:
+		temp |= ADPA_DAC_ENABLE | ADPA_HSYNC_CNTL_DISABLE;
+		break;
+	case DPMSModeSuspend:
+		temp |= ADPA_DAC_ENABLE | ADPA_VSYNC_CNTL_DISABLE;
+		break;
+	case DPMSModeOff:
+		temp |= ADPA_HSYNC_CNTL_DISABLE | ADPA_VSYNC_CNTL_DISABLE;
+		break;
+	}
+
+	I915_WRITE(ADPA, temp);
+}
+
+static void intel_crt_save(struct drm_output *output)
+{
+
+}
+
+static void intel_crt_restore(struct drm_output *output)
+{
+
+}
+
+static int intel_crt_mode_valid(struct drm_output *output,
+				struct drm_display_mode *mode)
+{
+	if (mode->flags & V_DBLSCAN)
+		return MODE_NO_DBLESCAN;
+
+	if (mode->clock > 400000 || mode->clock < 25000)
+		return MODE_CLOCK_RANGE;
+
+	return MODE_OK;
+}
+
+static bool intel_crt_mode_fixup(struct drm_output *output,
+				 struct drm_display_mode *mode,
+				 struct drm_display_mode *adjusted_mode)
+{
+	return true;
+}
+
+static void intel_crt_mode_set(struct drm_output *output,
+			       struct drm_display_mode *mode,
+			       struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	struct drm_crtc *crtc = output->crtc;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	int dpll_md_reg;
+	u32 adpa, dpll_md;
+
+	if (intel_crtc->pipe == 0)
+		dpll_md_reg = DPLL_A_MD;
+	else
+		dpll_md_reg = DPLL_B_MD;
+
+	/*
+	 * Disable separate mode multiplier used when cloning SDVO to CRT
+	 * XXX this needs to be adjusted when we really are cloning
+	 */
+	if (IS_I965G(dev)) {
+		dpll_md = I915_READ(dpll_md_reg);
+		I915_WRITE(dpll_md_reg,
+			   dpll_md & ~DPLL_MD_UDI_MULTIPLIER_MASK);
+	}
+
+	adpa = 0;
+	if (adjusted_mode->flags & V_PHSYNC)
+		adpa |= ADPA_HSYNC_ACTIVE_HIGH;
+	if (adjusted_mode->flags & V_PVSYNC)
+		adpa |= ADPA_VSYNC_ACTIVE_HIGH;
+
+	if (intel_crtc->pipe == 0)
+		adpa |= ADPA_PIPE_A_SELECT;
+	else
+		adpa |= ADPA_PIPE_B_SELECT;
+
+	I915_WRITE(ADPA, adpa);
+}
+
+/**
+ * Uses CRT_HOTPLUG_EN and CRT_HOTPLUG_STAT to detect CRT presence.
+ *
+ * Only for I945G/GM.
+ *
+ * \return TRUE if CRT is connected.
+ * \return FALSE if CRT is disconnected.
+ */
+static bool intel_crt_detect_hotplug(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	u32 temp;
+	unsigned long timeout = jiffies + msecs_to_jiffies(1000);
+
+	temp = I915_READ(PORT_HOTPLUG_EN);
+
+	I915_WRITE(PORT_HOTPLUG_EN,
+		   temp | CRT_HOTPLUG_FORCE_DETECT | (1 << 5));
+
+	do {
+		if (!(I915_READ(PORT_HOTPLUG_EN) & CRT_HOTPLUG_FORCE_DETECT))
+			break;
+		msleep(1);
+	} while (time_after(timeout, jiffies));
+
+	if ((I915_READ(PORT_HOTPLUG_STAT) & CRT_HOTPLUG_MONITOR_MASK) ==
+	    CRT_HOTPLUG_MONITOR_COLOR)
+		return true;
+
+	return false;
+}
+
+static bool intel_crt_detect_ddc(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+
+	/* CRT should always be at 0, but check anyway */
+	if (intel_output->type != INTEL_OUTPUT_ANALOG)
+		return false;
+
+	return intel_ddc_probe(output);
+}
+
+static enum drm_output_status intel_crt_detect(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+
+	if (IS_I945G(dev) || IS_I945GM(dev) || IS_I965G(dev)) {
+		if (intel_crt_detect_hotplug(output))
+			return output_status_connected;
+		else
+			return output_status_disconnected;
+	}
+
+	if (intel_crt_detect_ddc(output))
+		return output_status_connected;
+
+	/* TODO use load detect */
+	return output_status_unknown;
+}
+
+static void intel_crt_destroy(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+
+	intel_i2c_destroy(intel_output->ddc_bus);
+	kfree(output->driver_private);
+}
+
+static int intel_crt_get_modes(struct drm_output *output)
+{
+	return intel_ddc_get_modes(output);
+}
+
+/*
+ * Routines for controlling stuff on the analog port
+ */
+static const struct drm_output_funcs intel_crt_output_funcs = {
+	.dpms = intel_crt_dpms,
+	.save = intel_crt_save,
+	.restore = intel_crt_restore,
+	.mode_valid = intel_crt_mode_valid,
+	.mode_fixup = intel_crt_mode_fixup,
+	.prepare = intel_output_prepare,
+	.mode_set = intel_crt_mode_set,
+	.commit = intel_output_commit,
+	.detect = intel_crt_detect,
+	.get_modes = intel_crt_get_modes,
+	.cleanup = intel_crt_destroy,
+};
+
+void intel_crt_init(struct drm_device *dev)
+{
+	struct drm_output *output;
+	struct intel_output *intel_output;
+
+	output = drm_output_create(dev, &intel_crt_output_funcs, "VGA");
+
+	intel_output = kmalloc(sizeof(struct intel_output), GFP_KERNEL);
+	if (!intel_output) {
+		drm_output_destroy(output);
+		return;
+	}
+	/* Set up the DDC bus. */
+	intel_output->ddc_bus = intel_i2c_create(dev, GPIOA, "CRTDDC_A");
+	if (!intel_output->ddc_bus) {
+		dev_printk(KERN_ERR, &dev->pdev->dev, "DDC bus registration "
+			   "failed.\n");
+		return;
+	}
+
+	intel_output->type = INTEL_OUTPUT_ANALOG;
+	output->driver_private = intel_output;
+	output->interlace_allowed = 0;
+	output->doublescan_allowed = 0;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_display.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_display.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,1472 @@
+/*
+ * Copyright Â© 2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+
+#include <linux/i2c.h>
+
+bool intel_pipe_has_type (struct drm_crtc *crtc, int type);
+
+typedef struct {
+    /* given values */
+    int n;
+    int m1, m2;
+    int p1, p2;
+    /* derived values */
+    int	dot;
+    int	vco;
+    int	m;
+    int	p;
+} intel_clock_t;
+
+typedef struct {
+    int	min, max;
+} intel_range_t;
+
+typedef struct {
+    int	dot_limit;
+    int	p2_slow, p2_fast;
+} intel_p2_t;
+
+#define INTEL_P2_NUM		      2
+
+typedef struct {
+    intel_range_t   dot, vco, n, m, m1, m2, p, p1;
+    intel_p2_t	    p2;
+} intel_limit_t;
+
+#define I8XX_DOT_MIN		  25000
+#define I8XX_DOT_MAX		 350000
+#define I8XX_VCO_MIN		 930000
+#define I8XX_VCO_MAX		1400000
+#define I8XX_N_MIN		      3
+#define I8XX_N_MAX		     16
+#define I8XX_M_MIN		     96
+#define I8XX_M_MAX		    140
+#define I8XX_M1_MIN		     18
+#define I8XX_M1_MAX		     26
+#define I8XX_M2_MIN		      6
+#define I8XX_M2_MAX		     16
+#define I8XX_P_MIN		      4
+#define I8XX_P_MAX		    128
+#define I8XX_P1_MIN		      2
+#define I8XX_P1_MAX		     33
+#define I8XX_P1_LVDS_MIN	      1
+#define I8XX_P1_LVDS_MAX	      6
+#define I8XX_P2_SLOW		      4
+#define I8XX_P2_FAST		      2
+#define I8XX_P2_LVDS_SLOW	      14
+#define I8XX_P2_LVDS_FAST	      14 /* No fast option */
+#define I8XX_P2_SLOW_LIMIT	 165000
+
+#define I9XX_DOT_MIN		  20000
+#define I9XX_DOT_MAX		 400000
+#define I9XX_VCO_MIN		1400000
+#define I9XX_VCO_MAX		2800000
+#define I9XX_N_MIN		      3
+#define I9XX_N_MAX		      8
+#define I9XX_M_MIN		     70
+#define I9XX_M_MAX		    120
+#define I9XX_M1_MIN		     10
+#define I9XX_M1_MAX		     20
+#define I9XX_M2_MIN		      5
+#define I9XX_M2_MAX		      9
+#define I9XX_P_SDVO_DAC_MIN	      5
+#define I9XX_P_SDVO_DAC_MAX	     80
+#define I9XX_P_LVDS_MIN		      7
+#define I9XX_P_LVDS_MAX		     98
+#define I9XX_P1_MIN		      1
+#define I9XX_P1_MAX		      8
+#define I9XX_P2_SDVO_DAC_SLOW		     10
+#define I9XX_P2_SDVO_DAC_FAST		      5
+#define I9XX_P2_SDVO_DAC_SLOW_LIMIT	 200000
+#define I9XX_P2_LVDS_SLOW		     14
+#define I9XX_P2_LVDS_FAST		      7
+#define I9XX_P2_LVDS_SLOW_LIMIT		 112000
+
+#define INTEL_LIMIT_I8XX_DVO_DAC    0
+#define INTEL_LIMIT_I8XX_LVDS	    1
+#define INTEL_LIMIT_I9XX_SDVO_DAC   2
+#define INTEL_LIMIT_I9XX_LVDS	    3
+
+static const intel_limit_t intel_limits[] = {
+    { /* INTEL_LIMIT_I8XX_DVO_DAC */
+        .dot = { .min = I8XX_DOT_MIN,		.max = I8XX_DOT_MAX },
+        .vco = { .min = I8XX_VCO_MIN,		.max = I8XX_VCO_MAX },
+        .n   = { .min = I8XX_N_MIN,		.max = I8XX_N_MAX },
+        .m   = { .min = I8XX_M_MIN,		.max = I8XX_M_MAX },
+        .m1  = { .min = I8XX_M1_MIN,		.max = I8XX_M1_MAX },
+        .m2  = { .min = I8XX_M2_MIN,		.max = I8XX_M2_MAX },
+        .p   = { .min = I8XX_P_MIN,		.max = I8XX_P_MAX },
+        .p1  = { .min = I8XX_P1_MIN,		.max = I8XX_P1_MAX },
+	.p2  = { .dot_limit = I8XX_P2_SLOW_LIMIT,
+		 .p2_slow = I8XX_P2_SLOW,	.p2_fast = I8XX_P2_FAST },
+    },
+    { /* INTEL_LIMIT_I8XX_LVDS */
+        .dot = { .min = I8XX_DOT_MIN,		.max = I8XX_DOT_MAX },
+        .vco = { .min = I8XX_VCO_MIN,		.max = I8XX_VCO_MAX },
+        .n   = { .min = I8XX_N_MIN,		.max = I8XX_N_MAX },
+        .m   = { .min = I8XX_M_MIN,		.max = I8XX_M_MAX },
+        .m1  = { .min = I8XX_M1_MIN,		.max = I8XX_M1_MAX },
+        .m2  = { .min = I8XX_M2_MIN,		.max = I8XX_M2_MAX },
+        .p   = { .min = I8XX_P_MIN,		.max = I8XX_P_MAX },
+        .p1  = { .min = I8XX_P1_LVDS_MIN,	.max = I8XX_P1_LVDS_MAX },
+	.p2  = { .dot_limit = I8XX_P2_SLOW_LIMIT,
+		 .p2_slow = I8XX_P2_LVDS_SLOW,	.p2_fast = I8XX_P2_LVDS_FAST },
+    },
+    { /* INTEL_LIMIT_I9XX_SDVO_DAC */
+        .dot = { .min = I9XX_DOT_MIN,		.max = I9XX_DOT_MAX },
+        .vco = { .min = I9XX_VCO_MIN,		.max = I9XX_VCO_MAX },
+        .n   = { .min = I9XX_N_MIN,		.max = I9XX_N_MAX },
+        .m   = { .min = I9XX_M_MIN,		.max = I9XX_M_MAX },
+        .m1  = { .min = I9XX_M1_MIN,		.max = I9XX_M1_MAX },
+        .m2  = { .min = I9XX_M2_MIN,		.max = I9XX_M2_MAX },
+        .p   = { .min = I9XX_P_SDVO_DAC_MIN,	.max = I9XX_P_SDVO_DAC_MAX },
+        .p1  = { .min = I9XX_P1_MIN,		.max = I9XX_P1_MAX },
+	.p2  = { .dot_limit = I9XX_P2_SDVO_DAC_SLOW_LIMIT,
+		 .p2_slow = I9XX_P2_SDVO_DAC_SLOW,	.p2_fast = I9XX_P2_SDVO_DAC_FAST },
+    },
+    { /* INTEL_LIMIT_I9XX_LVDS */
+        .dot = { .min = I9XX_DOT_MIN,		.max = I9XX_DOT_MAX },
+        .vco = { .min = I9XX_VCO_MIN,		.max = I9XX_VCO_MAX },
+        .n   = { .min = I9XX_N_MIN,		.max = I9XX_N_MAX },
+        .m   = { .min = I9XX_M_MIN,		.max = I9XX_M_MAX },
+        .m1  = { .min = I9XX_M1_MIN,		.max = I9XX_M1_MAX },
+        .m2  = { .min = I9XX_M2_MIN,		.max = I9XX_M2_MAX },
+        .p   = { .min = I9XX_P_LVDS_MIN,	.max = I9XX_P_LVDS_MAX },
+        .p1  = { .min = I9XX_P1_MIN,		.max = I9XX_P1_MAX },
+	/* The single-channel range is 25-112Mhz, and dual-channel
+	 * is 80-224Mhz.  Prefer single channel as much as possible.
+	 */
+	.p2  = { .dot_limit = I9XX_P2_LVDS_SLOW_LIMIT,
+		 .p2_slow = I9XX_P2_LVDS_SLOW,	.p2_fast = I9XX_P2_LVDS_FAST },
+    },
+};
+
+static const intel_limit_t *intel_limit(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	const intel_limit_t *limit;
+
+	if (IS_I9XX(dev)) {
+		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
+			limit = &intel_limits[INTEL_LIMIT_I9XX_LVDS];
+		else
+			limit = &intel_limits[INTEL_LIMIT_I9XX_SDVO_DAC];
+	} else {
+		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
+			limit = &intel_limits[INTEL_LIMIT_I8XX_LVDS];
+		else
+			limit = &intel_limits[INTEL_LIMIT_I8XX_DVO_DAC];
+	}
+	return limit;
+}
+
+/** Derive the pixel clock for the given refclk and divisors for 8xx chips. */
+
+static void i8xx_clock(int refclk, intel_clock_t *clock)
+{
+	clock->m = 5 * (clock->m1 + 2) + (clock->m2 + 2);
+	clock->p = clock->p1 * clock->p2;
+	clock->vco = refclk * clock->m / (clock->n + 2);
+	clock->dot = clock->vco / clock->p;
+}
+
+/** Derive the pixel clock for the given refclk and divisors for 9xx chips. */
+
+static void i9xx_clock(int refclk, intel_clock_t *clock)
+{
+	clock->m = 5 * (clock->m1 + 2) + (clock->m2 + 2);
+	clock->p = clock->p1 * clock->p2;
+	clock->vco = refclk * clock->m / (clock->n + 2);
+	clock->dot = clock->vco / clock->p;
+}
+
+static void intel_clock(struct drm_device *dev, int refclk,
+			intel_clock_t *clock)
+{
+	if (IS_I9XX(dev))
+		return i9xx_clock (refclk, clock);
+	else
+		return i8xx_clock (refclk, clock);
+}
+
+/**
+ * Returns whether any output on the specified pipe is of the specified type
+ */
+bool intel_pipe_has_type (struct drm_crtc *crtc, int type)
+{
+    struct drm_device *dev = crtc->dev;
+    struct drm_mode_config *mode_config = &dev->mode_config;
+    struct drm_output *l_entry;
+
+    list_for_each_entry(l_entry, &mode_config->output_list, head) {
+	    if (l_entry->crtc == crtc) {
+		    struct intel_output *intel_output = l_entry->driver_private;
+		    if (intel_output->type == type)
+			    return true;
+	    }
+    }
+    return false;
+}
+
+#define INTELPllInvalid(s)   { /* ErrorF (s) */; return false; }
+/**
+ * Returns whether the given set of divisors are valid for a given refclk with
+ * the given outputs.
+ */
+
+static bool intel_PLL_is_valid(struct drm_crtc *crtc, intel_clock_t *clock)
+{
+	const intel_limit_t *limit = intel_limit (crtc);
+
+	if (clock->p1  < limit->p1.min  || limit->p1.max  < clock->p1)
+		INTELPllInvalid ("p1 out of range\n");
+	if (clock->p   < limit->p.min   || limit->p.max   < clock->p)
+		INTELPllInvalid ("p out of range\n");
+	if (clock->m2  < limit->m2.min  || limit->m2.max  < clock->m2)
+		INTELPllInvalid ("m2 out of range\n");
+	if (clock->m1  < limit->m1.min  || limit->m1.max  < clock->m1)
+		INTELPllInvalid ("m1 out of range\n");
+	if (clock->m1 <= clock->m2)
+		INTELPllInvalid ("m1 <= m2\n");
+	if (clock->m   < limit->m.min   || limit->m.max   < clock->m)
+		INTELPllInvalid ("m out of range\n");
+	if (clock->n   < limit->n.min   || limit->n.max   < clock->n)
+		INTELPllInvalid ("n out of range\n");
+	if (clock->vco < limit->vco.min || limit->vco.max < clock->vco)
+		INTELPllInvalid ("vco out of range\n");
+	/* XXX: We may need to be checking "Dot clock" depending on the multiplier,
+	 * output, etc., rather than just a single range.
+	 */
+	if (clock->dot < limit->dot.min || limit->dot.max < clock->dot)
+		INTELPllInvalid ("dot out of range\n");
+
+	return true;
+}
+
+/**
+ * Returns a set of divisors for the desired target clock with the given
+ * refclk, or FALSE.  The returned values represent the clock equation:
+ * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.
+ */
+static bool intel_find_best_PLL(struct drm_crtc *crtc, int target,
+				int refclk, intel_clock_t *best_clock)
+{
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	intel_clock_t clock;
+	const intel_limit_t *limit = intel_limit(crtc);
+	int err = target;
+
+	if (IS_I9XX(dev) && intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) &&
+	    (I915_READ(LVDS) & LVDS_PORT_EN) != 0) {
+		/*
+		 * For LVDS, if the panel is on, just rely on its current
+		 * settings for dual-channel.  We haven't figured out how to
+		 * reliably set up different single/dual channel state, if we
+		 * even can.
+		 */
+		if ((I915_READ(LVDS) & LVDS_CLKB_POWER_MASK) ==
+		    LVDS_CLKB_POWER_UP)
+			clock.p2 = limit->p2.p2_fast;
+		else
+			clock.p2 = limit->p2.p2_slow;
+	} else {
+		if (target < limit->p2.dot_limit)
+			clock.p2 = limit->p2.p2_slow;
+		else
+			clock.p2 = limit->p2.p2_fast;
+	}
+
+	memset (best_clock, 0, sizeof (*best_clock));
+
+	for (clock.m1 = limit->m1.min; clock.m1 <= limit->m1.max; clock.m1++) {
+		for (clock.m2 = limit->m2.min; clock.m2 < clock.m1 &&
+			     clock.m2 <= limit->m2.max; clock.m2++) {
+			for (clock.n = limit->n.min; clock.n <= limit->n.max;
+			     clock.n++) {
+				for (clock.p1 = limit->p1.min;
+				     clock.p1 <= limit->p1.max; clock.p1++) {
+					int this_err;
+
+					intel_clock(dev, refclk, &clock);
+
+					if (!intel_PLL_is_valid(crtc, &clock))
+						continue;
+
+					this_err = abs(clock.dot - target);
+					if (this_err < err) {
+						*best_clock = clock;
+						err = this_err;
+					}
+				}
+			}
+		}
+	}
+
+	return (err != target);
+}
+
+#if 0
+void
+intel_set_vblank(struct drm_device *dev)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc;
+	struct intel_crtc *intel_crtc;
+	int vbl_pipe = 0;
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		intel_crtc = crtc->driver_private;
+
+		if (crtc->enabled)
+			vbl_pipe |= (1<<intel_crtc->pipe);
+	}
+
+	dev_priv->vblank_pipe = vbl_pipe;
+	i915_enable_interrupt(dev);
+}
+#endif
+
+void
+intel_wait_for_vblank(struct drm_device *dev)
+{
+	/* Wait for 20ms, i.e. one cycle at 50hz. */
+	udelay(20000);
+}
+
+void
+intel_pipe_set_base(struct drm_crtc *crtc, int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	unsigned long Start, Offset;
+	int dspbase = (pipe == 0 ? DSPABASE : DSPBBASE);
+	int dspsurf = (pipe == 0 ? DSPASURF : DSPBSURF);
+
+	Start = crtc->fb->offset;
+	Offset = y * crtc->fb->pitch + x;
+
+	DRM_DEBUG("Writing base %08lX %08lX %d %d\n", Start, Offset, x, y);
+	if (IS_I965G(dev)) {
+		I915_WRITE(dspbase, Offset);
+		I915_READ(dspbase);
+		I915_WRITE(dspsurf, Start);
+		I915_READ(dspsurf);
+	} else {
+		I915_WRITE(dspbase, Start + Offset);
+		I915_READ(dspbase);
+	}
+
+
+	if (!dev_priv->sarea_priv)
+		return;
+
+	switch (pipe) {
+	case 0:
+		dev_priv->sarea_priv->planeA_x = x;
+		dev_priv->sarea_priv->planeA_y = y;
+		break;
+	case 1:
+		dev_priv->sarea_priv->planeB_x = x;
+		dev_priv->sarea_priv->planeB_y = y;
+		break;
+	default:
+		DRM_ERROR("Can't update pipe %d in SAREA\n", pipe);
+		break;
+	}
+}
+
+/**
+ * Sets the power management mode of the pipe and plane.
+ *
+ * This code should probably grow support for turning the cursor off and back
+ * on appropriately at the same time as we're turning the pipe off/on.
+ */
+static void intel_crtc_dpms(struct drm_crtc *crtc, int mode)
+{
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	int dpll_reg = (pipe == 0) ? DPLL_A : DPLL_B;
+	int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+	int dspbase_reg = (pipe == 0) ? DSPABASE : DSPBBASE;
+	int pipeconf_reg = (pipe == 0) ? PIPEACONF : PIPEBCONF;
+	u32 temp, temp2;
+	bool enabled;
+
+	/* XXX: When our outputs are all unaware of DPMS modes other than off
+	 * and on, we should map those modes to DPMSModeOff in the CRTC.
+	 */
+	switch (mode) {
+	case DPMSModeOn:
+	case DPMSModeStandby:
+	case DPMSModeSuspend:
+		/* Enable the DPLL */
+		temp = I915_READ(dpll_reg);
+		if ((temp & DPLL_VCO_ENABLE) == 0) {
+			I915_WRITE(dpll_reg, temp);
+			I915_READ(dpll_reg);
+			/* Wait for the clocks to stabilize. */
+			udelay(150);
+			I915_WRITE(dpll_reg, temp | DPLL_VCO_ENABLE);
+			I915_READ(dpll_reg);
+			/* Wait for the clocks to stabilize. */
+			udelay(150);
+			I915_WRITE(dpll_reg, temp | DPLL_VCO_ENABLE);
+			I915_READ(dpll_reg);
+			/* Wait for the clocks to stabilize. */
+			udelay(150);
+		}
+
+		/* Enable the pipe */
+		temp = I915_READ(pipeconf_reg);
+		if ((temp & PIPEACONF_ENABLE) == 0)
+			I915_WRITE(pipeconf_reg, temp | PIPEACONF_ENABLE);
+
+		/* Enable the plane */
+		temp = I915_READ(dspcntr_reg);
+		if (mode != DPMSModeOn)
+			temp2 = temp & ~DISPLAY_PLANE_ENABLE;
+		else
+			temp2 = temp | DISPLAY_PLANE_ENABLE;
+
+		if (temp != temp2) {
+			I915_WRITE(dspcntr_reg, temp2);
+			/* Flush the plane changes */
+			I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+		}
+
+		intel_crtc_load_lut(crtc);
+
+		/* Give the overlay scaler a chance to enable if it's on this pipe */
+		//intel_crtc_dpms_video(crtc, TRUE); TODO
+	break;
+	case DPMSModeOff:
+		/* Give the overlay scaler a chance to disable if it's on this pipe */
+		//intel_crtc_dpms_video(crtc, FALSE); TODO
+
+		/* Disable display plane */
+		temp = I915_READ(dspcntr_reg);
+		if ((temp & DISPLAY_PLANE_ENABLE) != 0) {
+			I915_WRITE(dspcntr_reg, temp & ~DISPLAY_PLANE_ENABLE);
+			/* Flush the plane changes */
+			I915_WRITE(dspbase_reg, I915_READ(dspbase_reg));
+			I915_READ(dspbase_reg);
+		}
+
+		if (!IS_I9XX(dev)) {
+			/* Wait for vblank for the disable to take effect */
+			intel_wait_for_vblank(dev);
+		}
+
+		/* Next, disable display pipes */
+		temp = I915_READ(pipeconf_reg);
+		if ((temp & PIPEACONF_ENABLE) != 0) {
+			I915_WRITE(pipeconf_reg, temp & ~PIPEACONF_ENABLE);
+			I915_READ(pipeconf_reg);
+		}
+
+		/* Wait for vblank for the disable to take effect. */
+		intel_wait_for_vblank(dev);
+
+		temp = I915_READ(dpll_reg);
+		if ((temp & DPLL_VCO_ENABLE) != 0) {
+			I915_WRITE(dpll_reg, temp & ~DPLL_VCO_ENABLE);
+			I915_READ(dpll_reg);
+		}
+
+		/* Wait for the clocks to turn off. */
+		udelay(150);
+		break;
+	}
+
+
+	if (!dev_priv->sarea_priv)
+		return;
+
+	enabled = crtc->enabled && mode != DPMSModeOff;
+
+	switch (pipe) {
+	case 0:
+		dev_priv->sarea_priv->planeA_w = enabled ? crtc->mode.hdisplay : 0;
+		dev_priv->sarea_priv->planeA_h = enabled ? crtc->mode.vdisplay : 0;
+		break;
+	case 1:
+		dev_priv->sarea_priv->planeB_w = enabled ? crtc->mode.hdisplay : 0;
+		dev_priv->sarea_priv->planeB_h = enabled ? crtc->mode.vdisplay : 0;
+		break;
+	default:
+		DRM_ERROR("Can't update pipe %d in SAREA\n", pipe);
+		break;
+	}
+}
+
+static bool intel_crtc_lock(struct drm_crtc *crtc)
+{
+   /* Sync the engine before mode switch */
+//   i830WaitSync(crtc->scrn);
+
+#if 0 // TODO def XF86DRI
+    return I830DRILock(crtc->scrn);
+#else
+    return FALSE;
+#endif
+}
+
+static void intel_crtc_unlock (struct drm_crtc *crtc)
+{
+#if 0 // TODO def XF86DRI
+    I830DRIUnlock (crtc->scrn);
+#endif
+}
+
+static void intel_crtc_prepare (struct drm_crtc *crtc)
+{
+	crtc->funcs->dpms(crtc, DPMSModeOff);
+}
+
+static void intel_crtc_commit (struct drm_crtc *crtc)
+{
+	crtc->funcs->dpms(crtc, DPMSModeOn);
+}
+
+void intel_output_prepare (struct drm_output *output)
+{
+	/* lvds has its own version of prepare see intel_lvds_prepare */
+	output->funcs->dpms(output, DPMSModeOff);
+}
+
+void intel_output_commit (struct drm_output *output)
+{
+	/* lvds has its own version of commit see intel_lvds_commit */
+	output->funcs->dpms(output, DPMSModeOn);
+}
+
+static bool intel_crtc_mode_fixup(struct drm_crtc *crtc,
+				  struct drm_display_mode *mode,
+				  struct drm_display_mode *adjusted_mode)
+{
+	return true;
+}
+
+
+/** Returns the core display clock speed for i830 - i945 */
+int intel_get_core_clock_speed(struct drm_device *dev)
+{
+
+	/* Core clock values taken from the published datasheets.
+	 * The 830 may go up to 166 Mhz, which we should check.
+	 */
+	if (IS_I945G(dev))
+		return 400000;
+	else if (IS_I915G(dev))
+		return 333000;
+	else if (IS_I945GM(dev) || IS_POULSBO(dev) || IS_845G(dev))
+		return 200000;
+	else if (IS_I915GM(dev)) {
+		u16 gcfgc = 0;
+
+		pci_read_config_word(dev->pdev, I915_GCFGC, &gcfgc);
+
+		if (gcfgc & I915_LOW_FREQUENCY_ENABLE)
+			return 133000;
+		else {
+			switch (gcfgc & I915_DISPLAY_CLOCK_MASK) {
+			case I915_DISPLAY_CLOCK_333_MHZ:
+				return 333000;
+			default:
+			case I915_DISPLAY_CLOCK_190_200_MHZ:
+				return 190000;
+			}
+		}
+	} else if (IS_I865G(dev))
+		return 266000;
+	else if (IS_I855(dev)) {
+#if 0
+		PCITAG bridge = pciTag(0, 0, 0); /* This is always the host bridge */
+		u16 hpllcc = pciReadWord(bridge, I855_HPLLCC);
+
+#endif
+		u16 hpllcc = 0;
+		/* Assume that the hardware is in the high speed state.  This
+		 * should be the default.
+		 */
+		switch (hpllcc & I855_CLOCK_CONTROL_MASK) {
+		case I855_CLOCK_133_200:
+		case I855_CLOCK_100_200:
+			return 200000;
+		case I855_CLOCK_166_250:
+			return 250000;
+		case I855_CLOCK_100_133:
+			return 133000;
+		}
+	} else /* 852, 830 */
+		return 133000;
+
+	return 0; /* Silence gcc warning */
+}
+
+
+/**
+ * Return the pipe currently connected to the panel fitter,
+ * or -1 if the panel fitter is not present or not in use
+ */
+int intel_panel_fitter_pipe (struct drm_device *dev)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	u32  pfit_control;
+
+	/* i830 doesn't have a panel fitter */
+	if (IS_I830(dev))
+		return -1;
+
+	pfit_control = I915_READ(PFIT_CONTROL);
+
+	/* See if the panel fitter is in use */
+	if ((pfit_control & PFIT_ENABLE) == 0)
+		return -1;
+
+	/* 965 can place panel fitter on either pipe */
+	if (IS_I965G(dev))
+		return (pfit_control >> 29) & 0x3;
+
+	/* older chips can only use pipe 1 */
+	return 1;
+}
+
+#define WA_NO_FB_GARBAGE_DISPLAY
+#ifdef WA_NO_FB_GARBAGE_DISPLAY
+static u32 fp_reg_value[2];
+static u32 dpll_reg_value[2];
+static u32 dpll_md_reg_value[2];
+static u32 dspcntr_reg_value[2];
+static u32 pipeconf_reg_value[2];
+static u32 htot_reg_value[2];
+static u32 hblank_reg_value[2];
+static u32 hsync_reg_value[2];
+static u32 vtot_reg_value[2];
+static u32 vblank_reg_value[2];
+static u32 vsync_reg_value[2];
+static u32 dspsize_reg_value[2];
+static u32 dspstride_reg_value[2];
+static u32 dsppos_reg_value[2];
+static u32 pipesrc_reg_value[2];
+
+static u32 dspbase_value[2];
+
+static u32 lvds_reg_value[2];
+static u32 vgacntrl_reg_value[2];
+static u32 pfit_control_reg_value[2];
+
+void intel_crtc_mode_restore(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	int fp_reg = (pipe == 0) ? FPA0 : FPB0;
+	int dpll_reg = (pipe == 0) ? DPLL_A : DPLL_B;
+	int dpll_md_reg = (intel_crtc->pipe == 0) ? DPLL_A_MD : DPLL_B_MD;
+	int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+	int pipeconf_reg = (pipe == 0) ? PIPEACONF : PIPEBCONF;
+	int htot_reg = (pipe == 0) ? HTOTAL_A : HTOTAL_B;
+	int hblank_reg = (pipe == 0) ? HBLANK_A : HBLANK_B;
+	int hsync_reg = (pipe == 0) ? HSYNC_A : HSYNC_B;
+	int vtot_reg = (pipe == 0) ? VTOTAL_A : VTOTAL_B;
+	int vblank_reg = (pipe == 0) ? VBLANK_A : VBLANK_B;
+	int vsync_reg = (pipe == 0) ? VSYNC_A : VSYNC_B;
+	int dspsize_reg = (pipe == 0) ? DSPASIZE : DSPBSIZE;
+	int dspstride_reg = (pipe == 0) ? DSPASTRIDE : DSPBSTRIDE;
+	int dsppos_reg = (pipe == 0) ? DSPAPOS : DSPBPOS;
+	int pipesrc_reg = (pipe == 0) ? PIPEASRC : PIPEBSRC;
+	int dspbase = (pipe == 0 ? DSPABASE : DSPBBASE);
+
+	bool ok, is_sdvo = false, is_dvo = false;
+	bool is_crt = false, is_lvds = false, is_tv = false;
+	struct drm_mode_config *mode_config = &dev->mode_config;
+	struct drm_output *output;
+
+	list_for_each_entry(output, &mode_config->output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+
+		if (output->crtc != crtc)
+			continue;
+
+		switch (intel_output->type) {
+		case INTEL_OUTPUT_LVDS:
+			is_lvds = TRUE;
+			break;
+		case INTEL_OUTPUT_SDVO:
+			is_sdvo = TRUE;
+			break;
+		case INTEL_OUTPUT_DVO:
+			is_dvo = TRUE;
+			break;
+		case INTEL_OUTPUT_TVOUT:
+			is_tv = TRUE;
+			break;
+		case INTEL_OUTPUT_ANALOG:
+			is_crt = TRUE;
+			break;
+		}
+		if(is_lvds && ((lvds_reg_value[pipe] & LVDS_PORT_EN) == 0))
+		{
+			printk("%s: is_lvds but not the boot display, so return\n",
+							__FUNCTION__);
+			return;
+		}
+		output->funcs->prepare(output);
+	}
+
+	intel_crtc_prepare(crtc);
+	/* Disable the panel fitter if it was on our pipe */
+	if (intel_panel_fitter_pipe(dev) == pipe)
+		I915_WRITE(PFIT_CONTROL, 0);
+
+	if (dpll_reg_value[pipe] & DPLL_VCO_ENABLE) {
+		I915_WRITE(fp_reg, fp_reg_value[pipe]);
+		I915_WRITE(dpll_reg, dpll_reg_value[pipe]& ~DPLL_VCO_ENABLE);
+		I915_READ(dpll_reg);
+		udelay(150);
+	}
+
+	/*
+	if(is_lvds)
+		I915_WRITE(LVDS, lvds_reg_value[pipe]);
+	*/
+	if (is_lvds) {
+		I915_WRITE(LVDS, lvds_reg_value[pipe]);
+		I915_READ(LVDS);
+	}
+
+	I915_WRITE(fp_reg, fp_reg_value[pipe]);
+	I915_WRITE(dpll_reg, dpll_reg_value[pipe]);
+	I915_READ(dpll_reg);
+	udelay(150);
+	//I915_WRITE(dpll_md_reg, dpll_md_reg_value[pipe]);
+	I915_WRITE(dpll_reg, dpll_reg_value[pipe]);
+	I915_READ(dpll_reg);
+	udelay(150);
+	I915_WRITE(htot_reg, htot_reg_value[pipe]);
+	I915_WRITE(hblank_reg, hblank_reg_value[pipe]);
+	I915_WRITE(hsync_reg, hsync_reg_value[pipe]);
+	I915_WRITE(vtot_reg, vtot_reg_value[pipe]);
+	I915_WRITE(vblank_reg, vblank_reg_value[pipe]);
+	I915_WRITE(vsync_reg, vsync_reg_value[pipe]);
+	I915_WRITE(dspstride_reg, dspstride_reg_value[pipe]);
+	I915_WRITE(dspsize_reg, dspsize_reg_value[pipe]);
+	I915_WRITE(dsppos_reg, dsppos_reg_value[pipe]);
+	I915_WRITE(pipesrc_reg, pipesrc_reg_value[pipe]);
+	I915_WRITE(pipeconf_reg, pipeconf_reg_value[pipe]);
+	I915_READ(pipeconf_reg);
+	intel_wait_for_vblank(dev);
+	I915_WRITE(dspcntr_reg, dspcntr_reg_value[pipe]);
+	I915_WRITE(dspbase, dspbase_value[pipe]);
+	I915_READ(dspbase);
+	I915_WRITE(VGACNTRL, vgacntrl_reg_value[pipe]);
+	intel_wait_for_vblank(dev);
+	I915_WRITE(PFIT_CONTROL, pfit_control_reg_value[pipe]);
+
+	intel_crtc_commit(crtc);
+	list_for_each_entry(output, &mode_config->output_list, head) {
+		if (output->crtc != crtc)
+			continue;
+
+		output->funcs->commit(output);
+		//output->funcs->dpms(output, DPMSModeOff);
+		//printk("turn off the display first\n");
+	}
+	return;
+}
+
+void intel_crtc_mode_save(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	int fp_reg = (pipe == 0) ? FPA0 : FPB0;
+	int dpll_reg = (pipe == 0) ? DPLL_A : DPLL_B;
+	int dpll_md_reg = (intel_crtc->pipe == 0) ? DPLL_A_MD : DPLL_B_MD;
+	int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+	int pipeconf_reg = (pipe == 0) ? PIPEACONF : PIPEBCONF;
+	int htot_reg = (pipe == 0) ? HTOTAL_A : HTOTAL_B;
+	int hblank_reg = (pipe == 0) ? HBLANK_A : HBLANK_B;
+	int hsync_reg = (pipe == 0) ? HSYNC_A : HSYNC_B;
+	int vtot_reg = (pipe == 0) ? VTOTAL_A : VTOTAL_B;
+	int vblank_reg = (pipe == 0) ? VBLANK_A : VBLANK_B;
+	int vsync_reg = (pipe == 0) ? VSYNC_A : VSYNC_B;
+	int dspsize_reg = (pipe == 0) ? DSPASIZE : DSPBSIZE;
+	int dspstride_reg = (pipe == 0) ? DSPASTRIDE : DSPBSTRIDE;
+	int dsppos_reg = (pipe == 0) ? DSPAPOS : DSPBPOS;
+	int pipesrc_reg = (pipe == 0) ? PIPEASRC : PIPEBSRC;
+	int dspbase = (pipe == 0 ? DSPABASE : DSPBBASE);
+	bool ok, is_sdvo = false, is_dvo = false;
+	bool is_crt = false, is_lvds = false, is_tv = false;
+	struct drm_mode_config *mode_config = &dev->mode_config;
+	struct drm_output *output;
+
+	list_for_each_entry(output, &mode_config->output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+
+		if (output->crtc != crtc)
+			continue;
+
+		switch (intel_output->type) {
+		case INTEL_OUTPUT_LVDS:
+			is_lvds = TRUE;
+			break;
+		case INTEL_OUTPUT_SDVO:
+			is_sdvo = TRUE;
+			break;
+		case INTEL_OUTPUT_DVO:
+			is_dvo = TRUE;
+			break;
+		case INTEL_OUTPUT_TVOUT:
+			is_tv = TRUE;
+			break;
+		case INTEL_OUTPUT_ANALOG:
+			is_crt = TRUE;
+			break;
+		}
+	}
+
+	fp_reg_value[pipe] = I915_READ(fp_reg);
+	dpll_reg_value[pipe] = I915_READ(dpll_reg);
+	dpll_md_reg_value[pipe] = I915_READ(dpll_md_reg);
+	dspcntr_reg_value[pipe] = I915_READ(dspcntr_reg);
+	pipeconf_reg_value[pipe] = I915_READ(pipeconf_reg);
+	htot_reg_value[pipe] = I915_READ(htot_reg);
+	hblank_reg_value[pipe] = I915_READ(hblank_reg);
+	hsync_reg_value[pipe] = I915_READ(hsync_reg);
+	vtot_reg_value[pipe] = I915_READ(vtot_reg);
+	vblank_reg_value[pipe] = I915_READ(vblank_reg);
+	vsync_reg_value[pipe] = I915_READ(vsync_reg);
+	dspsize_reg_value[pipe] = I915_READ(dspsize_reg);
+	dspstride_reg_value[pipe] = I915_READ(dspstride_reg);
+	dsppos_reg_value[pipe] = I915_READ(dsppos_reg);
+	pipesrc_reg_value[pipe] = I915_READ(pipesrc_reg);
+	dspbase_value[pipe] = I915_READ(dspbase);
+	if(is_lvds)
+		lvds_reg_value[pipe] = I915_READ(LVDS);
+	vgacntrl_reg_value[pipe] = I915_READ(VGACNTRL);
+	pfit_control_reg_value[pipe] = I915_READ(PFIT_CONTROL);
+}
+#endif
+
+static void intel_crtc_mode_set(struct drm_crtc *crtc,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adjusted_mode,
+				int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	int fp_reg = (pipe == 0) ? FPA0 : FPB0;
+	int dpll_reg = (pipe == 0) ? DPLL_A : DPLL_B;
+	int dpll_md_reg = (intel_crtc->pipe == 0) ? DPLL_A_MD : DPLL_B_MD;
+	int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+	int pipeconf_reg = (pipe == 0) ? PIPEACONF : PIPEBCONF;
+	int htot_reg = (pipe == 0) ? HTOTAL_A : HTOTAL_B;
+	int hblank_reg = (pipe == 0) ? HBLANK_A : HBLANK_B;
+	int hsync_reg = (pipe == 0) ? HSYNC_A : HSYNC_B;
+	int vtot_reg = (pipe == 0) ? VTOTAL_A : VTOTAL_B;
+	int vblank_reg = (pipe == 0) ? VBLANK_A : VBLANK_B;
+	int vsync_reg = (pipe == 0) ? VSYNC_A : VSYNC_B;
+	int dspsize_reg = (pipe == 0) ? DSPASIZE : DSPBSIZE;
+	int dspstride_reg = (pipe == 0) ? DSPASTRIDE : DSPBSTRIDE;
+	int dsppos_reg = (pipe == 0) ? DSPAPOS : DSPBPOS;
+	int pipesrc_reg = (pipe == 0) ? PIPEASRC : PIPEBSRC;
+	int refclk;
+	intel_clock_t clock;
+	u32 dpll = 0, fp = 0, dspcntr, pipeconf;
+	bool ok, is_sdvo = false, is_dvo = false;
+	bool is_crt = false, is_lvds = false, is_tv = false;
+	struct drm_mode_config *mode_config = &dev->mode_config;
+	struct drm_output *output;
+
+	if (!crtc->fb) {
+		DRM_ERROR("Can't set mode without attached fb\n");
+		return;
+	}
+
+	list_for_each_entry(output, &mode_config->output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+
+		if (output->crtc != crtc)
+			continue;
+
+		switch (intel_output->type) {
+		case INTEL_OUTPUT_LVDS:
+			is_lvds = TRUE;
+			break;
+		case INTEL_OUTPUT_SDVO:
+			is_sdvo = TRUE;
+			break;
+		case INTEL_OUTPUT_DVO:
+			is_dvo = TRUE;
+			break;
+		case INTEL_OUTPUT_TVOUT:
+			is_tv = TRUE;
+			break;
+		case INTEL_OUTPUT_ANALOG:
+			is_crt = TRUE;
+			break;
+		}
+	}
+
+	if (IS_I9XX(dev)) {
+		refclk = 96000;
+	} else {
+		refclk = 48000;
+	}
+
+	ok = intel_find_best_PLL(crtc, adjusted_mode->clock, refclk, &clock);
+	if (!ok) {
+		DRM_ERROR("Couldn't find PLL settings for mode!\n");
+		return;
+	}
+
+	fp = clock.n << 16 | clock.m1 << 8 | clock.m2;
+
+	dpll = DPLL_VGA_MODE_DIS;
+	if (IS_I9XX(dev)) {
+		if (is_lvds) {
+			dpll |= DPLLB_MODE_LVDS;
+			if (IS_POULSBO(dev))
+				dpll |= DPLL_DVO_HIGH_SPEED;
+		} else
+			dpll |= DPLLB_MODE_DAC_SERIAL;
+		if (is_sdvo) {
+			dpll |= DPLL_DVO_HIGH_SPEED;
+			if (IS_I945G(dev) || IS_I945GM(dev) || IS_POULSBO(dev)) {
+				int sdvo_pixel_multiply = adjusted_mode->clock / mode->clock;
+				dpll |= (sdvo_pixel_multiply - 1) << SDVO_MULTIPLIER_SHIFT_HIRES;
+			}
+		}
+
+		/* compute bitmask from p1 value */
+		dpll |= (1 << (clock.p1 - 1)) << 16;
+		switch (clock.p2) {
+		case 5:
+			dpll |= DPLL_DAC_SERIAL_P2_CLOCK_DIV_5;
+			break;
+		case 7:
+			dpll |= DPLLB_LVDS_P2_CLOCK_DIV_7;
+			break;
+		case 10:
+			dpll |= DPLL_DAC_SERIAL_P2_CLOCK_DIV_10;
+			break;
+		case 14:
+			dpll |= DPLLB_LVDS_P2_CLOCK_DIV_14;
+			break;
+		}
+		if (IS_I965G(dev))
+			dpll |= (6 << PLL_LOAD_PULSE_PHASE_SHIFT);
+	} else {
+		if (is_lvds) {
+			dpll |= (1 << (clock.p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
+		} else {
+			if (clock.p1 == 2)
+				dpll |= PLL_P1_DIVIDE_BY_TWO;
+			else
+				dpll |= (clock.p1 - 2) << DPLL_FPA01_P1_POST_DIV_SHIFT;
+			if (clock.p2 == 4)
+				dpll |= PLL_P2_DIVIDE_BY_4;
+		}
+	}
+
+	if (is_tv) {
+		/* XXX: just matching BIOS for now */
+/*	dpll |= PLL_REF_INPUT_TVCLKINBC; */
+		dpll |= 3;
+	}
+#if 0
+	else if (is_lvds)
+		dpll |= PLLB_REF_INPUT_SPREADSPECTRUMIN;
+#endif
+	else
+		dpll |= PLL_REF_INPUT_DREFCLK;
+
+	/* setup pipeconf */
+	pipeconf = I915_READ(pipeconf_reg);
+
+	/* Set up the display plane register */
+	dspcntr = DISPPLANE_GAMMA_ENABLE;
+
+	switch (crtc->fb->bits_per_pixel) {
+	case 8:
+		dspcntr |= DISPPLANE_8BPP;
+		break;
+	case 16:
+		if (crtc->fb->depth == 15)
+			dspcntr |= DISPPLANE_15_16BPP;
+		else
+			dspcntr |= DISPPLANE_16BPP;
+		break;
+	case 32:
+		dspcntr |= DISPPLANE_32BPP_NO_ALPHA;
+		break;
+	default:
+		DRM_ERROR("Unknown color depth\n");
+		return;
+	}
+
+
+	if (pipe == 0)
+		dspcntr |= DISPPLANE_SEL_PIPE_A;
+	else
+		dspcntr |= DISPPLANE_SEL_PIPE_B;
+
+	if (pipe == 0 && !IS_I965G(dev)) {
+		/* Enable pixel doubling when the dot clock is > 90% of the (display)
+		 * core speed.
+		 *
+		 * XXX: No double-wide on 915GM pipe B. Is that the only reason for the
+		 * pipe == 0 check?
+		 */
+		if (mode->clock > intel_get_core_clock_speed(dev) * 9 / 10)
+			pipeconf |= PIPEACONF_DOUBLE_WIDE;
+		else
+			pipeconf &= ~PIPEACONF_DOUBLE_WIDE;
+	}
+
+	dspcntr |= DISPLAY_PLANE_ENABLE;
+	pipeconf |= PIPEACONF_ENABLE;
+	dpll |= DPLL_VCO_ENABLE;
+
+
+	/* Disable the panel fitter if it was on our pipe */
+	if (intel_panel_fitter_pipe(dev) == pipe)
+		I915_WRITE(PFIT_CONTROL, 0);
+
+	DRM_DEBUG("Mode for pipe %c:\n", pipe == 0 ? 'A' : 'B');
+	drm_mode_debug_printmodeline(dev, mode);
+
+        /*psbPrintPll("chosen", &clock);*/
+    DRM_DEBUG("clock regs: 0x%08x, 0x%08x,dspntr is 0x%8x, pipeconf is 0x%8x\n", (int)dpll,
+	      (int)fp,(int)dspcntr,(int)pipeconf);
+#if 0
+	if (!xf86ModesEqual(mode, adjusted_mode)) {
+		xf86DrvMsg(pScrn->scrnIndex, X_INFO,
+			   "Adjusted mode for pipe %c:\n", pipe == 0 ? 'A' : 'B');
+		xf86PrintModeline(pScrn->scrnIndex, mode);
+	}
+	i830PrintPll("chosen", &clock);
+#endif
+
+	if (dpll & DPLL_VCO_ENABLE) {
+		I915_WRITE(fp_reg, fp);
+		I915_WRITE(dpll_reg, dpll & ~DPLL_VCO_ENABLE);
+		I915_READ(dpll_reg);
+		udelay(150);
+	}
+
+	/* The LVDS pin pair needs to be on before the DPLLs are enabled.
+	 * This is an exception to the general rule that mode_set doesn't turn
+	 * things on.
+	 */
+	if (is_lvds) {
+		u32 lvds = I915_READ(LVDS);
+
+		lvds |= LVDS_PORT_EN | LVDS_A0A2_CLKA_POWER_UP | LVDS_PIPEB_SELECT;
+		/* Set the B0-B3 data pairs corresponding to whether we're going to
+		 * set the DPLLs for dual-channel mode or not.
+		 */
+		if (clock.p2 == 7)
+			lvds |= LVDS_B0B3_POWER_UP | LVDS_CLKB_POWER_UP;
+		else
+			lvds &= ~(LVDS_B0B3_POWER_UP | LVDS_CLKB_POWER_UP);
+
+		/* It would be nice to set 24 vs 18-bit mode (LVDS_A3_POWER_UP)
+		 * appropriately here, but we need to look more thoroughly into how
+		 * panels behave in the two modes.
+		 */
+
+		I915_WRITE(LVDS, lvds);
+		I915_READ(LVDS);
+	}
+
+	I915_WRITE(fp_reg, fp);
+	I915_WRITE(dpll_reg, dpll);
+	I915_READ(dpll_reg);
+	/* Wait for the clocks to stabilize. */
+	udelay(150);
+
+	if (IS_I965G(dev)) {
+		int sdvo_pixel_multiply = adjusted_mode->clock / mode->clock;
+		I915_WRITE(dpll_md_reg, (0 << DPLL_MD_UDI_DIVIDER_SHIFT) |
+			   ((sdvo_pixel_multiply - 1) << DPLL_MD_UDI_MULTIPLIER_SHIFT));
+	} else {
+		/* write it again -- the BIOS does, after all */
+		I915_WRITE(dpll_reg, dpll);
+	}
+	I915_READ(dpll_reg);
+	/* Wait for the clocks to stabilize. */
+	udelay(150);
+
+	I915_WRITE(htot_reg, (adjusted_mode->crtc_hdisplay - 1) |
+		   ((adjusted_mode->crtc_htotal - 1) << 16));
+	I915_WRITE(hblank_reg, (adjusted_mode->crtc_hblank_start - 1) |
+		   ((adjusted_mode->crtc_hblank_end - 1) << 16));
+	I915_WRITE(hsync_reg, (adjusted_mode->crtc_hsync_start - 1) |
+		   ((adjusted_mode->crtc_hsync_end - 1) << 16));
+	I915_WRITE(vtot_reg, (adjusted_mode->crtc_vdisplay - 1) |
+		   ((adjusted_mode->crtc_vtotal - 1) << 16));
+	I915_WRITE(vblank_reg, (adjusted_mode->crtc_vblank_start - 1) |
+		   ((adjusted_mode->crtc_vblank_end - 1) << 16));
+	I915_WRITE(vsync_reg, (adjusted_mode->crtc_vsync_start - 1) |
+		   ((adjusted_mode->crtc_vsync_end - 1) << 16));
+	I915_WRITE(dspstride_reg, crtc->fb->pitch);
+	/* pipesrc and dspsize control the size that is scaled from, which should
+	 * always be the user's requested size.
+	 */
+	I915_WRITE(dspsize_reg, ((mode->vdisplay - 1) << 16) | (mode->hdisplay - 1));
+	I915_WRITE(dsppos_reg, 0);
+	I915_WRITE(pipesrc_reg, ((mode->hdisplay - 1) << 16) | (mode->vdisplay - 1));
+	I915_WRITE(pipeconf_reg, pipeconf);
+	I915_READ(pipeconf_reg);
+
+	intel_wait_for_vblank(dev);
+
+	I915_WRITE(dspcntr_reg, dspcntr);
+
+	/* Flush the plane changes */
+	intel_pipe_set_base(crtc, x, y);
+
+#if 0
+	intel_set_vblank(dev);
+#endif
+
+	/* Disable the VGA plane that we never use */
+	I915_WRITE(VGACNTRL, VGA_DISP_DISABLE);
+
+	intel_wait_for_vblank(dev);
+}
+
+/** Loads the palette/gamma unit for the CRTC with the prepared values */
+void intel_crtc_load_lut(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int palreg = (intel_crtc->pipe == 0) ? PALETTE_A : PALETTE_B;
+	int i;
+
+	/* The clocks have to be on to load the palette. */
+	if (!crtc->enabled)
+		return;
+
+	for (i = 0; i < 256; i++) {
+		I915_WRITE(palreg + 4 * i,
+			   (intel_crtc->lut_r[i] << 16) |
+			   (intel_crtc->lut_g[i] << 8) |
+			   intel_crtc->lut_b[i]);
+	}
+}
+
+/** Sets the color ramps on behalf of RandR */
+static void intel_crtc_gamma_set(struct drm_crtc *crtc, u16 red, u16 green,
+				 u16 blue, int regno)
+{
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+
+	intel_crtc->lut_r[regno] = red >> 8;
+	intel_crtc->lut_g[regno] = green >> 8;
+	intel_crtc->lut_b[regno] = blue >> 8;
+}
+
+/* Returns the clock of the currently programmed mode of the given pipe. */
+static int intel_crtc_clock_get(struct drm_device *dev, struct drm_crtc *crtc)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	u32 dpll = I915_READ((pipe == 0) ? DPLL_A : DPLL_B);
+	u32 fp;
+	intel_clock_t clock;
+
+	if ((dpll & DISPLAY_RATE_SELECT_FPA1) == 0)
+		fp = I915_READ((pipe == 0) ? FPA0 : FPB0);
+	else
+		fp = I915_READ((pipe == 0) ? FPA1 : FPB1);
+
+	clock.m1 = (fp & FP_M1_DIV_MASK) >> FP_M1_DIV_SHIFT;
+	clock.m2 = (fp & FP_M2_DIV_MASK) >> FP_M2_DIV_SHIFT;
+	clock.n = (fp & FP_N_DIV_MASK) >> FP_N_DIV_SHIFT;
+	if (IS_I9XX(dev)) {
+		clock.p1 = ffs((dpll & DPLL_FPA01_P1_POST_DIV_MASK) >>
+			       DPLL_FPA01_P1_POST_DIV_SHIFT);
+
+		switch (dpll & DPLL_MODE_MASK) {
+		case DPLLB_MODE_DAC_SERIAL:
+			clock.p2 = dpll & DPLL_DAC_SERIAL_P2_CLOCK_DIV_5 ?
+				5 : 10;
+			break;
+		case DPLLB_MODE_LVDS:
+			clock.p2 = dpll & DPLLB_LVDS_P2_CLOCK_DIV_7 ?
+				7 : 14;
+			break;
+		default:
+			DRM_DEBUG("Unknown DPLL mode %08x in programmed "
+				  "mode\n", (int)(dpll & DPLL_MODE_MASK));
+			return 0;
+		}
+
+		/* XXX: Handle the 100Mhz refclk */
+		i9xx_clock(96000, &clock);
+	} else {
+		bool is_lvds = (pipe == 1) && (I915_READ(LVDS) & LVDS_PORT_EN);
+
+		if (is_lvds) {
+			clock.p1 = ffs((dpll & DPLL_FPA01_P1_POST_DIV_MASK_I830_LVDS) >>
+				       DPLL_FPA01_P1_POST_DIV_SHIFT);
+			clock.p2 = 14;
+
+			if ((dpll & PLL_REF_INPUT_MASK) ==
+			    PLLB_REF_INPUT_SPREADSPECTRUMIN) {
+				/* XXX: might not be 66MHz */
+				i8xx_clock(66000, &clock);
+			} else
+				i8xx_clock(48000, &clock);
+		} else {
+			if (dpll & PLL_P1_DIVIDE_BY_TWO)
+				clock.p1 = 2;
+			else {
+				clock.p1 = ((dpll & DPLL_FPA01_P1_POST_DIV_MASK_I830) >>
+					    DPLL_FPA01_P1_POST_DIV_SHIFT) + 2;
+			}
+			if (dpll & PLL_P2_DIVIDE_BY_4)
+				clock.p2 = 4;
+			else
+				clock.p2 = 2;
+
+			i8xx_clock(48000, &clock);
+		}
+	}
+
+	/* XXX: It would be nice to validate the clocks, but we can't reuse
+	 * i830PllIsValid() because it relies on the xf86_config output
+	 * configuration being accurate, which it isn't necessarily.
+	 */
+
+	return clock.dot;
+}
+
+/** Returns the currently programmed mode of the given pipe. */
+struct drm_display_mode *intel_crtc_mode_get(struct drm_device *dev,
+					     struct drm_crtc *crtc)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	int pipe = intel_crtc->pipe;
+	struct drm_display_mode *mode;
+	int htot = I915_READ((pipe == 0) ? HTOTAL_A : HTOTAL_B);
+	int hsync = I915_READ((pipe == 0) ? HSYNC_A : HSYNC_B);
+	int vtot = I915_READ((pipe == 0) ? VTOTAL_A : VTOTAL_B);
+	int vsync = I915_READ((pipe == 0) ? VSYNC_A : VSYNC_B);
+
+	mode = kzalloc(sizeof(*mode), GFP_KERNEL);
+	if (!mode)
+		return NULL;
+
+	mode->clock = intel_crtc_clock_get(dev, crtc);
+	mode->hdisplay = (htot & 0xffff) + 1;
+	mode->htotal = ((htot & 0xffff0000) >> 16) + 1;
+	mode->hsync_start = (hsync & 0xffff) + 1;
+	mode->hsync_end = ((hsync & 0xffff0000) >> 16) + 1;
+	mode->vdisplay = (vtot & 0xffff) + 1;
+	mode->vtotal = ((vtot & 0xffff0000) >> 16) + 1;
+	mode->vsync_start = (vsync & 0xffff) + 1;
+	mode->vsync_end = ((vsync & 0xffff0000) >> 16) + 1;
+
+	drm_mode_set_name(mode);
+	drm_mode_set_crtcinfo(mode, 0);
+
+	return mode;
+}
+
+static const struct drm_crtc_funcs intel_crtc_funcs = {
+	.dpms = intel_crtc_dpms,
+	.lock = intel_crtc_lock,
+	.unlock = intel_crtc_unlock,
+	.mode_fixup = intel_crtc_mode_fixup,
+	.mode_set = intel_crtc_mode_set,
+	.gamma_set = intel_crtc_gamma_set,
+	.prepare = intel_crtc_prepare,
+	.commit = intel_crtc_commit,
+};
+
+
+void intel_crtc_init(struct drm_device *dev, int pipe)
+{
+	struct drm_crtc *crtc;
+	struct intel_crtc *intel_crtc;
+	int i;
+
+	crtc = drm_crtc_create(dev, &intel_crtc_funcs);
+	if (crtc == NULL)
+		return;
+
+	intel_crtc = kzalloc(sizeof(struct intel_crtc), GFP_KERNEL);
+	if (intel_crtc == NULL) {
+		kfree(crtc);
+		return;
+	}
+
+	intel_crtc->pipe = pipe;
+	for (i = 0; i < 256; i++) {
+		intel_crtc->lut_r[i] = i;
+		intel_crtc->lut_g[i] = i;
+		intel_crtc->lut_b[i] = i;
+	}
+
+	crtc->driver_private = intel_crtc;
+}
+
+struct drm_crtc *intel_get_crtc_from_pipe(struct drm_device *dev, int pipe)
+{
+	struct drm_crtc *crtc = NULL;
+
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		struct intel_crtc *intel_crtc = crtc->driver_private;
+		if (intel_crtc->pipe == pipe)
+			break;
+	}
+	return crtc;
+}
+
+int intel_output_clones(struct drm_device *dev, int type_mask)
+{
+	int index_mask = 0;
+	struct drm_output *output;
+	int entry = 0;
+
+        list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+		if (type_mask & (1 << intel_output->type))
+			index_mask |= (1 << entry);
+		entry++;
+	}
+	return index_mask;
+}
+
+
+static void intel_setup_outputs(struct drm_device *dev)
+{
+	struct drm_output *output;
+
+	if (!IS_POULSBO(dev))
+		intel_crt_init(dev);
+
+	/* Set up integrated LVDS */
+	if (IS_MOBILE(dev) && !IS_I830(dev))
+		intel_lvds_init(dev);
+
+	if (IS_I9XX(dev)) {
+		intel_sdvo_init(dev, SDVOB);
+		intel_sdvo_init(dev, SDVOC);
+	}
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		struct intel_output *intel_output = output->driver_private;
+		int crtc_mask = 0, clone_mask = 0;
+
+		/* valid crtcs */
+		switch(intel_output->type) {
+		case INTEL_OUTPUT_DVO:
+		case INTEL_OUTPUT_SDVO:
+			crtc_mask = ((1 << 0)|
+				     (1 << 1));
+			clone_mask = ((1 << INTEL_OUTPUT_ANALOG) |
+				      (1 << INTEL_OUTPUT_DVO) |
+				      (1 << INTEL_OUTPUT_SDVO));
+			break;
+		case INTEL_OUTPUT_ANALOG:
+			crtc_mask = ((1 << 0)|
+				     (1 << 1));
+			clone_mask = ((1 << INTEL_OUTPUT_ANALOG) |
+				      (1 << INTEL_OUTPUT_DVO) |
+				      (1 << INTEL_OUTPUT_SDVO));
+			break;
+		case INTEL_OUTPUT_LVDS:
+			crtc_mask = (1 << 1);
+			clone_mask = (1 << INTEL_OUTPUT_LVDS);
+			break;
+		case INTEL_OUTPUT_TVOUT:
+			crtc_mask = ((1 << 0) |
+				     (1 << 1));
+			clone_mask = (1 << INTEL_OUTPUT_TVOUT);
+			break;
+		}
+		output->possible_crtcs = crtc_mask;
+		output->possible_clones = intel_output_clones(dev, clone_mask);
+	}
+}
+
+void intel_modeset_init(struct drm_device *dev)
+{
+	int num_pipe;
+	int i;
+
+	drm_mode_config_init(dev);
+
+	dev->mode_config.min_width = 0;
+	dev->mode_config.min_height = 0;
+
+	dev->mode_config.max_width = 4096;
+	dev->mode_config.max_height = 4096;
+
+	/* set memory base */
+	if (IS_I9XX(dev))
+		dev->mode_config.fb_base = pci_resource_start(dev->pdev, 2);
+	else
+		dev->mode_config.fb_base = pci_resource_start(dev->pdev, 0);
+
+	if (IS_MOBILE(dev) || IS_I9XX(dev))
+		num_pipe = 2;
+	else
+		num_pipe = 1;
+	DRM_DEBUG("%d display pipe%s available.\n",
+		  num_pipe, num_pipe > 1 ? "s" : "");
+
+	for (i = 0; i < num_pipe; i++) {
+		intel_crtc_init(dev, i);
+	}
+
+	intel_setup_outputs(dev);
+
+	//drm_initial_config(dev, false);
+}
+
+void intel_modeset_cleanup(struct drm_device *dev)
+{
+	drm_mode_config_cleanup(dev);
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_drv.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_drv.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,91 @@
+/*
+ * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>
+ * Copyright (c) 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+#ifndef __INTEL_DRV_H__
+#define __INTEL_DRV_H__
+
+#include <linux/i2c.h>
+#include <linux/i2c-id.h>
+#include <linux/i2c-algo-bit.h>
+#include "drm_crtc.h"
+
+/*
+ * Display related stuff
+ */
+
+/* store information about an Ixxx DVO */
+/* The i830->i865 use multiple DVOs with multiple i2cs */
+/* the i915, i945 have a single sDVO i2c bus - which is different */
+#define MAX_OUTPUTS 6
+
+#define INTEL_I2C_BUS_DVO 1
+#define INTEL_I2C_BUS_SDVO 2
+
+/* these are outputs from the chip - integrated only
+   external chips are via DVO or SDVO output */
+#define INTEL_OUTPUT_UNUSED 0
+#define INTEL_OUTPUT_ANALOG 1
+#define INTEL_OUTPUT_DVO 2
+#define INTEL_OUTPUT_SDVO 3
+#define INTEL_OUTPUT_LVDS 4
+#define INTEL_OUTPUT_TVOUT 5
+
+#define INTEL_DVO_CHIP_NONE 0
+#define INTEL_DVO_CHIP_LVDS 1
+#define INTEL_DVO_CHIP_TMDS 2
+#define INTEL_DVO_CHIP_TVOUT 4
+
+struct intel_i2c_chan {
+	struct drm_device *drm_dev; /* for getting at dev. private (mmio etc.) */
+	u32 reg; /* GPIO reg */
+	struct i2c_adapter adapter;
+	struct i2c_algo_bit_data algo;
+        u8 slave_addr;
+};
+
+struct intel_output {
+	int type;
+	struct intel_i2c_chan *i2c_bus; /* for control functions */
+	struct intel_i2c_chan *ddc_bus; /* for DDC only stuff */
+	bool load_detect_tmp;
+	void *dev_priv;
+};
+
+struct intel_crtc {
+	int pipe;
+	u8 lut_r[256], lut_g[256], lut_b[256];
+};
+
+struct intel_i2c_chan *intel_i2c_create(struct drm_device *dev, const u32 reg,
+					const char *name);
+void intel_i2c_destroy(struct intel_i2c_chan *chan);
+int intel_ddc_get_modes(struct drm_output *output);
+extern bool intel_ddc_probe(struct drm_output *output);
+
+extern void intel_crt_init(struct drm_device *dev);
+extern void intel_sdvo_init(struct drm_device *dev, int output_device);
+extern void intel_lvds_init(struct drm_device *dev);
+
+extern void intel_crtc_load_lut(struct drm_crtc *crtc);
+extern void intel_output_prepare (struct drm_output *output);
+extern void intel_output_commit (struct drm_output *output);
+extern struct drm_display_mode *intel_crtc_mode_get(struct drm_device *dev,
+ 						    struct drm_crtc *crtc);
+extern void intel_wait_for_vblank(struct drm_device *dev);
+extern struct drm_crtc *intel_get_crtc_from_pipe(struct drm_device *dev, int pipe);
+
+extern int intelfb_probe(struct drm_device *dev, struct drm_crtc *crtc);
+extern int intelfb_remove(struct drm_device *dev, struct drm_crtc *crtc);
+
+extern void intel_modeset_init(struct drm_device *dev);
+extern void intel_modeset_cleanup(struct drm_device *dev);
+
+#define WA_NO_FB_GARBAGE_DISPLAY
+#ifdef WA_NO_FB_GARBAGE_DISPLAY
+extern void intel_crtc_mode_restore(struct drm_crtc *crtc);
+extern void intel_crtc_mode_save(struct drm_crtc *crtc);
+#endif
+
+#endif /* __INTEL_DRV_H__ */
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_lvds.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_lvds.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,913 @@
+/*
+ * Copyright Â© 2006-2007 Intel Corporation
+ * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ *      Dave Airlie <airlied@linux.ie>
+ *      Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include <linux/backlight.h>
+#include "drm_crtc.h"
+#include "drm_edid.h"
+#include "intel_lvds.h"
+
+#include <acpi/acpi_drivers.h>
+
+int drm_intel_ignore_acpi = 0;
+MODULE_PARM_DESC(ignore_acpi, "Ignore ACPI");
+module_param_named(ignore_acpi, drm_intel_ignore_acpi, int, 0600);
+
+uint8_t blc_type;
+uint8_t blc_pol;
+uint8_t blc_freq;
+uint8_t blc_minbrightness;
+uint8_t blc_i2caddr;
+uint8_t blc_brightnesscmd;
+int lvds_backlight;	/* restore backlight to this value */
+
+struct intel_i2c_chan *lvds_i2c_bus;
+u32 CoreClock;
+u32 PWMControlRegFreq;
+
+unsigned char * dev_OpRegion = NULL;
+unsigned int dev_OpRegionSize;
+
+#define PCI_PORT5_REG80_FFUSE				0xD0058000
+#define PCI_PORT5_REG80_MAXRES_INT_EN		0x0040
+#define MAX_HDISPLAY 800
+#define MAX_VDISPLAY 480
+bool sku_bMaxResEnableInt = false;
+
+/** Set BLC through I2C*/
+static int
+LVDSI2CSetBacklight(struct drm_device *dev, unsigned char ch)
+{
+	u8 out_buf[2];
+	struct i2c_msg msgs[] = {
+		{
+			.addr = lvds_i2c_bus->slave_addr,
+			.flags = 0,
+			.len = 2,
+			.buf = out_buf,
+		}
+	};
+
+	DRM_INFO("LVDSI2CSetBacklight: the slave_addr is 0x%x, the backlight value is %d\n", lvds_i2c_bus->slave_addr, ch);
+
+	out_buf[0] = blc_brightnesscmd;
+	out_buf[1] = ch;
+
+	if (i2c_transfer(&lvds_i2c_bus->adapter, msgs, 1) == 1)
+	{
+		DRM_INFO("LVDSI2CSetBacklight: i2c_transfer done\n");
+		return true;
+	}
+
+	DRM_ERROR("msg: i2c_transfer error\n");
+	return false;
+}
+
+/**
+ * Calculate PWM control register value.
+ */
+static int
+LVDSCalculatePWMCtrlRegFreq(struct drm_device *dev)
+{
+	unsigned long value = 0;
+
+	DRM_INFO("Enter LVDSCalculatePWMCtrlRegFreq.\n");
+	if (blc_freq == 0) {
+		DRM_ERROR("LVDSCalculatePWMCtrlRegFreq:  Frequency Requested is 0.\n");
+		return FALSE;
+	}
+	value = (CoreClock * MHz);
+	value = (value / BLC_PWM_FREQ_CALC_CONSTANT);
+	value = (value * BLC_PWM_PRECISION_FACTOR);
+	value = (value / blc_freq);
+	value = (value / BLC_PWM_PRECISION_FACTOR);
+
+	if (value > (unsigned long)BLC_MAX_PWM_REG_FREQ ||
+			value < (unsigned long)BLC_MIN_PWM_REG_FREQ) {
+		return FALSE;
+	} else {
+		PWMControlRegFreq = ((u32)value & ~BLC_PWM_LEGACY_MODE_ENABLE);
+		return TRUE;
+	}
+}
+
+/**
+ * Returns the maximum level of the backlight duty cycle field.
+ */
+static u32
+LVDSGetPWMMaxBacklight(struct drm_device *dev)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	u32 max_pwm_blc = 0;
+
+	max_pwm_blc = ((I915_READ(BLC_PWM_CTL) & BACKLIGHT_MODULATION_FREQ_MASK) >> \
+			BACKLIGHT_MODULATION_FREQ_SHIFT) * 2;
+
+	if (!(max_pwm_blc & BLC_MAX_PWM_REG_FREQ)) {
+		if (LVDSCalculatePWMCtrlRegFreq(dev)) {
+			max_pwm_blc = PWMControlRegFreq;
+		}
+	}
+
+	DRM_INFO("LVDSGetPWMMaxBacklight: the max_pwm_blc is %d.\n", max_pwm_blc);
+	return max_pwm_blc;
+}
+
+
+/**
+ * Sets the backlight level.
+ *
+ * \param level backlight level, from 0 to intel_lvds_get_max_backlight().
+ */
+static void intel_lvds_set_backlight(struct drm_device *dev, int level)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	//u32 blc_pwm_ctl;
+
+	/*
+     	blc_pwm_ctl = I915_READ(BLC_PWM_CTL) & ~BACKLIGHT_DUTY_CYCLE_MASK;
+	I915_WRITE(BLC_PWM_CTL, (blc_pwm_ctl |
+		(level << BACKLIGHT_DUTY_CYCLE_SHIFT)));
+	 */
+	u32 newbacklight = 0;
+
+	DRM_INFO("intel_lvds_set_backlight: the level is %d\n", level);
+
+	if(blc_type == BLC_I2C_TYPE){
+		newbacklight = BRIGHTNESS_MASK & ((unsigned long)level * \
+				BRIGHTNESS_MASK /BRIGHTNESS_MAX_LEVEL);
+
+		if (blc_pol == BLC_POLARITY_INVERSE) {
+			newbacklight = BRIGHTNESS_MASK - newbacklight;
+		}
+
+		LVDSI2CSetBacklight(dev, newbacklight);
+
+	} else if (blc_type == BLC_PWM_TYPE) {
+		u32 max_pwm_blc = LVDSGetPWMMaxBacklight(dev);
+
+		u32 blc_pwm_duty_cycle;
+
+		/* Provent LVDS going to total black */
+		if ( level < 20) {
+			level = 20;
+		}
+		blc_pwm_duty_cycle = level * max_pwm_blc/BRIGHTNESS_MAX_LEVEL;
+
+		if (blc_pol == BLC_POLARITY_INVERSE) {
+			blc_pwm_duty_cycle = max_pwm_blc - blc_pwm_duty_cycle;
+		}
+
+		blc_pwm_duty_cycle &= BACKLIGHT_PWM_POLARITY_BIT_CLEAR;
+
+		I915_WRITE(BLC_PWM_CTL,
+				(max_pwm_blc << BACKLIGHT_PWM_CTL_SHIFT)| (blc_pwm_duty_cycle));
+	}
+}
+
+/**
+ * Returns the maximum level of the backlight duty cycle field.
+ */
+static u32 intel_lvds_get_max_backlight(struct drm_device *dev)
+{
+	return BRIGHTNESS_MAX_LEVEL;
+	/*
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+
+	return ((I915_READ(BLC_PWM_CTL) & BACKLIGHT_MODULATION_FREQ_MASK) >>
+		BACKLIGHT_MODULATION_FREQ_SHIFT) * 2;
+	*/
+}
+
+/**
+ * Sets the power state for the panel.
+ */
+static void intel_lvds_set_power(struct drm_device *dev, bool on)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	u32 pp_status;
+
+	DRM_INFO("intel_lvds_set_power: %d\n", on);
+	if (on) {
+		I915_WRITE(PP_CONTROL, I915_READ(PP_CONTROL) |
+				POWER_TARGET_ON);
+		do {
+			pp_status = I915_READ(PP_STATUS);
+		} while ((pp_status & PP_ON) == 0);
+
+		intel_lvds_set_backlight(dev, lvds_backlight);
+	} else {
+		intel_lvds_set_backlight(dev, 0);
+
+		I915_WRITE(PP_CONTROL, I915_READ(PP_CONTROL) &
+				~POWER_TARGET_ON);
+		do {
+			pp_status = I915_READ(PP_STATUS);
+		} while (pp_status & PP_ON);
+	}
+}
+
+static void intel_lvds_dpms(struct drm_output *output, int mode)
+{
+	struct drm_device *dev = output->dev;
+
+	DRM_INFO("intel_lvds_dpms: the mode is %d\n", mode);
+	if (mode == DPMSModeOn)
+		intel_lvds_set_power(dev, true);
+	else
+		intel_lvds_set_power(dev, false);
+
+	/* XXX: We never power down the LVDS pairs. */
+}
+
+static void intel_lvds_save(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+
+	dev_priv->savePP_ON = I915_READ(LVDSPP_ON);
+	dev_priv->savePP_OFF = I915_READ(LVDSPP_OFF);
+	dev_priv->savePP_CONTROL = I915_READ(PP_CONTROL);
+	dev_priv->savePP_CYCLE = I915_READ(PP_CYCLE);
+	dev_priv->saveBLC_PWM_CTL = I915_READ(BLC_PWM_CTL);
+	dev_priv->backlight_duty_cycle = (dev_priv->saveBLC_PWM_CTL &
+				       BACKLIGHT_DUTY_CYCLE_MASK);
+
+	/*
+	 * If the light is off at server startup, just make it full brightness
+	 */
+	if (dev_priv->backlight_duty_cycle == 0)
+		lvds_backlight=
+			intel_lvds_get_max_backlight(dev);
+}
+
+static void intel_lvds_restore(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+
+	I915_WRITE(BLC_PWM_CTL, dev_priv->saveBLC_PWM_CTL);
+	I915_WRITE(LVDSPP_ON, dev_priv->savePP_ON);
+	I915_WRITE(LVDSPP_OFF, dev_priv->savePP_OFF);
+	I915_WRITE(PP_CYCLE, dev_priv->savePP_CYCLE);
+	I915_WRITE(PP_CONTROL, dev_priv->savePP_CONTROL);
+	if (dev_priv->savePP_CONTROL & POWER_TARGET_ON)
+		intel_lvds_set_power(dev, true);
+	else
+		intel_lvds_set_power(dev, false);
+}
+
+static int intel_lvds_mode_valid(struct drm_output *output,
+				 struct drm_display_mode *mode)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct drm_display_mode *fixed_mode = dev_priv->panel_fixed_mode;
+
+	if (fixed_mode)	{
+		if (mode->hdisplay > fixed_mode->hdisplay)
+			return MODE_PANEL;
+		if (mode->vdisplay > fixed_mode->vdisplay)
+			return MODE_PANEL;
+	}
+
+	if (IS_POULSBO(dev) && sku_bMaxResEnableInt) {
+		if (mode->hdisplay > MAX_HDISPLAY)
+			return MODE_PANEL;
+		if (mode->vdisplay > MAX_VDISPLAY)
+			return MODE_PANEL;
+	}
+
+	return MODE_OK;
+}
+
+static bool intel_lvds_mode_fixup(struct drm_output *output,
+				  struct drm_display_mode *mode,
+				  struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = output->crtc->driver_private;
+	struct drm_output *tmp_output;
+
+	/* Should never happen!! */
+	if (!IS_I965G(dev) && intel_crtc->pipe == 0) {
+		DRM_ERROR(KERN_ERR "Can't support LVDS on pipe A\n");
+		return false;
+	}
+
+	/* Should never happen!! */
+	list_for_each_entry(tmp_output, &dev->mode_config.output_list, head) {
+		if (tmp_output != output && tmp_output->crtc == output->crtc) {
+			DRM_ERROR("Can't enable LVDS and another "
+			       "output on the same pipe\n");
+			return false;
+		}
+	}
+
+	/*
+	 * If we have timings from the BIOS for the panel, put them in
+	 * to the adjusted mode.  The CRTC will be set up for this mode,
+	 * with the panel scaling set up to source from the H/VDisplay
+	 * of the original mode.
+	 */
+	if (dev_priv->panel_fixed_mode != NULL) {
+		adjusted_mode->hdisplay = dev_priv->panel_fixed_mode->hdisplay;
+		adjusted_mode->hsync_start =
+			dev_priv->panel_fixed_mode->hsync_start;
+		adjusted_mode->hsync_end =
+			dev_priv->panel_fixed_mode->hsync_end;
+		adjusted_mode->htotal = dev_priv->panel_fixed_mode->htotal;
+		adjusted_mode->vdisplay = dev_priv->panel_fixed_mode->vdisplay;
+		adjusted_mode->vsync_start =
+			dev_priv->panel_fixed_mode->vsync_start;
+		adjusted_mode->vsync_end =
+			dev_priv->panel_fixed_mode->vsync_end;
+		adjusted_mode->vtotal = dev_priv->panel_fixed_mode->vtotal;
+		adjusted_mode->clock = dev_priv->panel_fixed_mode->clock;
+		drm_mode_set_crtcinfo(adjusted_mode, CRTC_INTERLACE_HALVE_V);
+	}
+
+	/*
+	 * XXX: It would be nice to support lower refresh rates on the
+	 * panels to reduce power consumption, and perhaps match the
+	 * user's requested refresh rate.
+	 */
+
+	return true;
+}
+
+static void intel_lvds_prepare(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+
+	DRM_INFO("intel_lvds_prepare\n");
+	dev_priv->saveBLC_PWM_CTL = I915_READ(BLC_PWM_CTL);
+	dev_priv->backlight_duty_cycle = (dev_priv->saveBLC_PWM_CTL &
+				       BACKLIGHT_DUTY_CYCLE_MASK);
+
+	intel_lvds_set_power(dev, false);
+}
+
+static void intel_lvds_commit( struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+
+	DRM_INFO("intel_lvds_commit\n");
+	if (dev_priv->backlight_duty_cycle == 0)
+		//dev_priv->backlight_duty_cycle =
+		lvds_backlight =
+			intel_lvds_get_max_backlight(dev);
+
+	intel_lvds_set_power(dev, true);
+}
+
+static void intel_lvds_mode_set(struct drm_output *output,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = output->crtc->driver_private;
+	u32 pfit_control;
+
+	/*
+	 * The LVDS pin pair will already have been turned on in the
+	 * intel_crtc_mode_set since it has a large impact on the DPLL
+	 * settings.
+	 */
+
+	/*
+	 * Enable automatic panel scaling so that non-native modes fill the
+	 * screen.  Should be enabled before the pipe is enabled, according to
+	 * register description and PRM.
+	 */
+	if (mode->hdisplay != adjusted_mode->hdisplay ||
+	    mode->vdisplay != adjusted_mode->vdisplay)
+		pfit_control = (PFIT_ENABLE | VERT_AUTO_SCALE |
+				HORIZ_AUTO_SCALE | VERT_INTERP_BILINEAR |
+				HORIZ_INTERP_BILINEAR);
+	else
+		pfit_control = 0;
+
+	if (!IS_I965G(dev)) {
+		if (dev_priv->panel_wants_dither)
+			pfit_control |= PANEL_8TO6_DITHER_ENABLE;
+	}
+	else
+		pfit_control |= intel_crtc->pipe << PFIT_PIPE_SHIFT;
+
+	I915_WRITE(PFIT_CONTROL, pfit_control);
+}
+
+/**
+ * Detect the LVDS connection.
+ *
+ * This always returns OUTPUT_STATUS_CONNECTED.  This output should only have
+ * been set up if the LVDS was actually connected anyway.
+ */
+static enum drm_output_status intel_lvds_detect(struct drm_output *output)
+{
+	return output_status_connected;
+}
+
+/**
+ * Return the list of DDC modes if available.
+ */
+static int intel_lvds_get_modes(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	struct intel_output *intel_output = output->driver_private;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+        struct edid *edid;
+
+	/* Try reading DDC from the adapter */
+        edid = (struct edid *)drm_ddc_read(&intel_output->ddc_bus->adapter);
+
+        if (!edid) {
+                DRM_INFO("%s: no EDID data from device, reading ACPI _DDC data.\n",
+                         output->name);
+                edid = kzalloc(sizeof(struct edid), GFP_KERNEL);
+                drm_get_acpi_edid(ACPI_EDID_LCD, (char*)edid, 128);
+        }
+
+	if (edid)
+	        drm_add_edid_modes(output, edid);
+
+	/* Didn't get an EDID */
+	if (!output->monitor_info) {
+		struct drm_display_info *dspinfo;
+		dspinfo = kzalloc(sizeof(*output->monitor_info), GFP_KERNEL);
+		if (!dspinfo)
+			goto out;
+
+		/* Set wide sync ranges so we get all modes
+		 * handed to valid_mode for checking
+		 */
+		dspinfo->min_vfreq = 0;
+		dspinfo->max_vfreq = 200;
+		dspinfo->min_hfreq = 0;
+		dspinfo->max_hfreq = 200;
+		output->monitor_info = dspinfo;
+	}
+
+out:
+	if (dev_priv->panel_fixed_mode != NULL) {
+		struct drm_display_mode *mode =
+			drm_mode_duplicate(dev, dev_priv->panel_fixed_mode);
+		drm_mode_probed_add(output, mode);
+		return 1;
+	}
+
+	return 0;
+}
+
+/* added by alek du to add /sys/class/backlight interface */
+static int update_bl_status(struct backlight_device *bd)
+{
+        int value = bd->props.brightness;
+
+	struct drm_device *dev = bl_get_data(bd);
+
+	lvds_backlight = value;
+	intel_lvds_set_backlight(dev, value);
+	/*value = (bd->props.power == FB_BLANK_UNBLANK) ? 1 : 0;
+	intel_lvds_set_power(dev,value);*/
+	return 0;
+}
+
+static int read_brightness(struct backlight_device *bd)
+{
+        return bd->props.brightness;
+}
+
+static struct backlight_device *psbbl_device = NULL;
+static struct backlight_ops psbbl_ops = {
+        .get_brightness = read_brightness,
+        .update_status = update_bl_status,
+};
+
+/**
+ * intel_lvds_destroy - unregister and free LVDS structures
+ * @output: output to free
+ *
+ * Unregister the DDC bus for this output then free the driver private
+ * structure.
+ */
+static void intel_lvds_destroy(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+
+	if (psbbl_device){
+		backlight_device_unregister(psbbl_device);
+	}
+	if(dev_OpRegion != NULL)
+		iounmap(dev_OpRegion);
+	intel_i2c_destroy(intel_output->ddc_bus);
+	intel_i2c_destroy(lvds_i2c_bus);
+	kfree(output->driver_private);
+}
+
+static const struct drm_output_funcs intel_lvds_output_funcs = {
+	.dpms = intel_lvds_dpms,
+	.save = intel_lvds_save,
+	.restore = intel_lvds_restore,
+	.mode_valid = intel_lvds_mode_valid,
+	.mode_fixup = intel_lvds_mode_fixup,
+	.prepare = intel_lvds_prepare,
+	.mode_set = intel_lvds_mode_set,
+	.commit = intel_lvds_commit,
+	.detect = intel_lvds_detect,
+	.get_modes = intel_lvds_get_modes,
+	.cleanup = intel_lvds_destroy
+};
+
+int intel_get_acpi_dod(char *method)
+{
+	int status;
+	int found = 0;
+	int i;
+	struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };
+	union acpi_object *dod = NULL;
+	union acpi_object *obj;
+
+	status = acpi_evaluate_object(NULL, method, NULL, &buffer);
+	if (ACPI_FAILURE(status))
+		return -ENODEV;
+
+	dod = buffer.pointer;
+	if (!dod || (dod->type != ACPI_TYPE_PACKAGE)) {
+		status = -EFAULT;
+		goto out;
+	}
+
+	DRM_DEBUG("Found %d video heads in _DOD\n", dod->package.count);
+
+	for (i = 0; i < dod->package.count; i++) {
+		obj = &dod->package.elements[i];
+
+		if (obj->type != ACPI_TYPE_INTEGER) {
+			DRM_DEBUG("Invalid _DOD data\n");
+		} else {
+			DRM_DEBUG("dod element[%d] = 0x%x\n", i,
+				  (int)obj->integer.value);
+
+			/* look for an LVDS type */
+			if (obj->integer.value & 0x00000400)
+				found = 1;
+		}
+	}
+      out:
+	kfree(buffer.pointer);
+	return found;
+}
+/**
+ * intel_lvds_init - setup LVDS outputs on this device
+ * @dev: drm device
+ *
+ * Create the output, register the LVDS DDC bus, and try to figure out what
+ * modes we can display on the LVDS panel (if present).
+ */
+void intel_lvds_init(struct drm_device *dev)
+{
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct drm_output *output;
+	struct intel_output *intel_output;
+	struct drm_display_mode *scan; /* *modes, *bios_mode; */
+	struct drm_crtc *crtc;
+	u32 lvds;
+	int pipe;
+
+	if (!drm_intel_ignore_acpi && !intel_get_acpi_dod(ACPI_DOD))
+		return;
+
+	output = drm_output_create(dev, &intel_lvds_output_funcs, "LVDS");
+	if (!output)
+		return;
+
+	intel_output = kmalloc(sizeof(struct intel_output), GFP_KERNEL);
+	if (!intel_output) {
+		drm_output_destroy(output);
+		return;
+	}
+
+	intel_output->type = INTEL_OUTPUT_LVDS;
+	output->driver_private = intel_output;
+	output->subpixel_order = SubPixelHorizontalRGB;
+	output->interlace_allowed = FALSE;
+	output->doublescan_allowed = FALSE;
+
+	//initialize the I2C bus and BLC data
+	lvds_i2c_bus = intel_i2c_create(dev, GPIOB, "LVDSBLC_B");
+	if (!lvds_i2c_bus) {
+		dev_printk(KERN_ERR, &dev->pdev->dev, "i2c bus registration "
+			   "failed.\n");
+		return;
+	}
+	lvds_i2c_bus->slave_addr = 0x2c;//0x58;
+	lvds_backlight = BRIGHTNESS_MAX_LEVEL;
+	blc_type = 0;
+	blc_pol = 0;
+
+	if (1) { //get the BLC init data from VBT
+		u32 OpRegion_Phys;
+		unsigned int OpRegion_Size = 0x100;
+		OpRegionPtr OpRegion;
+		char *OpRegion_String = "IntelGraphicsMem";
+
+		struct vbt_header *vbt;
+		struct bdb_header *bdb;
+		int vbt_off, bdb_off, bdb_block_off, block_size;
+		int panel_type = -1;
+		unsigned char *bios;
+		unsigned char *vbt_buf;
+
+		pci_read_config_dword(dev->pdev, 0xFC, &OpRegion_Phys);
+
+		//dev_OpRegion =  phys_to_virt(OpRegion_Phys);
+		dev_OpRegion = ioremap(OpRegion_Phys, OpRegion_Size);
+		dev_OpRegionSize = OpRegion_Size;
+
+		OpRegion = (OpRegionPtr) dev_OpRegion;
+
+		if (!memcmp(OpRegion->sign, OpRegion_String, 16)) {
+			unsigned int OpRegion_NewSize;
+
+			OpRegion_NewSize = OpRegion->size * 1024;
+
+			dev_OpRegionSize = OpRegion_NewSize;
+
+			iounmap(dev_OpRegion);
+			dev_OpRegion = ioremap(OpRegion_Phys, OpRegion_NewSize);
+		} else {
+			iounmap(dev_OpRegion);
+			dev_OpRegion = NULL;
+		}
+
+		if((dev_OpRegion != NULL)&&(dev_OpRegionSize >= OFFSET_OPREGION_VBT)) {
+			DRM_INFO("intel_lvds_init: OpRegion has the VBT address\n");
+			vbt_buf = dev_OpRegion + OFFSET_OPREGION_VBT;
+			vbt = (struct vbt_header *)(dev_OpRegion + OFFSET_OPREGION_VBT);
+		} else {
+			DRM_INFO("intel_lvds_init: No OpRegion, use the bios at fixed address 0xc0000\n");
+			bios = phys_to_virt(0xC0000);
+			if(*((u16 *)bios) != 0xAA55){
+				bios = NULL;
+				DRM_ERROR("the bios is incorrect\n");
+				goto blc_out;
+			}
+			vbt_off = bios[0x1a] | (bios[0x1a + 1] << 8);
+			DRM_INFO("intel_lvds_init: the vbt off is %x\n", vbt_off);
+			vbt_buf = bios + vbt_off;
+			vbt = (struct vbt_header *)(bios + vbt_off);
+		}
+
+		bdb_off = vbt->bdb_offset;
+		bdb = (struct bdb_header *)(vbt_buf + bdb_off);
+
+		DRM_INFO("intel_lvds_init: The bdb->signature is %s, the bdb_off is %d\n",bdb->signature, bdb_off);
+
+		if (memcmp(bdb->signature, "BIOS_DATA_BLOCK ", 16) != 0) {
+			DRM_ERROR("the vbt is error\n");
+			goto blc_out;
+		}
+
+		for (bdb_block_off = bdb->header_size; bdb_block_off < bdb->bdb_size;
+				bdb_block_off += block_size) {
+			int start = bdb_off + bdb_block_off;
+			int id, num_entries;
+			struct lvds_bdb_1 *lvds1;
+			struct lvds_blc *lvdsblc;
+			struct lvds_bdb_blc *bdbblc;
+
+			id = vbt_buf[start];
+			block_size = (vbt_buf[start + 1] | (vbt_buf[start + 2] << 8)) + 3;
+			switch (id) {
+				case 40:
+					lvds1 = (struct lvds_bdb_1 *)(vbt_buf+ start);
+					panel_type = lvds1->panel_type;
+					//if (lvds1->caps & LVDS_CAP_DITHER)
+					//	*panelWantsDither = TRUE;
+					break;
+
+				case 43:
+					bdbblc = (struct lvds_bdb_blc *)(vbt_buf + start);
+					num_entries = bdbblc->table_size? (bdbblc->size - \
+							sizeof(bdbblc->table_size))/bdbblc->table_size : 0;
+					if (num_entries << 16 && bdbblc->table_size == sizeof(struct lvds_blc)) {
+						lvdsblc = (struct lvds_blc *)(vbt_buf + start + sizeof(struct lvds_bdb_blc));
+						lvdsblc += panel_type;
+						blc_type = lvdsblc->type;
+						blc_pol = lvdsblc->pol;
+						blc_freq = lvdsblc->freq;
+						blc_minbrightness = lvdsblc->minbrightness;
+						blc_i2caddr = lvdsblc->i2caddr;
+						blc_brightnesscmd = lvdsblc->brightnesscmd;
+						DRM_INFO("intel_lvds_init: BLC Data in BIOS VBT tables: datasize=%d paneltype=%d \
+								type=0x%02x pol=0x%02x freq=0x%04x minlevel=0x%02x    \
+								i2caddr=0x%02x cmd=0x%02x \n",
+								0,
+								panel_type,
+								lvdsblc->type,
+								lvdsblc->pol,
+								lvdsblc->freq,
+								lvdsblc->minbrightness,
+								lvdsblc->i2caddr,
+								lvdsblc->brightnesscmd);
+					}
+					break;
+			}
+		}
+
+	}
+
+	if(1){
+		//get the Core Clock for calculating MAX PWM value
+		//check whether the MaxResEnableInt is
+		struct pci_dev * pci_root = pci_get_bus_and_slot(0, 0);
+		u32 clock;
+		u32 sku_value = 0;
+		unsigned int CoreClocks[] = {
+			100,
+			133,
+			150,
+			178,
+			200,
+			266,
+			266,
+			266
+		};
+		if(pci_root)
+		{
+			pci_write_config_dword(pci_root, 0xD0, 0xD0050300);
+			pci_read_config_dword(pci_root, 0xD4, &clock);
+			CoreClock = CoreClocks[clock & 0x07];
+			DRM_INFO("intel_lvds_init: the CoreClock is %d\n", CoreClock);
+
+			pci_write_config_dword(pci_root, 0xD0, PCI_PORT5_REG80_FFUSE);
+			pci_read_config_dword(pci_root, 0xD4, &sku_value);
+			sku_bMaxResEnableInt = (sku_value & PCI_PORT5_REG80_MAXRES_INT_EN)? true : false;
+			DRM_INFO("intel_lvds_init: sku_value is 0x%08x\n", sku_value);
+			DRM_INFO("intel_lvds_init: sku_bMaxResEnableInt is %d\n", sku_bMaxResEnableInt);
+		}
+	}
+
+	if ((blc_type == BLC_I2C_TYPE) || (blc_type == BLC_PWM_TYPE)){
+		/* add /sys/class/backlight interface as standard */
+		psbbl_device = backlight_device_register("psblvds", &dev->pdev->dev, dev, &psbbl_ops);
+		if (psbbl_device){
+			psbbl_device->props.max_brightness = BRIGHTNESS_MAX_LEVEL;
+			psbbl_device->props.brightness = lvds_backlight;
+			psbbl_device->props.power = FB_BLANK_UNBLANK;
+			backlight_update_status(psbbl_device);
+		}
+	}
+
+blc_out:
+
+	/* Set up the DDC bus. */
+	intel_output->ddc_bus = intel_i2c_create(dev, GPIOC, "LVDSDDC_C");
+	if (!intel_output->ddc_bus) {
+		dev_printk(KERN_ERR, &dev->pdev->dev, "DDC bus registration "
+			   "failed.\n");
+		intel_i2c_destroy(lvds_i2c_bus);
+		return;
+	}
+
+	/*
+	 * Attempt to get the fixed panel mode from DDC.  Assume that the
+	 * preferred mode is the right one.
+	 */
+	intel_lvds_get_modes(output);
+
+	list_for_each_entry(scan, &output->probed_modes, head) {
+		if (scan->type & DRM_MODE_TYPE_PREFERRED) {
+			dev_priv->panel_fixed_mode =
+				drm_mode_duplicate(dev, scan);
+			goto out; /* FIXME: check for quirks */
+		}
+	}
+
+	/*
+	 * If we didn't get EDID, try checking if the panel is already turned
+	 * on.  If so, assume that whatever is currently programmed is the
+	 * correct mode.
+	 */
+	lvds = I915_READ(LVDS);
+	pipe = (lvds & LVDS_PIPEB_SELECT) ? 1 : 0;
+	crtc = intel_get_crtc_from_pipe(dev, pipe);
+
+	if (crtc && (lvds & LVDS_PORT_EN)) {
+		dev_priv->panel_fixed_mode = intel_crtc_mode_get(dev, crtc);
+		if (dev_priv->panel_fixed_mode) {
+			dev_priv->panel_fixed_mode->type |=
+				DRM_MODE_TYPE_PREFERRED;
+			goto out; /* FIXME: check for quirks */
+		}
+	}
+
+	/* If we still don't have a mode after all that, give up. */
+	if (!dev_priv->panel_fixed_mode)
+		goto failed;
+
+	/* FIXME: probe the BIOS for modes and check for LVDS quirks */
+#if 0
+	/* Get the LVDS fixed mode out of the BIOS.  We should support LVDS
+	 * with the BIOS being unavailable or broken, but lack the
+	 * configuration options for now.
+	 */
+	bios_mode = intel_bios_get_panel_mode(pScrn);
+	if (bios_mode != NULL) {
+		if (dev_priv->panel_fixed_mode != NULL) {
+			if (dev_priv->debug_modes &&
+			    !xf86ModesEqual(dev_priv->panel_fixed_mode,
+					    bios_mode))
+			{
+				xf86DrvMsg(pScrn->scrnIndex, X_WARNING,
+					   "BIOS panel mode data doesn't match probed data, "
+					   "continuing with probed.\n");
+				xf86DrvMsg(pScrn->scrnIndex, X_INFO, "BIOS mode:\n");
+				xf86PrintModeline(pScrn->scrnIndex, bios_mode);
+				xf86DrvMsg(pScrn->scrnIndex, X_INFO, "probed mode:\n");
+				xf86PrintModeline(pScrn->scrnIndex, dev_priv->panel_fixed_mode);
+				xfree(bios_mode->name);
+				xfree(bios_mode);
+			}
+		}  else {
+			dev_priv->panel_fixed_mode = bios_mode;
+		}
+	} else {
+		xf86DrvMsg(pScrn->scrnIndex, X_WARNING,
+			   "Couldn't detect panel mode.  Disabling panel\n");
+		goto disable_exit;
+	}
+
+	/*
+	 * Blacklist machines with BIOSes that list an LVDS panel without
+	 * actually having one.
+	 */
+	if (dev_priv->PciInfo->chipType == PCI_CHIP_I945_GM) {
+		/* aopen mini pc */
+		if (dev_priv->PciInfo->subsysVendor == 0xa0a0)
+			goto disable_exit;
+
+		if ((dev_priv->PciInfo->subsysVendor == 0x8086) &&
+		    (dev_priv->PciInfo->subsysCard == 0x7270)) {
+			/* It's a Mac Mini or Macbook Pro.
+			 *
+			 * Apple hardware is out to get us.  The macbook pro
+			 * has a real LVDS panel, but the mac mini does not,
+			 * and they have the same device IDs.  We'll
+			 * distinguish by panel size, on the assumption
+			 * that Apple isn't about to make any machines with an
+			 * 800x600 display.
+			 */
+
+			if (dev_priv->panel_fixed_mode != NULL &&
+			    dev_priv->panel_fixed_mode->HDisplay == 800 &&
+			    dev_priv->panel_fixed_mode->VDisplay == 600)
+			{
+				xf86DrvMsg(pScrn->scrnIndex, X_INFO,
+					   "Suspected Mac Mini, ignoring the LVDS\n");
+				goto disable_exit;
+			}
+		}
+	}
+
+#endif
+
+out:
+	return;
+
+failed:
+        DRM_DEBUG("No LVDS modes found, disabling.\n");
+	drm_output_destroy(output); /* calls intel_lvds_destroy above */
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_lvds.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_lvds.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,174 @@
+/*
+ * Copyright Â© 2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+/**
+ * @file lvds definitions and structures.
+ */
+
+#define BLC_I2C_TYPE 0x01
+#define BLC_PWM_TYPE 0x02
+#define BRIGHTNESS_MASK 0xff
+#define BRIGHTNESS_MAX_LEVEL 100
+#define BLC_POLARITY_NORMAL 0
+#define BLC_POLARITY_INVERSE 1
+#define BACKLIGHT_PWM_POLARITY_BIT_CLEAR (0xfffe)
+#define BACKLIGHT_PWM_CTL_SHIFT (16)
+#define BLC_MAX_PWM_REG_FREQ 0xfffe
+#define BLC_MIN_PWM_REG_FREQ 0x2
+#define BLC_PWM_LEGACY_MODE_ENABLE 0x0001
+#define BLC_PWM_PRECISION_FACTOR 10//10000000
+#define BLC_PWM_FREQ_CALC_CONSTANT 32
+#define MHz 1000000
+#define OFFSET_OPREGION_VBT	0x400
+
+typedef struct OpRegion_Header
+{
+	char sign[16];
+	u32 size;
+	u32 over;
+	char sver[32];
+	char vver[16];
+	char gver[16];
+	u32 mbox;
+	char rhd1[164];
+} OpRegionRec, *OpRegionPtr;
+
+struct vbt_header
+{
+	char signature[20];		/**< Always starts with 'VBT$' */
+	u16 version;			/**< decimal */
+	u16 header_size;		/**< in bytes */
+	u16 vbt_size;			/**< in bytes */
+	u8 vbt_checksum;
+	u8 reserved0;
+	u32 bdb_offset;			/**< from beginning of VBT */
+	u32 aim1_offset;		/**< from beginning of VBT */
+	u32 aim2_offset;		/**< from beginning of VBT */
+	u32 aim3_offset;		/**< from beginning of VBT */
+	u32 aim4_offset;		/**< from beginning of VBT */
+} __attribute__ ((packed));
+
+struct bdb_header
+{
+	char signature[16];		/**< Always 'BIOS_DATA_BLOCK' */
+	u16 version;			/**< decimal */
+	u16 header_size;		/**< in bytes */
+	u16 bdb_size;			/**< in bytes */
+} __attribute__ ((packed));
+
+#define LVDS_CAP_EDID			(1 << 6)
+#define LVDS_CAP_DITHER			(1 << 5)
+#define LVDS_CAP_PFIT_AUTO_RATIO	(1 << 4)
+#define LVDS_CAP_PFIT_GRAPHICS_MODE	(1 << 3)
+#define LVDS_CAP_PFIT_TEXT_MODE		(1 << 2)
+#define LVDS_CAP_PFIT_GRAPHICS		(1 << 1)
+#define LVDS_CAP_PFIT_TEXT		(1 << 0)
+struct lvds_bdb_1
+{
+	u8 id;				/**< 40 */
+	u16 size;
+	u8 panel_type;
+	u8 reserved0;
+	u16 caps;
+} __attribute__ ((packed));
+
+struct lvds_bdb_2_fp_params
+{
+	u16 x_res;
+	u16 y_res;
+	u32 lvds_reg;
+	u32 lvds_reg_val;
+	u32 pp_on_reg;
+	u32 pp_on_reg_val;
+	u32 pp_off_reg;
+	u32 pp_off_reg_val;
+	u32 pp_cycle_reg;
+	u32 pp_cycle_reg_val;
+	u32 pfit_reg;
+	u32 pfit_reg_val;
+	u16 terminator;
+} __attribute__ ((packed));
+
+struct lvds_bdb_2_fp_edid_dtd
+{
+	u16 dclk;		/**< In 10khz */
+	u8 hactive;
+	u8 hblank;
+	u8 high_h;		/**< 7:4 = hactive 11:8, 3:0 = hblank 11:8 */
+	u8 vactive;
+	u8 vblank;
+	u8 high_v;		/**< 7:4 = vactive 11:8, 3:0 = vblank 11:8 */
+	u8 hsync_off;
+	u8 hsync_pulse_width;
+	u8 vsync_off;
+	u8 high_hsync_off;	/**< 7:6 = hsync off 9:8 */
+	u8 h_image;
+	u8 v_image;
+	u8 max_hv;
+	u8 h_border;
+	u8 v_border;
+	u8 flags;
+#define FP_EDID_FLAG_VSYNC_POSITIVE	(1 << 2)
+#define FP_EDID_FLAG_HSYNC_POSITIVE	(1 << 1)
+} __attribute__ ((packed));
+
+struct lvds_bdb_2_entry
+{
+	u16 fp_params_offset;		/**< From beginning of BDB */
+	u8 fp_params_size;
+	u16 fp_edid_dtd_offset;
+	u8 fp_edid_dtd_size;
+	u16 fp_edid_pid_offset;
+	u8 fp_edid_pid_size;
+} __attribute__ ((packed));
+
+struct lvds_bdb_2
+{
+	u8 id;				/**< 41 */
+	u16 size;
+	u8 table_size;		       /* not sure on this one */
+	struct lvds_bdb_2_entry panels[16];
+} __attribute__ ((packed));
+
+
+struct lvds_bdb_blc
+{
+	u8 id;				/**< 43 */
+	u16 size;
+	u8 table_size;
+} __attribute__ ((packed));
+
+struct lvds_blc
+{
+	u8 type:2;
+	u8 pol:1;
+	u8 gpio:3;
+	u8 gmbus:2;
+	u16 freq;
+	u8 minbrightness;
+	u8 i2caddr;
+	u8 brightnesscmd;
+	/* more... */
+} __attribute__ ((packed));
+
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_modes.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_modes.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,60 @@
+/*
+ * Copyright (c) 2007 Dave Airlie <airlied@linux.ie>
+ * Copyright (c) 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include <linux/fb.h>
+
+/**
+ * intel_ddc_probe
+ *
+ */
+bool intel_ddc_probe(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	u8 out_buf[] = { 0x0, 0x0};
+	u8 buf[2];
+	int ret;
+	struct i2c_msg msgs[] = {
+		{
+			.addr = 0x50,
+			.flags = 0,
+			.len = 1,
+			.buf = out_buf,
+		},
+		{
+			.addr = 0x50,
+			.flags = I2C_M_RD,
+			.len = 1,
+			.buf = buf,
+		}
+	};
+
+	ret = i2c_transfer(&intel_output->ddc_bus->adapter, msgs, 2);
+	if (ret == 2)
+		return true;
+
+	return false;
+}
+
+/**
+ * intel_ddc_get_modes - get modelist from monitor
+ * @output: DRM output device to use
+ *
+ * Fetch the EDID information from @output using the DDC bus.
+ */
+int intel_ddc_get_modes(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct edid *edid;
+	int ret = 0;
+
+	edid = drm_get_edid(output, &intel_output->ddc_bus->adapter);
+	if (edid) {
+		ret = drm_add_edid_modes(output, edid);
+		kfree(edid);
+	}
+	return ret;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_sdvo.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_sdvo.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,3973 @@
+/*
+ * Copyright Â© 2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+/*
+ * Copyright 2006 Dave Airlie <airlied@linux.ie>
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include <linux/delay.h>
+#include "drm_crtc.h"
+#include "intel_sdvo_regs.h"
+
+#define MAX_VAL 1000
+#define DPLL_CLOCK_PHASE_9 (1<<9 | 1<<12)
+
+#define PCI_PORT5_REG80_FFUSE				0xD0058000
+#define PCI_PORT5_REG80_SDVO_DISABLE		0x0020
+
+#define SII_1392_WA
+#ifdef SII_1392_WA
+int SII_1392=0;
+extern int drm_psb_no_fb;
+#endif
+
+typedef struct _EXTVDATA
+{
+    u32 Value;
+    u32 Default;
+    u32 Min;
+    u32 Max;
+    u32 Step;		       // arbitrary unit (e.g. pixel, percent) returned during VP_COMMAND_GET
+} EXTVDATA, *PEXTVDATA;
+
+typedef struct _sdvo_display_params
+{
+    EXTVDATA FlickerFilter;	       /* Flicker Filter : for TV onl */
+    EXTVDATA AdaptiveFF;	       /* Adaptive Flicker Filter : for TV onl */
+    EXTVDATA TwoD_FlickerFilter;       /* 2D Flicker Filter : for TV onl */
+    EXTVDATA Brightness;	       /* Brightness : for TV & CRT onl */
+    EXTVDATA Contrast;		       /* Contrast : for TV & CRT onl */
+    EXTVDATA PositionX;		       /* Horizontal Position : for all device */
+    EXTVDATA PositionY;		       /* Vertical Position : for all device */
+    /*EXTVDATA    OverScanX;         Horizontal Overscan : for TV onl */
+    EXTVDATA DotCrawl;		       /* Dot crawl value : for TV onl */
+    EXTVDATA ChromaFilter;	       /* Chroma Filter : for TV onl */
+    /* EXTVDATA    OverScanY;        Vertical Overscan : for TV onl */
+    EXTVDATA LumaFilter;	       /* Luma Filter : for TV only */
+    EXTVDATA Sharpness;		       /* Sharpness : for TV & CRT onl */
+    EXTVDATA Saturation;	       /* Saturation : for TV & CRT onl */
+    EXTVDATA Hue;		       /* Hue : for TV & CRT onl */
+    EXTVDATA Dither;		       /* Dither : For LVDS onl */
+} sdvo_display_params;
+
+typedef enum _SDVO_PICTURE_ASPECT_RATIO_T
+{
+    UAIM_PAR_NO_DATA = 0x00000000,
+    UAIM_PAR_4_3 = 0x00000100,
+    UAIM_PAR_16_9 = 0x00000200,
+    UAIM_PAR_FUTURE = 0x00000300,
+    UAIM_PAR_MASK = 0x00000300,
+} SDVO_PICTURE_ASPECT_RATIO_T;
+
+typedef enum _SDVO_FORMAT_ASPECT_RATIO_T
+{
+    UAIM_FAR_NO_DATA = 0x00000000,
+    UAIM_FAR_SAME_AS_PAR = 0x00002000,
+    UAIM_FAR_4_BY_3_CENTER = 0x00002400,
+    UAIM_FAR_16_BY_9_CENTER = 0x00002800,
+    UAIM_FAR_14_BY_9_CENTER = 0x00002C00,
+    UAIM_FAR_16_BY_9_LETTERBOX_TOP = 0x00000800,
+    UAIM_FAR_14_BY_9_LETTERBOX_TOP = 0x00000C00,
+    UAIM_FAR_GT_16_BY_9_LETTERBOX_CENTER = 0x00002000,
+    UAIM_FAR_4_BY_3_SNP_14_BY_9_CENTER = 0x00003400,	/* With shoot and protect 14:9 cente */
+    UAIM_FAR_16_BY_9_SNP_14_BY_9_CENTER = 0x00003800,	/* With shoot and protect 14:9 cente */
+    UAIM_FAR_16_BY_9_SNP_4_BY_3_CENTER = 0x00003C00,	/* With shoot and protect 4:3 cente */
+    UAIM_FAR_MASK = 0x00003C00,
+} SDVO_FORMAT_ASPECT_RATIO_T;
+
+// TV image aspect ratio
+typedef enum _CP_IMAGE_ASPECT_RATIO
+{
+    CP_ASPECT_RATIO_FF_4_BY_3 = 0,
+    CP_ASPECT_RATIO_14_BY_9_CENTER = 1,
+    CP_ASPECT_RATIO_14_BY_9_TOP = 2,
+    CP_ASPECT_RATIO_16_BY_9_CENTER = 3,
+    CP_ASPECT_RATIO_16_BY_9_TOP = 4,
+    CP_ASPECT_RATIO_GT_16_BY_9_CENTER = 5,
+    CP_ASPECT_RATIO_FF_4_BY_3_PROT_CENTER = 6,
+    CP_ASPECT_RATIO_FF_16_BY_9_ANAMORPHIC = 7,
+} CP_IMAGE_ASPECT_RATIO;
+
+typedef struct _SDVO_ANCILLARY_INFO_T
+{
+    CP_IMAGE_ASPECT_RATIO AspectRatio;
+    u32 RedistCtrlFlag;	       /* Redistribution control flag (get and set */
+} SDVO_ANCILLARY_INFO_T, *PSDVO_ANCILLARY_INFO_T;
+
+struct intel_sdvo_priv {
+	struct intel_i2c_chan *i2c_bus;
+	int slaveaddr;
+	int output_device;
+
+	u16 active_outputs;
+
+	struct intel_sdvo_caps caps;
+	int pixel_clock_min, pixel_clock_max;
+
+	int save_sdvo_mult;
+	u16 save_active_outputs;
+	struct intel_sdvo_dtd save_input_dtd_1, save_input_dtd_2;
+	struct intel_sdvo_dtd save_output_dtd[16];
+	u32 save_SDVOX;
+	/**
+	* SDVO TV encoder support
+	*/
+    u32 ActiveDevice;	       /* CRT, TV, LVDS, TMDS */
+    u32 TVStandard;		       /* PAL, NTSC */
+    int TVOutput;		       /* S-Video, CVBS,YPbPr,RGB */
+    int TVMode;			       /* SDTV/HDTV/SECAM mod */
+    u32 TVStdBitmask;
+    u32 dwSDVOHDTVBitMask;
+    u32 dwSDVOSDTVBitMask;
+    u8 byInputWiring;
+    bool bGetClk;
+    u32 dwMaxDotClk;
+    u32 dwMinDotClk;
+
+    u32 dwMaxInDotClk;
+    u32 dwMinInDotClk;
+
+    u32 dwMaxOutDotClk;
+    u32 dwMinOutDotClk;
+    u32 dwSupportedEnhancements;
+    EXTVDATA OverScanY;		       /* Vertical Overscan : for TV onl */
+    EXTVDATA OverScanX;		       /* Horizontal Overscan : for TV onl */
+    sdvo_display_params dispParams;
+    SDVO_ANCILLARY_INFO_T AncillaryInfo;
+};
+
+/* Define TV mode type */
+/* The full set are defined in xf86str.h*/
+#define M_T_TV 0x80
+
+typedef struct _tv_mode_t
+{
+    /* the following data is detailed mode information as it would be passed to the hardware: */
+    struct drm_display_mode mode_entry;
+    u32 dwSupportedSDTVvss;
+    u32 dwSupportedHDTVvss;
+    bool m_preferred;
+    bool isTVMode;
+} tv_mode_t;
+
+static tv_mode_t tv_modes[] = {
+    {
+     .mode_entry =
+     {DRM_MODE("800x600", DRM_MODE_TYPE_DRIVER | M_T_TV, 0x2625a00 / 1000, 800, 840, 968, 1056, 0,
+      600, 601,
+      604, 628, 0, V_PHSYNC | V_PVSYNC)},
+     .dwSupportedSDTVvss = TVSTANDARD_SDTV_ALL,
+     .dwSupportedHDTVvss = TVSTANDARD_HDTV_ALL,
+     .m_preferred = TRUE,
+     .isTVMode = TRUE,
+     },
+    {
+     .mode_entry =
+     {DRM_MODE("1024x768", DRM_MODE_TYPE_DRIVER | M_T_TV, 0x3dfd240 / 1000, 1024, 0x418, 0x49f, 0x540,
+      0, 768,
+      0x303, 0x308, 0x325, 0, V_PHSYNC | V_PVSYNC)},
+     .dwSupportedSDTVvss = TVSTANDARD_SDTV_ALL,
+     .dwSupportedHDTVvss = TVSTANDARD_HDTV_ALL,
+     .m_preferred = FALSE,
+     .isTVMode = TRUE,
+     },
+    {
+     .mode_entry =
+     {DRM_MODE("720x480", DRM_MODE_TYPE_DRIVER | M_T_TV, 0x1978ff0 / 1000, 720, 0x2e1, 0x326, 0x380, 0,
+      480,
+      0x1f0, 0x1e1, 0x1f1, 0, V_PHSYNC | V_PVSYNC)},
+     .dwSupportedSDTVvss =
+     TVSTANDARD_NTSC_M | TVSTANDARD_NTSC_M_J | TVSTANDARD_NTSC_433,
+     .dwSupportedHDTVvss = 0x0,
+     .m_preferred = FALSE,
+     .isTVMode = TRUE,
+     },
+    {
+     /*Modeline "720x576_SDVO"   0.96 720 756 788 864  576 616 618 700 +vsync  */
+     .mode_entry =
+     {DRM_MODE("720x576", DRM_MODE_TYPE_DRIVER | M_T_TV, 0x1f25a20 / 1000, 720, 756, 788, 864, 0, 576,
+      616,
+      618, 700, 0, V_PHSYNC | V_PVSYNC)},
+     .dwSupportedSDTVvss =
+     (TVSTANDARD_PAL_B | TVSTANDARD_PAL_D | TVSTANDARD_PAL_H |
+      TVSTANDARD_PAL_I | TVSTANDARD_PAL_N | TVSTANDARD_SECAM_B |
+      TVSTANDARD_SECAM_D | TVSTANDARD_SECAM_G | TVSTANDARD_SECAM_H |
+      TVSTANDARD_SECAM_K | TVSTANDARD_SECAM_K1 | TVSTANDARD_SECAM_L |
+      TVSTANDARD_PAL_G | TVSTANDARD_SECAM_L1),
+     .dwSupportedHDTVvss = 0x0,
+     .m_preferred = FALSE,
+     .isTVMode = TRUE,
+     },
+    {
+     .mode_entry =
+     {DRM_MODE("1280x720@60",DRM_MODE_TYPE_DRIVER | M_T_TV, 74250000 / 1000, 1280, 1390, 1430, 1650, 0,
+      720,
+      725, 730, 750, 0, V_PHSYNC | V_PVSYNC)},
+     .dwSupportedSDTVvss = 0x0,
+     .dwSupportedHDTVvss = HDTV_SMPTE_296M_720p60,
+     .m_preferred = FALSE,
+     .isTVMode = TRUE,
+     },
+    {
+     .mode_entry =
+     {DRM_MODE("1280x720@50", DRM_MODE_TYPE_DRIVER | M_T_TV, 74250000 / 1000, 1280, 1720, 1759, 1980, 0,
+      720,
+      725, 730, 750, 0, V_PHSYNC | V_PVSYNC)},
+     .dwSupportedSDTVvss = 0x0,
+     .dwSupportedHDTVvss = HDTV_SMPTE_296M_720p50,
+     .m_preferred = FALSE,
+     .isTVMode = TRUE,
+     },
+    {
+     .mode_entry =
+     {DRM_MODE("1920x1080@60", DRM_MODE_TYPE_DRIVER | M_T_TV, 148500000 / 1000, 1920, 2008, 2051, 2200, 0,
+      1080,
+      1084, 1088, 1124, 0, V_PHSYNC | V_PVSYNC)},
+     .dwSupportedSDTVvss = 0x0,
+     .dwSupportedHDTVvss = HDTV_SMPTE_274M_1080i60,
+     .m_preferred = FALSE,
+     .isTVMode = TRUE,
+     },
+};
+
+#define NUM_TV_MODES sizeof(tv_modes) / sizeof (tv_modes[0])
+
+typedef struct {
+    /* given values */
+    int n;
+    int m1, m2;
+    int p1, p2;
+    /* derived values */
+    int	dot;
+    int	vco;
+    int	m;
+    int	p;
+} ex_intel_clock_t;
+
+
+/**
+ * Writes the SDVOB or SDVOC with the given value, but always writes both
+ * SDVOB and SDVOC to work around apparent hardware issues (according to
+ * comments in the BIOS).
+ */
+static void intel_sdvo_write_sdvox(struct drm_output *output, u32 val)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv   *sdvo_priv = intel_output->dev_priv;
+	u32 bval = val, cval = val;
+	int i;
+
+	if (sdvo_priv->output_device == SDVOB)
+		cval = I915_READ(SDVOC);
+	else
+		bval = I915_READ(SDVOB);
+	/*
+	 * Write the registers twice for luck. Sometimes,
+	 * writing them only once doesn't appear to 'stick'.
+	 * The BIOS does this too. Yay, magic
+	 */
+	for (i = 0; i < 2; i++)
+	{
+		I915_WRITE(SDVOB, bval);
+		I915_READ(SDVOB);
+		I915_WRITE(SDVOC, cval);
+		I915_READ(SDVOC);
+	}
+}
+
+static bool intel_sdvo_read_byte(struct drm_output *output, u8 addr,
+				 u8 *ch)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u8 out_buf[2];
+	u8 buf[2];
+	int ret;
+
+	struct i2c_msg msgs[] = {
+		{
+			.addr = sdvo_priv->i2c_bus->slave_addr,
+			.flags = 0,
+			.len = 1,
+			.buf = out_buf,
+		},
+		{
+			.addr = sdvo_priv->i2c_bus->slave_addr,
+			.flags = I2C_M_RD,
+			.len = 1,
+			.buf = buf,
+		}
+	};
+
+	out_buf[0] = addr;
+	out_buf[1] = 0;
+
+	if ((ret = i2c_transfer(&sdvo_priv->i2c_bus->adapter, msgs, 2)) == 2)
+	{
+//		DRM_DEBUG("got back from addr %02X = %02x\n", out_buf[0], buf[0]);
+		*ch = buf[0];
+		return true;
+	}
+
+	DRM_DEBUG("i2c transfer returned %d\n", ret);
+	return false;
+}
+
+
+#if 0
+static bool intel_sdvo_read_byte_quiet(struct drm_output *output, int addr,
+				       u8 *ch)
+{
+	return true;
+
+}
+#endif
+
+static bool intel_sdvo_write_byte(struct drm_output *output, int addr,
+				  u8 ch)
+{
+	struct intel_output *intel_output = output->driver_private;
+	u8 out_buf[2];
+	struct i2c_msg msgs[] = {
+		{
+			.addr = intel_output->i2c_bus->slave_addr,
+			.flags = 0,
+			.len = 2,
+			.buf = out_buf,
+		}
+	};
+
+	out_buf[0] = addr;
+	out_buf[1] = ch;
+
+	if (i2c_transfer(&intel_output->i2c_bus->adapter, msgs, 1) == 1)
+	{
+		return true;
+	}
+	return false;
+}
+
+#define SDVO_CMD_NAME_ENTRY(cmd) {cmd, #cmd}
+/** Mapping of command numbers to names, for debug output */
+const static struct _sdvo_cmd_name {
+    u8 cmd;
+    char *name;
+} sdvo_cmd_names[] = {
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_RESET),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_DEVICE_CAPS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_FIRMWARE_REV),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_TRAINED_INPUTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_ACTIVE_OUTPUTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_ACTIVE_OUTPUTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_IN_OUT_MAP),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_IN_OUT_MAP),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_ATTACHED_DISPLAYS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_HOT_PLUG_SUPPORT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_ACTIVE_HOT_PLUG),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_ACTIVE_HOT_PLUG),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INTERRUPT_EVENT_SOURCE),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TARGET_INPUT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TARGET_OUTPUT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_INPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_INPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_INPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_OUTPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_OUTPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_OUTPUT_TIMINGS_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_OUTPUT_TIMINGS_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_CREATE_PREFERRED_INPUT_TIMING),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART1),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART2),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_INPUT_PIXEL_CLOCK_RANGE),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_OUTPUT_PIXEL_CLOCK_RANGE),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_SUPPORTED_CLOCK_RATE_MULTS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_CLOCK_RATE_MULT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_CLOCK_RATE_MULT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_SUPPORTED_TV_FORMATS),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_GET_TV_FORMAT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TV_FORMAT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_TV_RESOLUTION_SUPPORT),
+    SDVO_CMD_NAME_ENTRY(SDVO_CMD_SET_CONTROL_BUS_SWITCH),
+};
+
+#define SDVO_NAME(dev_priv) ((dev_priv)->output_device == SDVOB ? "SDVOB" : "SDVOC")
+#define SDVO_PRIV(output)   ((struct intel_sdvo_priv *) (output)->dev_priv)
+
+static void intel_sdvo_write_cmd(struct drm_output *output, u8 cmd,
+				 void *args, int args_len)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	int i;
+
+        if (drm_debug) {
+                DRM_DEBUG("%s: W: %02X ", SDVO_NAME(sdvo_priv), cmd);
+                for (i = 0; i < args_len; i++)
+                        printk("%02X ", ((u8 *)args)[i]);
+                for (; i < 8; i++)
+                        printk("   ");
+                for (i = 0; i < sizeof(sdvo_cmd_names) / sizeof(sdvo_cmd_names[0]); i++) {
+                        if (cmd == sdvo_cmd_names[i].cmd) {
+                                printk("(%s)", sdvo_cmd_names[i].name);
+                                break;
+                        }
+                }
+                if (i == sizeof(sdvo_cmd_names)/ sizeof(sdvo_cmd_names[0]))
+                        printk("(%02X)",cmd);
+                printk("\n");
+        }
+
+	for (i = 0; i < args_len; i++) {
+		intel_sdvo_write_byte(output, SDVO_I2C_ARG_0 - i, ((u8*)args)[i]);
+	}
+
+	intel_sdvo_write_byte(output, SDVO_I2C_OPCODE, cmd);
+}
+
+static const char *cmd_status_names[] = {
+	"Power on",
+	"Success",
+	"Not supported",
+	"Invalid arg",
+	"Pending",
+	"Target not specified",
+	"Scaling not supported"
+};
+
+static u8 intel_sdvo_read_response(struct drm_output *output, void *response,
+				   int response_len)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	int i;
+	u8 status;
+	u8 retry = 50;
+
+	while (retry--) {
+		/* Read the command response */
+		for (i = 0; i < response_len; i++) {
+			intel_sdvo_read_byte(output, SDVO_I2C_RETURN_0 + i,
+				     &((u8 *)response)[i]);
+		}
+
+		/* read the return status */
+		intel_sdvo_read_byte(output, SDVO_I2C_CMD_STATUS, &status);
+
+	        if (drm_debug) {
+			DRM_DEBUG("%s: R: ", SDVO_NAME(sdvo_priv));
+       			for (i = 0; i < response_len; i++)
+                        	printk("%02X ", ((u8 *)response)[i]);
+                	for (; i < 8; i++)
+                        	printk("   ");
+                	if (status <= SDVO_CMD_STATUS_SCALING_NOT_SUPP)
+                        	printk("(%s)", cmd_status_names[status]);
+                	else
+                        	printk("(??? %d)", status);
+                	printk("\n");
+        	}
+
+		if (status != SDVO_CMD_STATUS_PENDING)
+			return status;
+
+		mdelay(50);
+	}
+
+	return status;
+}
+
+int intel_sdvo_get_pixel_multiplier(struct drm_display_mode *mode)
+{
+	if (mode->clock >= 100000)
+		return 1;
+	else if (mode->clock >= 50000)
+		return 2;
+	else
+		return 4;
+}
+
+/**
+ * Don't check status code from this as it switches the bus back to the
+ * SDVO chips which defeats the purpose of doing a bus switch in the first
+ * place.
+ */
+void intel_sdvo_set_control_bus_switch(struct drm_output *output, u8 target)
+{
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_CONTROL_BUS_SWITCH, &target, 1);
+}
+
+static bool intel_sdvo_set_target_input(struct drm_output *output, bool target_0, bool target_1)
+{
+	struct intel_sdvo_set_target_input_args targets = {0};
+	u8 status;
+
+	if (target_0 && target_1)
+		return SDVO_CMD_STATUS_NOTSUPP;
+
+	if (target_1)
+		targets.target_1 = 1;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_TARGET_INPUT, &targets,
+			     sizeof(targets));
+
+	status = intel_sdvo_read_response(output, NULL, 0);
+
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+/**
+ * Return whether each input is trained.
+ *
+ * This function is making an assumption about the layout of the response,
+ * which should be checked against the docs.
+ */
+static bool intel_sdvo_get_trained_inputs(struct drm_output *output, bool *input_1, bool *input_2)
+{
+	struct intel_sdvo_get_trained_inputs_response response;
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_TRAINED_INPUTS, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, sizeof(response));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	*input_1 = response.input0_trained;
+	*input_2 = response.input1_trained;
+	return true;
+}
+
+static bool intel_sdvo_get_active_outputs(struct drm_output *output,
+					  u16 *outputs)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_ACTIVE_OUTPUTS, NULL, 0);
+	status = intel_sdvo_read_response(output, outputs, sizeof(*outputs));
+
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_set_active_outputs(struct drm_output *output,
+					  u16 outputs)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_ACTIVE_OUTPUTS, &outputs,
+			     sizeof(outputs));
+	status = intel_sdvo_read_response(output, NULL, 0);
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_set_encoder_power_state(struct drm_output *output,
+					       int mode)
+{
+	u8 status, state = SDVO_ENCODER_STATE_ON;
+
+	switch (mode) {
+	case DPMSModeOn:
+		state = SDVO_ENCODER_STATE_ON;
+		break;
+	case DPMSModeStandby:
+		state = SDVO_ENCODER_STATE_STANDBY;
+		break;
+	case DPMSModeSuspend:
+		state = SDVO_ENCODER_STATE_SUSPEND;
+		break;
+	case DPMSModeOff:
+		state = SDVO_ENCODER_STATE_OFF;
+		break;
+	}
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_ENCODER_POWER_STATE, &state,
+			     sizeof(state));
+	status = intel_sdvo_read_response(output, NULL, 0);
+
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_get_input_pixel_clock_range(struct drm_output *output,
+						   int *clock_min,
+						   int *clock_max)
+{
+	struct intel_sdvo_pixel_clock_range clocks;
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_INPUT_PIXEL_CLOCK_RANGE,
+			     NULL, 0);
+
+	status = intel_sdvo_read_response(output, &clocks, sizeof(clocks));
+
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	/* Convert the values from units of 10 kHz to kHz. */
+	*clock_min = clocks.min * 10;
+	*clock_max = clocks.max * 10;
+
+	return true;
+}
+
+static bool intel_sdvo_set_target_output(struct drm_output *output,
+					 u16 outputs)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_TARGET_OUTPUT, &outputs,
+			     sizeof(outputs));
+
+	status = intel_sdvo_read_response(output, NULL, 0);
+	return (status == SDVO_CMD_STATUS_SUCCESS);
+}
+
+static bool intel_sdvo_get_timing(struct drm_output *output, u8 cmd,
+				  struct intel_sdvo_dtd *dtd)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, cmd, NULL, 0);
+	status = intel_sdvo_read_response(output, &dtd->part1,
+					  sizeof(dtd->part1));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	intel_sdvo_write_cmd(output, cmd + 1, NULL, 0);
+	status = intel_sdvo_read_response(output, &dtd->part2,
+					  sizeof(dtd->part2));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+static bool intel_sdvo_get_input_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_get_timing(output,
+				     SDVO_CMD_GET_INPUT_TIMINGS_PART1, dtd);
+}
+
+static bool intel_sdvo_get_output_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_get_timing(output,
+				     SDVO_CMD_GET_OUTPUT_TIMINGS_PART1, dtd);
+}
+
+static bool intel_sdvo_set_timing(struct drm_output *output, u8 cmd,
+				  struct intel_sdvo_dtd *dtd)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, cmd, &dtd->part1, sizeof(dtd->part1));
+	status = intel_sdvo_read_response(output, NULL, 0);
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	intel_sdvo_write_cmd(output, cmd + 1, &dtd->part2, sizeof(dtd->part2));
+	status = intel_sdvo_read_response(output, NULL, 0);
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+static bool intel_sdvo_set_input_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_set_timing(output,
+				     SDVO_CMD_SET_INPUT_TIMINGS_PART1, dtd);
+}
+
+static bool intel_sdvo_set_output_timing(struct drm_output *output,
+					 struct intel_sdvo_dtd *dtd)
+{
+	return intel_sdvo_set_timing(output,
+				     SDVO_CMD_SET_OUTPUT_TIMINGS_PART1, dtd);
+}
+
+#if 0
+static bool intel_sdvo_get_preferred_input_timing(struct drm_output *output,
+						  struct intel_sdvo_dtd *dtd)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART1,
+			     NULL, 0);
+
+	status = intel_sdvo_read_response(output, &dtd->part1,
+					  sizeof(dtd->part1));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART2,
+			     NULL, 0);
+	status = intel_sdvo_read_response(output, &dtd->part2,
+					  sizeof(dtd->part2));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+#endif
+
+static int intel_sdvo_get_clock_rate_mult(struct drm_output *output)
+{
+	u8 response, status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_CLOCK_RATE_MULT, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, 1);
+
+	if (status != SDVO_CMD_STATUS_SUCCESS) {
+		DRM_DEBUG("Couldn't get SDVO clock rate multiplier\n");
+		return SDVO_CLOCK_RATE_MULT_1X;
+	} else {
+		DRM_DEBUG("Current clock rate multiplier: %d\n", response);
+	}
+
+	return response;
+}
+
+static bool intel_sdvo_set_clock_rate_mult(struct drm_output *output, u8 val)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_SET_CLOCK_RATE_MULT, &val, 1);
+	status = intel_sdvo_read_response(output, NULL, 0);
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+static bool intel_sdvo_mode_fixup(struct drm_output *output,
+				  struct drm_display_mode *mode,
+				  struct drm_display_mode *adjusted_mode)
+{
+	/* Make the CRTC code factor in the SDVO pixel multiplier.  The SDVO
+	 * device will be told of the multiplier during mode_set.
+	 */
+	DRM_DEBUG("xxintel_sdvo_fixup\n");
+	adjusted_mode->clock *= intel_sdvo_get_pixel_multiplier(mode);
+	return true;
+}
+
+#if 0
+static void i830_sdvo_map_hdtvstd_bitmask(struct drm_output * output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+    switch (sdvo_priv->TVStandard) {
+    case HDTV_SMPTE_274M_1080i50:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_274M_1080i50;
+	break;
+
+    case HDTV_SMPTE_274M_1080i59:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_274M_1080i59;
+	break;
+
+    case HDTV_SMPTE_274M_1080i60:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_274M_1080i60;
+	break;
+    case HDTV_SMPTE_274M_1080p60:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_274M_1080p60;
+	break;
+    case HDTV_SMPTE_296M_720p59:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_296M_720p59;
+	break;
+
+    case HDTV_SMPTE_296M_720p60:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_296M_720p60;
+	break;
+
+    case HDTV_SMPTE_296M_720p50:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_296M_720p50;
+	break;
+
+    case HDTV_SMPTE_293M_480p59:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_293M_480p59;
+	break;
+
+    case HDTV_SMPTE_293M_480p60:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_EIA_7702A_480p60;
+	break;
+
+    case HDTV_SMPTE_170M_480i59:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_170M_480i59;
+	break;
+
+    case HDTV_ITURBT601_576i50:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_ITURBT601_576i50;
+	break;
+
+    case HDTV_ITURBT601_576p50:
+	sdvo_priv->TVStdBitmask = SDVO_HDTV_STD_ITURBT601_576p50;
+	break;
+    default:
+	DRM_DEBUG("ERROR: Unknown TV Standard!!!\n");
+	/*Invalid return 0 */
+	sdvo_priv->TVStdBitmask = 0;
+    }
+
+}
+
+static void i830_sdvo_map_sdtvstd_bitmask(struct drm_output * output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+    switch (sdvo_priv->TVStandard) {
+    case TVSTANDARD_NTSC_M:
+	sdvo_priv->TVStdBitmask = SDVO_NTSC_M;
+	break;
+
+    case TVSTANDARD_NTSC_M_J:
+	sdvo_priv->TVStdBitmask = SDVO_NTSC_M_J;
+	break;
+
+    case TVSTANDARD_NTSC_433:
+	sdvo_priv->TVStdBitmask = SDVO_NTSC_433;
+	break;
+
+    case TVSTANDARD_PAL_B:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_B;
+	break;
+
+    case TVSTANDARD_PAL_D:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_D;
+	break;
+
+    case TVSTANDARD_PAL_G:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_G;
+	break;
+
+    case TVSTANDARD_PAL_H:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_H;
+	break;
+
+    case TVSTANDARD_PAL_I:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_I;
+	break;
+
+    case TVSTANDARD_PAL_M:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_M;
+	break;
+
+    case TVSTANDARD_PAL_N:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_N;
+	break;
+
+    case TVSTANDARD_PAL_60:
+	sdvo_priv->TVStdBitmask = SDVO_PAL_60;
+	break;
+
+    case TVSTANDARD_SECAM_B:
+	sdvo_priv->TVStdBitmask = SDVO_SECAM_B;
+	break;
+
+    case TVSTANDARD_SECAM_D:
+	sdvo_priv->TVStdBitmask = SDVO_SECAM_D;
+	break;
+
+    case TVSTANDARD_SECAM_G:
+	sdvo_priv->TVStdBitmask = SDVO_SECAM_G;
+	break;
+
+    case TVSTANDARD_SECAM_K:
+	sdvo_priv->TVStdBitmask = SDVO_SECAM_K;
+	break;
+
+    case TVSTANDARD_SECAM_K1:
+	sdvo_priv->TVStdBitmask = SDVO_SECAM_K1;
+	break;
+
+    case TVSTANDARD_SECAM_L:
+	sdvo_priv->TVStdBitmask = SDVO_SECAM_L;
+	break;
+
+    case TVSTANDARD_SECAM_L1:
+	DRM_DEBUG("TVSTANDARD_SECAM_L1 not supported by encoder\n");
+	break;
+
+    case TVSTANDARD_SECAM_H:
+	DRM_DEBUG("TVSTANDARD_SECAM_H not supported by encoder\n");
+	break;
+
+    default:
+	DRM_DEBUG("ERROR: Unknown TV Standard\n");
+	/*Invalid return 0 */
+	sdvo_priv->TVStdBitmask = 0;
+	break;
+    }
+}
+#endif
+
+static bool i830_sdvo_set_tvoutputs_formats(struct drm_output * output)
+{
+    u8 byArgs[6];
+    u8 status;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    if (sdvo_priv->TVMode & (TVMODE_SDTV)) {
+	/* Fill up the arguement value */
+	byArgs[0] = (u8) (sdvo_priv->TVStdBitmask & 0xFF);
+	byArgs[1] = (u8) ((sdvo_priv->TVStdBitmask >> 8) & 0xFF);
+	byArgs[2] = (u8) ((sdvo_priv->TVStdBitmask >> 16) & 0xFF);
+    } else {
+	/* Fill up the arguement value */
+	byArgs[0] = 0;
+	byArgs[1] = 0;
+	byArgs[2] = (u8) ((sdvo_priv->TVStdBitmask & 0xFF));
+	byArgs[3] = (u8) ((sdvo_priv->TVStdBitmask >> 8) & 0xFF);
+	byArgs[4] = (u8) ((sdvo_priv->TVStdBitmask >> 16) & 0xFF);
+	byArgs[5] = (u8) ((sdvo_priv->TVStdBitmask >> 24) & 0xFF);
+    }
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_TV_FORMATS, byArgs, 6);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+
+}
+
+static bool i830_sdvo_create_preferred_input_timing(struct drm_output * output,
+					struct drm_display_mode * mode)
+{
+    u8 byArgs[7];
+    u8 status;
+    u32 dwClk;
+    u32 dwHActive, dwVActive;
+    bool bIsInterlaced, bIsScaled;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement values */
+    dwHActive = mode->crtc_hdisplay;
+    dwVActive = mode->crtc_vdisplay;
+
+    dwClk = mode->clock * 1000 / 10000;
+    byArgs[0] = (u8) (dwClk & 0xFF);
+    byArgs[1] = (u8) ((dwClk >> 8) & 0xFF);
+
+    /* HActive & VActive should not exceed 12 bits each. So check it */
+    if ((dwHActive > 0xFFF) || (dwVActive > 0xFFF))
+	return FALSE;
+
+    byArgs[2] = (u8) (dwHActive & 0xFF);
+    byArgs[3] = (u8) ((dwHActive >> 8) & 0xF);
+    byArgs[4] = (u8) (dwVActive & 0xFF);
+    byArgs[5] = (u8) ((dwVActive >> 8) & 0xF);
+
+    bIsInterlaced = 1;
+    bIsScaled = 0;
+
+    byArgs[6] = bIsInterlaced ? 1 : 0;
+    byArgs[6] |= bIsScaled ? 2 : 0;
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_CREATE_PREFERRED_INPUT_TIMINGS,
+			byArgs, 7);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+
+}
+
+static bool i830_sdvo_get_preferred_input_timing(struct drm_output * output,
+				     struct intel_sdvo_dtd *output_dtd)
+{
+    return intel_sdvo_get_timing(output,
+				SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART1,
+				output_dtd);
+}
+
+static bool i830_sdvo_set_current_inoutmap(struct drm_output * output, u32 in0outputmask,
+			       u32 in1outputmask)
+{
+    u8 byArgs[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement values; */
+    byArgs[0] = (u8) (in0outputmask & 0xFF);
+    byArgs[1] = (u8) ((in0outputmask >> 8) & 0xFF);
+    byArgs[2] = (u8) (in1outputmask & 0xFF);
+    byArgs[3] = (u8) ((in1outputmask >> 8) & 0xFF);
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_IN_OUT_MAP, byArgs, 4);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+
+}
+
+void i830_sdvo_set_iomap(struct drm_output * output)
+{
+    u32 dwCurrentSDVOIn0 = 0;
+    u32 dwCurrentSDVOIn1 = 0;
+    u32 dwDevMask = 0;
+
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    /* Please DO NOT change the following code. */
+    /* SDVOB_IN0 or SDVOB_IN1 ==> sdvo_in0 */
+    /* SDVOC_IN0 or SDVOC_IN1 ==> sdvo_in1 */
+    if (sdvo_priv->byInputWiring & (SDVOB_IN0 | SDVOC_IN0)) {
+	switch (sdvo_priv->ActiveDevice) {
+	case SDVO_DEVICE_LVDS:
+	    dwDevMask = SDVO_OUTPUT_LVDS0 | SDVO_OUTPUT_LVDS1;
+	    break;
+
+	case SDVO_DEVICE_TMDS:
+	    dwDevMask = SDVO_OUTPUT_TMDS0 | SDVO_OUTPUT_TMDS1;
+	    break;
+
+	case SDVO_DEVICE_TV:
+	    dwDevMask =
+		SDVO_OUTPUT_YPRPB0 | SDVO_OUTPUT_SVID0 | SDVO_OUTPUT_CVBS0 |
+		SDVO_OUTPUT_YPRPB1 | SDVO_OUTPUT_SVID1 | SDVO_OUTPUT_CVBS1 |
+		SDVO_OUTPUT_SCART0 | SDVO_OUTPUT_SCART1;
+	    break;
+
+	case SDVO_DEVICE_CRT:
+	    dwDevMask = SDVO_OUTPUT_RGB0 | SDVO_OUTPUT_RGB1;
+	    break;
+	}
+	dwCurrentSDVOIn0 = (sdvo_priv->active_outputs & dwDevMask);
+    } else if (sdvo_priv->byInputWiring & (SDVOB_IN1 | SDVOC_IN1)) {
+	switch (sdvo_priv->ActiveDevice) {
+	case SDVO_DEVICE_LVDS:
+	    dwDevMask = SDVO_OUTPUT_LVDS0 | SDVO_OUTPUT_LVDS1;
+	    break;
+
+	case SDVO_DEVICE_TMDS:
+	    dwDevMask = SDVO_OUTPUT_TMDS0 | SDVO_OUTPUT_TMDS1;
+	    break;
+
+	case SDVO_DEVICE_TV:
+	    dwDevMask =
+		SDVO_OUTPUT_YPRPB0 | SDVO_OUTPUT_SVID0 | SDVO_OUTPUT_CVBS0 |
+		SDVO_OUTPUT_YPRPB1 | SDVO_OUTPUT_SVID1 | SDVO_OUTPUT_CVBS1 |
+		SDVO_OUTPUT_SCART0 | SDVO_OUTPUT_SCART1;
+	    break;
+
+	case SDVO_DEVICE_CRT:
+	    dwDevMask = SDVO_OUTPUT_RGB0 | SDVO_OUTPUT_RGB1;
+	    break;
+	}
+	dwCurrentSDVOIn1 = (sdvo_priv->active_outputs & dwDevMask);
+    }
+
+    i830_sdvo_set_current_inoutmap(output, dwCurrentSDVOIn0,
+				   dwCurrentSDVOIn1);
+}
+
+static bool i830_sdvo_get_input_output_pixelclock_range(struct drm_output * output,
+					    bool direction)
+{
+    u8 byRets[4];
+    u8 status;
+
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+    if (direction)		       /* output pixel clock */
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_OUTPUT_PIXEL_CLOCK_RANGE,
+			    NULL, 0);
+    else
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_INPUT_PIXEL_CLOCK_RANGE,
+			    NULL, 0);
+    status = intel_sdvo_read_response(output, byRets, 4);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    if (direction) {
+	/* Fill up the return values. */
+	sdvo_priv->dwMinOutDotClk =
+	    (u32) byRets[0] | ((u32) byRets[1] << 8);
+	sdvo_priv->dwMaxOutDotClk =
+	    (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+	/* Multiply 10000 with the clocks obtained */
+	sdvo_priv->dwMinOutDotClk = (sdvo_priv->dwMinOutDotClk) * 10000;
+	sdvo_priv->dwMaxOutDotClk = (sdvo_priv->dwMaxOutDotClk) * 10000;
+
+    } else {
+	/* Fill up the return values. */
+	sdvo_priv->dwMinInDotClk = (u32) byRets[0] | ((u32) byRets[1] << 8);
+	sdvo_priv->dwMaxInDotClk = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+	/* Multiply 10000 with the clocks obtained */
+	sdvo_priv->dwMinInDotClk = (sdvo_priv->dwMinInDotClk) * 10000;
+	sdvo_priv->dwMaxInDotClk = (sdvo_priv->dwMaxInDotClk) * 10000;
+    }
+    DRM_DEBUG("MinDotClk = 0x%x\n", sdvo_priv->dwMinInDotClk);
+    DRM_DEBUG("MaxDotClk = 0x%x\n", sdvo_priv->dwMaxInDotClk);
+
+    return TRUE;
+
+}
+
+static bool i830_sdvo_get_supported_tvoutput_formats(struct drm_output * output,
+					 u32 * pTVStdMask,
+					 u32 * pHDTVStdMask, u32 *pTVStdFormat)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+    u8 byRets[6];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_SUPPORTED_TV_FORMATS, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 6);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values; */
+    *pTVStdMask = (((u32) byRets[0]) |
+		   ((u32) byRets[1] << 8) |
+		   ((u32) (byRets[2] & 0x7) << 16));
+
+    *pHDTVStdMask = (((u32) byRets[2] & 0xF8) |
+		     ((u32) byRets[3] << 8) |
+		     ((u32) byRets[4] << 16) | ((u32) byRets[5] << 24));
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_TV_FORMATS, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 6);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values; */
+	if(sdvo_priv->TVMode == TVMODE_SDTV)
+    *pTVStdFormat = (((u32) byRets[0]) |
+		   ((u32) byRets[1] << 8) |
+		   ((u32) (byRets[2] & 0x7) << 16));
+    else
+    *pTVStdFormat = (((u32) byRets[2] & 0xF8) |
+		     ((u32) byRets[3] << 8) |
+		     ((u32) byRets[4] << 16) | ((u32) byRets[5] << 24));
+	DRM_DEBUG("BIOS TV format is %d\n",*pTVStdFormat);
+    return TRUE;
+
+}
+
+static bool i830_sdvo_get_supported_enhancements(struct drm_output * output,
+				     u32 * psupported_enhancements)
+{
+
+    u8 status;
+    u8 byRets[2];
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_SUPPORTED_ENHANCEMENTS, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 2);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    sdvo_priv->dwSupportedEnhancements = *psupported_enhancements =
+	((u32) byRets[0] | ((u32) byRets[1] << 8));
+    return TRUE;
+
+}
+
+static bool i830_sdvo_get_max_horizontal_overscan(struct drm_output * output, u32 * pMaxVal,
+				      u32 * pDefaultVal)
+{
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_HORIZONTAL_OVERSCAN, NULL,
+			0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_vertical_overscan(struct drm_output * output, u32 * pMaxVal,
+				    u32 * pDefaultVal)
+{
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_VERTICAL_OVERSCAN, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_horizontal_position(struct drm_output * output, u32 * pMaxVal,
+				      u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_HORIZONTAL_POSITION, NULL,
+			0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_vertical_position(struct drm_output * output,
+				    u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_VERTICAL_POSITION, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_flickerfilter(struct drm_output * output,
+				u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_FLICKER_FILTER, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_brightness(struct drm_output * output,
+			     u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_BRIGHTNESS, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_contrast(struct drm_output * output,
+			   u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_CONTRAST, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_sharpness(struct drm_output * output,
+			    u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_SHARPNESS, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_hue(struct drm_output * output,
+		      u32 * pMaxVal, u32 * pDefaultVal)
+{
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_HUE, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_saturation(struct drm_output * output,
+			     u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_SATURATION, NULL, 0);
+
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_adaptive_flickerfilter(struct drm_output * output,
+					 u32 * pMaxVal,
+					 u32 * pDefaultVal)
+{
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_ADAPTIVE_FLICKER_FILTER,
+			NULL, 0);
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_lumafilter(struct drm_output * output,
+			     u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_TV_LUMA_FILTER, NULL, 0);
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_chromafilter(struct drm_output * output,
+			       u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_TV_CHROMA_FILTER, NULL, 0);
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_get_dotcrawl(struct drm_output * output,
+		       u32 * pCurrentVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_DOT_CRAWL, NULL, 0);
+    status = intel_sdvo_read_response(output, byRets, 2);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Tibet issue 1603772: Dot crawl do not persist after reboot/Hibernate */
+    /* Details : Bit0 is considered as DotCrawl Max value. But according to EDS, Bit0 */
+    /*           represents the Current DotCrawl value. */
+    /* Fix     : The current value is updated with Bit0. */
+
+    /* Fill up the return values. */
+    *pCurrentVal = (u32) (byRets[0] & 0x1);
+    *pDefaultVal = (u32) ((byRets[0] >> 1) & 0x1);
+    return TRUE;
+}
+
+static bool i830_sdvo_get_max_2D_flickerfilter(struct drm_output * output,
+				   u32 * pMaxVal, u32 * pDefaultVal)
+{
+
+    u8 byRets[4];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byRets, 0, sizeof(byRets));
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_GET_MAX_2D_FLICKER_FILTER, NULL, 0);
+    status = intel_sdvo_read_response(output, byRets, 4);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    /* Fill up the return values. */
+    *pMaxVal = (u32) byRets[0] | ((u32) byRets[1] << 8);
+    *pDefaultVal = (u32) byRets[2] | ((u32) byRets[3] << 8);
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_horizontal_overscan(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_HORIZONTAL_OVERSCAN, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+    return TRUE;
+}
+
+static bool i830_sdvo_set_vertical_overscan(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_VERTICAL_OVERSCAN, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+    return TRUE;
+}
+
+static bool i830_sdvo_set_horizontal_position(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_HORIZONTAL_POSITION, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_vertical_position(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_VERTICAL_POSITION, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+
+}
+
+static bool i830_sdvo_set_flickerilter(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_FLICKER_FILTER, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_brightness(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_BRIGHTNESS, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_contrast(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_CONTRAST, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_sharpness(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_SHARPNESS, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_hue(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_HUE, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_saturation(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_SATURATION, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_adaptive_flickerfilter(struct drm_output * output, u32 dwVal)
+{
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_ADAPTIVE_FLICKER_FILTER, byArgs,
+			2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+
+}
+
+static bool i830_sdvo_set_lumafilter(struct drm_output * output, u32 dwVal)
+{
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_TV_LUMA_FILTER, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_chromafilter(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_TV_CHROMA_FILTER, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_dotcrawl(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_DOT_CRAWL, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+static bool i830_sdvo_set_2D_flickerfilter(struct drm_output * output, u32 dwVal)
+{
+
+    u8 byArgs[2];
+    u8 status;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Fill up the arguement value */
+    byArgs[0] = (u8) (dwVal & 0xFF);
+    byArgs[1] = (u8) ((dwVal >> 8) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_2D_FLICKER_FILTER, byArgs, 2);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+}
+
+#if 0
+static bool i830_sdvo_set_ancillary_video_information(struct drm_output * output)
+{
+
+    u8 status;
+    u8 byArgs[4];
+    u32 dwAncillaryBits = 0;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    PSDVO_ANCILLARY_INFO_T pAncillaryInfo = &sdvo_priv->AncillaryInfo;
+
+    /* Make all fields of the  args/ret to zero */
+    memset(byArgs, 0, sizeof(byArgs));
+
+    /* Handle picture aspect ratio (bits 8, 9) and */
+    /* active format aspect ratio (bits 10, 13) */
+    switch (pAncillaryInfo->AspectRatio) {
+    case CP_ASPECT_RATIO_FF_4_BY_3:
+	dwAncillaryBits |= UAIM_PAR_4_3;
+	dwAncillaryBits |= UAIM_FAR_4_BY_3_CENTER;
+	break;
+    case CP_ASPECT_RATIO_14_BY_9_CENTER:
+	dwAncillaryBits |= UAIM_FAR_14_BY_9_CENTER;
+	break;
+    case CP_ASPECT_RATIO_14_BY_9_TOP:
+	dwAncillaryBits |= UAIM_FAR_14_BY_9_LETTERBOX_TOP;
+	break;
+    case CP_ASPECT_RATIO_16_BY_9_CENTER:
+	dwAncillaryBits |= UAIM_PAR_16_9;
+	dwAncillaryBits |= UAIM_FAR_16_BY_9_CENTER;
+	break;
+    case CP_ASPECT_RATIO_16_BY_9_TOP:
+	dwAncillaryBits |= UAIM_PAR_16_9;
+	dwAncillaryBits |= UAIM_FAR_16_BY_9_LETTERBOX_TOP;
+	break;
+    case CP_ASPECT_RATIO_GT_16_BY_9_CENTER:
+	dwAncillaryBits |= UAIM_PAR_16_9;
+	dwAncillaryBits |= UAIM_FAR_GT_16_BY_9_LETTERBOX_CENTER;
+	break;
+    case CP_ASPECT_RATIO_FF_4_BY_3_PROT_CENTER:
+	dwAncillaryBits |= UAIM_FAR_4_BY_3_SNP_14_BY_9_CENTER;
+	break;
+    case CP_ASPECT_RATIO_FF_16_BY_9_ANAMORPHIC:
+	dwAncillaryBits |= UAIM_PAR_16_9;
+	break;
+    default:
+	DRM_DEBUG("fail to set ancillary video info\n");
+	return FALSE;
+
+    }
+
+    /* Fill up the argument value */
+    byArgs[0] = (u8) ((dwAncillaryBits >> 0) & 0xFF);
+    byArgs[1] = (u8) ((dwAncillaryBits >> 8) & 0xFF);
+    byArgs[2] = (u8) ((dwAncillaryBits >> 16) & 0xFF);
+    byArgs[3] = (u8) ((dwAncillaryBits >> 24) & 0xFF);
+
+    /* Send the arguements & SDVO opcode to the h/w */
+
+    intel_sdvo_write_cmd(output, SDVO_CMD_SET_ANCILLARY_VIDEO_INFORMATION,
+			byArgs, 4);
+    status = intel_sdvo_read_response(output, NULL, 0);
+
+    if (status != SDVO_CMD_STATUS_SUCCESS)
+	return FALSE;
+
+    return TRUE;
+
+}
+#endif
+static bool i830_tv_program_display_params(struct drm_output * output)
+
+{
+    u8 status;
+    u32 dwMaxVal = 0;
+    u32 dwDefaultVal = 0;
+    u32 dwCurrentVal = 0;
+
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    /* X & Y Positions */
+
+    /* Horizontal postition */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_HORIZONTAL_POSITION) {
+	status =
+	    i830_sdvo_get_max_horizontal_position(output, &dwMaxVal,
+						  &dwDefaultVal);
+
+	if (status) {
+	    /*Tibet issue 1596943: After changing mode from 8x6 to 10x7 open CUI and press Restore Defaults */
+	    /*Position changes. */
+
+	    /* Tibet:1629992 : can't keep previous TV setting status if re-boot system after TV setting(screen position & size) of CUI */
+	    /* Fix : compare whether current postion is greater than max value and then assign the default value. Earlier the check was */
+	    /*       against the pAim->PositionX.Max value to dwMaxVal. When we boot the PositionX.Max value is 0 and so after every reboot, */
+	    /*       position is set to default. */
+
+	    if (sdvo_priv->dispParams.PositionX.Value > dwMaxVal)
+		sdvo_priv->dispParams.PositionX.Value = dwDefaultVal;
+
+	    status =
+		i830_sdvo_set_horizontal_position(output,
+						  sdvo_priv->dispParams.PositionX.
+						  Value);
+
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.PositionX.Max = dwMaxVal;
+	    sdvo_priv->dispParams.PositionX.Min = 0;
+	    sdvo_priv->dispParams.PositionX.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.PositionX.Step = 1;
+	} else {
+	    return status;
+	}
+    }
+
+    /* Vertical position */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_VERTICAL_POSITION) {
+	status =
+	    i830_sdvo_get_max_vertical_position(output, &dwMaxVal,
+						&dwDefaultVal);
+
+	if (status) {
+
+	    /*Tibet issue 1596943: After changing mode from 8x6 to 10x7 open CUI and press Restore Defaults */
+	    /*Position changes. */
+	    /*currently if we are out of range get back to default */
+
+	    /* Tibet:1629992 : can't keep previous TV setting status if re-boot system after TV setting(screen position & size) of CUI */
+	    /* Fix : compare whether current postion is greater than max value and then assign the default value. Earlier the check was */
+	    /*       against the pAim->PositionY.Max  value to dwMaxVal. When we boot the PositionX.Max value is 0 and so after every reboot, */
+	    /*       position is set to default. */
+
+	    if (sdvo_priv->dispParams.PositionY.Value > dwMaxVal)
+		sdvo_priv->dispParams.PositionY.Value = dwDefaultVal;
+
+	    status =
+		i830_sdvo_set_vertical_position(output,
+						sdvo_priv->dispParams.PositionY.
+						Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.PositionY.Max = dwMaxVal;
+	    sdvo_priv->dispParams.PositionY.Min = 0;
+	    sdvo_priv->dispParams.PositionY.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.PositionY.Step = 1;
+	} else {
+	    return status;
+	}
+    }
+
+    /* Flicker Filter */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_FLICKER_FILTER) {
+	status =
+	    i830_sdvo_get_max_flickerfilter(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*currently if we are out of range get back to default */
+	    if (sdvo_priv->dispParams.FlickerFilter.Value > dwMaxVal)
+		sdvo_priv->dispParams.FlickerFilter.Value = dwDefaultVal;
+
+	    status =
+		i830_sdvo_set_flickerilter(output,
+					   sdvo_priv->dispParams.FlickerFilter.
+					   Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.FlickerFilter.Max = dwMaxVal;
+	    sdvo_priv->dispParams.FlickerFilter.Min = 0;
+	    sdvo_priv->dispParams.FlickerFilter.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.FlickerFilter.Step = 1;
+	} else {
+	    return status;
+	}
+    }
+
+    /* Brightness */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_BRIGHTNESS) {
+
+	status =
+	    i830_sdvo_get_max_brightness(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.Brightness.Value > dwMaxVal)
+		sdvo_priv->dispParams.Brightness.Value = dwDefaultVal;
+
+	    /* Program the device */
+	    status =
+		i830_sdvo_set_brightness(output,
+					 sdvo_priv->dispParams.Brightness.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.Brightness.Max = dwMaxVal;
+	    sdvo_priv->dispParams.Brightness.Min = 0;
+	    sdvo_priv->dispParams.Brightness.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.Brightness.Step = 1;
+	} else {
+	    return status;
+	}
+
+    }
+
+    /* Contrast */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_CONTRAST) {
+
+	status = i830_sdvo_get_max_contrast(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.Contrast.Value > dwMaxVal)
+		sdvo_priv->dispParams.Contrast.Value = dwDefaultVal;
+
+	    /* Program the device */
+	    status =
+		i830_sdvo_set_contrast(output,
+				       sdvo_priv->dispParams.Contrast.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.Contrast.Max = dwMaxVal;
+	    sdvo_priv->dispParams.Contrast.Min = 0;
+	    sdvo_priv->dispParams.Contrast.Default = dwDefaultVal;
+
+	    sdvo_priv->dispParams.Contrast.Step = 1;
+
+	} else {
+	    return status;
+	}
+    }
+
+    /* Sharpness */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_SHARPNESS) {
+
+	status =
+	    i830_sdvo_get_max_sharpness(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.Sharpness.Value > dwMaxVal)
+		sdvo_priv->dispParams.Sharpness.Value = dwDefaultVal;
+
+	    /* Program the device */
+	    status =
+		i830_sdvo_set_sharpness(output,
+					sdvo_priv->dispParams.Sharpness.Value);
+	    if (!status)
+		return status;
+	    sdvo_priv->dispParams.Sharpness.Max = dwMaxVal;
+	    sdvo_priv->dispParams.Sharpness.Min = 0;
+	    sdvo_priv->dispParams.Sharpness.Default = dwDefaultVal;
+
+	    sdvo_priv->dispParams.Sharpness.Step = 1;
+	} else {
+	    return status;
+	}
+    }
+
+    /* Hue */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_HUE) {
+
+	status = i830_sdvo_get_max_hue(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.Hue.Value > dwMaxVal)
+		sdvo_priv->dispParams.Hue.Value = dwDefaultVal;
+
+	    /* Program the device */
+	    status = i830_sdvo_set_hue(output, sdvo_priv->dispParams.Hue.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.Hue.Max = dwMaxVal;
+	    sdvo_priv->dispParams.Hue.Min = 0;
+	    sdvo_priv->dispParams.Hue.Default = dwDefaultVal;
+
+	    sdvo_priv->dispParams.Hue.Step = 1;
+
+	} else {
+	    return status;
+	}
+    }
+
+    /* Saturation */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_SATURATION) {
+	status =
+	    i830_sdvo_get_max_saturation(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.Saturation.Value > dwMaxVal)
+		sdvo_priv->dispParams.Saturation.Value = dwDefaultVal;
+
+	    /* Program the device */
+	    status =
+		i830_sdvo_set_saturation(output,
+					 sdvo_priv->dispParams.Saturation.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.Saturation.Max = dwMaxVal;
+	    sdvo_priv->dispParams.Saturation.Min = 0;
+	    sdvo_priv->dispParams.Saturation.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.Saturation.Step = 1;
+	} else {
+	    return status;
+	}
+
+    }
+
+    /* Adaptive Flicker filter */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_ADAPTIVE_FLICKER_FILTER) {
+	status =
+	    i830_sdvo_get_max_adaptive_flickerfilter(output, &dwMaxVal,
+						     &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.AdaptiveFF.Value > dwMaxVal)
+		sdvo_priv->dispParams.AdaptiveFF.Value = dwDefaultVal;
+
+	    status =
+		i830_sdvo_set_adaptive_flickerfilter(output,
+						     sdvo_priv->dispParams.
+						     AdaptiveFF.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.AdaptiveFF.Max = dwMaxVal;
+	    sdvo_priv->dispParams.AdaptiveFF.Min = 0;
+	    sdvo_priv->dispParams.AdaptiveFF.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.AdaptiveFF.Step = 1;
+	} else {
+	    return status;
+	}
+    }
+
+    /* 2D Flicker filter */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_2D_FLICKER_FILTER) {
+
+	status =
+	    i830_sdvo_get_max_2D_flickerfilter(output, &dwMaxVal,
+					       &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.TwoD_FlickerFilter.Value > dwMaxVal)
+		sdvo_priv->dispParams.TwoD_FlickerFilter.Value = dwDefaultVal;
+
+	    status =
+		i830_sdvo_set_2D_flickerfilter(output,
+					       sdvo_priv->dispParams.
+					       TwoD_FlickerFilter.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.TwoD_FlickerFilter.Max = dwMaxVal;
+	    sdvo_priv->dispParams.TwoD_FlickerFilter.Min = 0;
+	    sdvo_priv->dispParams.TwoD_FlickerFilter.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.TwoD_FlickerFilter.Step = 1;
+	} else {
+	    return status;
+	}
+    }
+
+    /* Luma Filter */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_TV_MAX_LUMA_FILTER) {
+	status =
+	    i830_sdvo_get_max_lumafilter(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.LumaFilter.Value > dwMaxVal)
+		sdvo_priv->dispParams.LumaFilter.Value = dwDefaultVal;
+
+	    /* Program the device */
+	    status =
+		i830_sdvo_set_lumafilter(output,
+					 sdvo_priv->dispParams.LumaFilter.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.LumaFilter.Max = dwMaxVal;
+	    sdvo_priv->dispParams.LumaFilter.Min = 0;
+	    sdvo_priv->dispParams.LumaFilter.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.LumaFilter.Step = 1;
+
+	} else {
+	    return status;
+	}
+
+    }
+
+    /* Chroma Filter */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_MAX_TV_CHROMA_FILTER) {
+
+	status =
+	    i830_sdvo_get_max_chromafilter(output, &dwMaxVal, &dwDefaultVal);
+
+	if (status) {
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+	    if (sdvo_priv->dispParams.ChromaFilter.Value > dwMaxVal)
+		sdvo_priv->dispParams.ChromaFilter.Value = dwDefaultVal;
+
+	    /* Program the device */
+	    status =
+		i830_sdvo_set_chromafilter(output,
+					   sdvo_priv->dispParams.ChromaFilter.
+					   Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.ChromaFilter.Max = dwMaxVal;
+	    sdvo_priv->dispParams.ChromaFilter.Min = 0;
+	    sdvo_priv->dispParams.ChromaFilter.Default = dwDefaultVal;
+	    sdvo_priv->dispParams.ChromaFilter.Step = 1;
+	} else {
+	    return status;
+	}
+
+    }
+
+    /* Dot Crawl */
+    if (sdvo_priv->dwSupportedEnhancements & SDVO_DOT_CRAWL) {
+	status = i830_sdvo_get_dotcrawl(output, &dwCurrentVal, &dwDefaultVal);
+
+	if (status) {
+
+	    dwMaxVal = 1;
+	    /*check whether the value is beyond the max value, min value as per EDS is always 0 so */
+	    /*no need to check it. */
+
+	    /* Tibet issue 1603772: Dot crawl do not persist after reboot/Hibernate */
+	    /* Details : "Dotcrawl.value" is compared with "dwDefaultVal". Since */
+	    /*            dwDefaultVal is always 0, dotCrawl value is always set to 0. */
+	    /* Fix     : Compare the current dotCrawl value with dwMaxValue. */
+
+	    if (sdvo_priv->dispParams.DotCrawl.Value > dwMaxVal)
+
+		sdvo_priv->dispParams.DotCrawl.Value = dwMaxVal;
+
+	    status =
+		i830_sdvo_set_dotcrawl(output,
+				       sdvo_priv->dispParams.DotCrawl.Value);
+	    if (!status)
+		return status;
+
+	    sdvo_priv->dispParams.DotCrawl.Max = dwMaxVal;
+	    sdvo_priv->dispParams.DotCrawl.Min = 0;
+	    sdvo_priv->dispParams.DotCrawl.Default = dwMaxVal;
+	    sdvo_priv->dispParams.DotCrawl.Step = 1;
+	} else {
+	    return status;
+	}
+    }
+
+    return TRUE;
+}
+
+static bool i830_tv_set_overscan_parameters(struct drm_output * output)
+{
+    u8 status;
+
+    u32 dwDefaultVal = 0;
+    u32 dwMaxVal = 0;
+    u32 dwPercentageValue = 0;
+    u32 dwDefOverscanXValue = 0;
+    u32 dwDefOverscanYValue = 0;
+    u32 dwOverscanValue = 0;
+    u32 dwSupportedEnhancements;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    /* Get supported picture enhancements */
+    status =
+	i830_sdvo_get_supported_enhancements(output,
+					     &dwSupportedEnhancements);
+    if (!status)
+	return status;
+
+    /* Horizontal Overscan */
+    if (dwSupportedEnhancements & SDVO_HORIZONTAL_OVERSCAN) {
+	status =
+	    i830_sdvo_get_max_horizontal_overscan(output, &dwMaxVal,
+						  &dwDefaultVal);
+	if (!status)
+	    return status;
+
+	/*Calculate the default value in terms of percentage */
+	dwDefOverscanXValue = ((dwDefaultVal * 100) / dwMaxVal);
+
+	/*Calculate the default value in 0-1000 range */
+	dwDefOverscanXValue = (dwDefOverscanXValue * 10);
+
+	/*Overscan is in the range of 0 to 10000 as per MS spec */
+	if (sdvo_priv->OverScanX.Value > MAX_VAL)
+	    sdvo_priv->OverScanX.Value = dwDefOverscanXValue;
+
+	/*Calculate the percentage(0-100%) of the overscan value */
+	dwPercentageValue = (sdvo_priv->OverScanX.Value * 100) / 1000;
+
+	/* Now map the % value to absolute value to be programed to the encoder */
+	dwOverscanValue = (dwMaxVal * dwPercentageValue) / 100;
+
+	status = i830_sdvo_set_horizontal_overscan(output, dwOverscanValue);
+	if (!status)
+	    return status;
+
+	sdvo_priv->OverScanX.Max = 1000;
+	sdvo_priv->OverScanX.Min = 0;
+	sdvo_priv->OverScanX.Default = dwDefOverscanXValue;
+	sdvo_priv->OverScanX.Step = 20;
+    }
+
+    /* Horizontal Overscan */
+    /* vertical Overscan */
+    if (dwSupportedEnhancements & SDVO_VERTICAL_OVERSCAN) {
+	status =
+	    i830_sdvo_get_max_vertical_overscan(output, &dwMaxVal,
+						&dwDefaultVal);
+	if (!status)
+	    return status;
+
+	/*Calculate the default value in terms of percentage */
+	dwDefOverscanYValue = ((dwDefaultVal * 100) / dwMaxVal);
+
+	/*Calculate the default value in 0-1000 range */
+	dwDefOverscanYValue = (dwDefOverscanYValue * 10);
+
+	/*Overscan is in the range of 0 to 10000 as per MS spec */
+	if (sdvo_priv->OverScanY.Value > MAX_VAL)
+	    sdvo_priv->OverScanY.Value = dwDefOverscanYValue;
+
+	/*Calculate the percentage(0-100%) of the overscan value */
+	dwPercentageValue = (sdvo_priv->OverScanY.Value * 100) / 1000;
+
+	/* Now map the % value to absolute value to be programed to the encoder */
+	dwOverscanValue = (dwMaxVal * dwPercentageValue) / 100;
+
+	status = i830_sdvo_set_vertical_overscan(output, dwOverscanValue);
+	if (!status)
+	    return status;
+
+	sdvo_priv->OverScanY.Max = 1000;
+	sdvo_priv->OverScanY.Min = 0;
+	sdvo_priv->OverScanY.Default = dwDefOverscanYValue;
+	sdvo_priv->OverScanY.Step = 20;
+
+    }
+    /* vertical Overscan */
+    return TRUE;
+}
+
+static bool i830_translate_dtd2timing(struct drm_display_mode * pTimingInfo,
+			  struct intel_sdvo_dtd *pDTD)
+{
+
+    u32 dwHBLHigh = 0;
+    u32 dwVBLHigh = 0;
+    u32 dwHSHigh1 = 0;
+    u32 dwHSHigh2 = 0;
+    u32 dwVSHigh1 = 0;
+    u32 dwVSHigh2 = 0;
+    u32 dwVPWLow = 0;
+    bool status = FALSE;
+
+    if ((pDTD == NULL) || (pTimingInfo == NULL)) {
+	return status;
+    }
+
+    pTimingInfo->clock= pDTD->part1.clock * 10000 / 1000;	/*fix me if i am wrong */
+
+    pTimingInfo->hdisplay = pTimingInfo->crtc_hdisplay =
+	(u32) pDTD->part1.
+	h_active | ((u32) (pDTD->part1.h_high & 0xF0) << 4);
+
+    pTimingInfo->vdisplay = pTimingInfo->crtc_vdisplay =
+	(u32) pDTD->part1.
+	v_active | ((u32) (pDTD->part1.v_high & 0xF0) << 4);
+
+    pTimingInfo->crtc_hblank_start = pTimingInfo->crtc_hdisplay;
+
+    /* Horizontal Total = Horizontal Active + Horizontal Blanking */
+    dwHBLHigh = (u32) (pDTD->part1.h_high & 0x0F);
+    pTimingInfo->htotal = pTimingInfo->crtc_htotal =
+	pTimingInfo->crtc_hdisplay + (u32) pDTD->part1.h_blank +
+	(dwHBLHigh << 8);
+
+    pTimingInfo->crtc_hblank_end = pTimingInfo->crtc_htotal - 1;
+
+    /* Vertical Total = Vertical Active + Vertical Blanking */
+    dwVBLHigh = (u32) (pDTD->part1.v_high & 0x0F);
+    pTimingInfo->vtotal = pTimingInfo->crtc_vtotal =
+	pTimingInfo->crtc_vdisplay + (u32) pDTD->part1.v_blank +
+	(dwVBLHigh << 8);
+    pTimingInfo->crtc_vblank_start = pTimingInfo->crtc_vdisplay;
+    pTimingInfo->crtc_vblank_end = pTimingInfo->crtc_vtotal - 1;
+
+    /* Horz Sync Start = Horz Blank Start + Horz Sync Offset */
+    dwHSHigh1 = (u32) (pDTD->part2.sync_off_width_high & 0xC0);
+    pTimingInfo->hsync_start = pTimingInfo->crtc_hsync_start =
+	pTimingInfo->crtc_hblank_start + (u32) pDTD->part2.h_sync_off +
+	(dwHSHigh1 << 2);
+
+    /* Horz Sync End = Horz Sync Start + Horz Sync Pulse Width */
+    dwHSHigh2 = (u32) (pDTD->part2.sync_off_width_high & 0x30);
+    pTimingInfo->hsync_end = pTimingInfo->crtc_hsync_end =
+	pTimingInfo->crtc_hsync_start + (u32) pDTD->part2.h_sync_width +
+	(dwHSHigh2 << 4) - 1;
+
+    /* Vert Sync Start = Vert Blank Start + Vert Sync Offset */
+    dwVSHigh1 = (u32) (pDTD->part2.sync_off_width_high & 0x0C);
+    dwVPWLow = (u32) (pDTD->part2.v_sync_off_width & 0xF0);
+
+    pTimingInfo->vsync_start = pTimingInfo->crtc_vsync_start =
+	pTimingInfo->crtc_vblank_start + (dwVPWLow >> 4) + (dwVSHigh1 << 2);
+
+    /* Vert Sync End = Vert Sync Start + Vert Sync Pulse Width */
+    dwVSHigh2 = (u32) (pDTD->part2.sync_off_width_high & 0x03);
+    pTimingInfo->vsync_end = pTimingInfo->crtc_vsync_end =
+	pTimingInfo->crtc_vsync_start +
+	(u32) (pDTD->part2.v_sync_off_width & 0x0F) + (dwVSHigh2 << 4) - 1;
+
+    /* Fillup flags */
+    status = TRUE;
+
+    return status;
+}
+
+static void i830_translate_timing2dtd(struct drm_display_mode * mode, struct intel_sdvo_dtd *dtd)
+{
+    u16 width, height;
+    u16 h_blank_len, h_sync_len, v_blank_len, v_sync_len;
+    u16 h_sync_offset, v_sync_offset;
+
+    width = mode->crtc_hdisplay;
+    height = mode->crtc_vdisplay;
+
+    /* do some mode translations */
+    h_blank_len = mode->crtc_hblank_end - mode->crtc_hblank_start;
+    h_sync_len = mode->crtc_hsync_end - mode->crtc_hsync_start;
+
+    v_blank_len = mode->crtc_vblank_end - mode->crtc_vblank_start;
+    v_sync_len = mode->crtc_vsync_end - mode->crtc_vsync_start;
+
+    h_sync_offset = mode->crtc_hsync_start - mode->crtc_hblank_start;
+    v_sync_offset = mode->crtc_vsync_start - mode->crtc_vblank_start;
+
+    dtd->part1.clock = mode->clock * 1000 / 10000;	/*xiaolin, fixme, do i need to by 1k hz */
+    dtd->part1.h_active = width & 0xff;
+    dtd->part1.h_blank = h_blank_len & 0xff;
+    dtd->part1.h_high = (((width >> 8) & 0xf) << 4) |
+	((h_blank_len >> 8) & 0xf);
+    dtd->part1.v_active = height & 0xff;
+    dtd->part1.v_blank = v_blank_len & 0xff;
+    dtd->part1.v_high = (((height >> 8) & 0xf) << 4) |
+	((v_blank_len >> 8) & 0xf);
+
+    dtd->part2.h_sync_off = h_sync_offset;
+    dtd->part2.h_sync_width = h_sync_len & 0xff;
+    dtd->part2.v_sync_off_width = ((v_sync_offset & 0xf) << 4 |
+				   (v_sync_len & 0xf)) + 1;
+    dtd->part2.sync_off_width_high = ((h_sync_offset & 0x300) >> 2) |
+	((h_sync_len & 0x300) >> 4) | ((v_sync_offset & 0x30) >> 2) |
+	((v_sync_len & 0x30) >> 4);
+
+    dtd->part2.dtd_flags = 0x18;
+    if (mode->flags & V_PHSYNC)
+	dtd->part2.dtd_flags |= 0x2;
+    if (mode->flags & V_PVSYNC)
+	dtd->part2.dtd_flags |= 0x4;
+
+    dtd->part2.sdvo_flags = 0;
+    dtd->part2.v_sync_off_high = v_sync_offset & 0xc0;
+    dtd->part2.reserved = 0;
+
+}
+
+static bool i830_tv_set_target_io(struct drm_output* output)
+{
+    bool status;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+    status = intel_sdvo_set_target_input(output, TRUE, FALSE);
+    if (status)
+	status = intel_sdvo_set_target_output(output, sdvo_priv->active_outputs);
+
+    return status;
+}
+
+static bool i830_tv_get_max_min_dotclock(struct drm_output* output)
+{
+    u32 dwMaxClkRateMul = 1;
+    u32 dwMinClkRateMul = 1;
+    u8 status;
+
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    /* Set Target Input/Outputs */
+    status = i830_tv_set_target_io(output);
+    if (!status) {
+	DRM_DEBUG("SetTargetIO function FAILED!!! \n");
+	return status;
+    }
+
+    /* Get the clock rate multiplies supported by the encoder */
+    dwMinClkRateMul = 1;
+#if 0
+    /* why we need do this, some time, tv can't bring up for the wrong setting in the last time */
+    dwClkRateMulMask = i830_sdvo_get_clock_rate_mult(output);
+
+    /* Find the minimum clock rate multiplier supported */
+
+    if (dwClkRateMulMask & SDVO_CLOCK_RATE_MULT_1X)
+	dwMinClkRateMul = 1;
+    else if (dwClkRateMulMask & SDVO_CLOCK_RATE_MULT_2X)
+	dwMinClkRateMul = 2;
+    else if (dwClkRateMulMask & SDVO_CLOCK_RATE_MULT_3X)
+	dwMinClkRateMul = 3;
+    else if (dwClkRateMulMask & SDVO_CLOCK_RATE_MULT_4X)
+	dwMinClkRateMul = 4;
+    else if (dwClkRateMulMask & SDVO_CLOCK_RATE_MULT_5X)
+	dwMinClkRateMul = 5;
+    else
+	return FALSE;
+#endif
+    /* Get the min and max input Dot Clock supported by the encoder */
+    status = i830_sdvo_get_input_output_pixelclock_range(output, FALSE);	/* input */
+
+    if (!status) {
+	DRM_DEBUG("SDVOGetInputPixelClockRange() FAILED!!! \n");
+	return status;
+    }
+
+    /* Get the min and max output Dot Clock supported by the encoder */
+    status = i830_sdvo_get_input_output_pixelclock_range(output, TRUE);	/* output */
+
+    if (!status) {
+	DRM_DEBUG("SDVOGetOutputPixelClockRange() FAILED!!! \n");
+	return status;
+    }
+
+    /* Maximum Dot Clock supported should be the minimum of the maximum */
+    /* dot clock supported by the encoder & the SDVO bus clock rate */
+    sdvo_priv->dwMaxDotClk =
+	((sdvo_priv->dwMaxInDotClk * dwMaxClkRateMul) <
+	 (sdvo_priv->dwMaxOutDotClk)) ? (sdvo_priv->dwMaxInDotClk *
+				     dwMaxClkRateMul) : (sdvo_priv->dwMaxOutDotClk);
+
+    /* Minimum Dot Clock supported should be the maximum of the minimum */
+    /* dot clocks supported by the input & output */
+    sdvo_priv->dwMinDotClk =
+	((sdvo_priv->dwMinInDotClk * dwMinClkRateMul) >
+	 (sdvo_priv->dwMinOutDotClk)) ? (sdvo_priv->dwMinInDotClk *
+				     dwMinClkRateMul) : (sdvo_priv->dwMinOutDotClk);
+
+    DRM_DEBUG("leave, i830_tv_get_max_min_dotclock() !!! \n");
+
+    return TRUE;
+
+}
+
+bool i830_tv_mode_check_support(struct drm_output* output, struct drm_display_mode* pMode)
+{
+    u32 dwDotClk = 0;
+    bool status;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    dwDotClk = pMode->clock * 1000;
+
+    /*TODO:  Need to fix this from SoftBios side........ */
+    if (sdvo_priv->TVMode == TVMODE_HDTV) {
+	if (((pMode->hdisplay == 1920) && (pMode->vdisplay== 1080)) ||
+	    ((pMode->hdisplay== 1864) && (pMode->vdisplay== 1050)) ||
+	    ((pMode->hdisplay== 1704) && (pMode->vdisplay== 960)) ||
+	    ((pMode->hdisplay== 640) && (pMode->vdisplay== 448)))
+	    return true;
+    }
+
+    if (sdvo_priv->bGetClk) {
+	status = i830_tv_get_max_min_dotclock(output);
+	if (!status) {
+	    DRM_DEBUG("get max min dotclok failed\n");
+	    return status;
+	}
+	sdvo_priv->bGetClk = false;
+    }
+
+    /* Check the Dot clock first. If the requested Dot Clock should fall */
+    /* in the supported range for the mode to be supported */
+    if ((dwDotClk <= sdvo_priv->dwMinDotClk) || (dwDotClk >= sdvo_priv->dwMaxDotClk)) {
+	DRM_DEBUG("dwDotClk value is out of range\n");
+	/*TODO: now consider VBT add and Remove mode. */
+	/* This mode can't be supported */
+	return false;
+    }
+    DRM_DEBUG("i830_tv_mode_check_support leave\n");
+    return true;
+
+}
+
+void print_Pll(char *prefix, ex_intel_clock_t * clock)
+{
+    DRM_DEBUG("%s: dotclock %d vco %d ((m %d, m1 %d, m2 %d), n %d, (p %d, p1 %d, p2 %d))\n",
+	      prefix, clock->dot, clock->vco, clock->m, clock->m1, clock->m2,
+	      clock->n, clock->p, clock->p1, clock->p2);
+}
+
+extern int intel_panel_fitter_pipe (struct drm_device *dev);
+extern int intel_get_core_clock_speed(struct drm_device *dev);
+
+void i830_sdvo_tv_settiming(struct drm_crtc *crtc, struct drm_display_mode * mode,
+		       struct drm_display_mode * adjusted_mode)
+{
+
+	struct drm_device *dev = crtc->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+
+    int pipe = 0;
+    int fp_reg = (pipe == 0) ? FPA0 : FPB0;
+    int dpll_reg = (pipe == 0) ? DPLL_A : DPLL_B;
+    int dspcntr_reg = (pipe == 0) ? DSPACNTR : DSPBCNTR;
+    int pipeconf_reg = (pipe == 0) ? PIPEACONF : PIPEBCONF;
+    int htot_reg = (pipe == 0) ? HTOTAL_A : HTOTAL_B;
+    int hblank_reg = (pipe == 0) ? HBLANK_A : HBLANK_B;
+    int hsync_reg = (pipe == 0) ? HSYNC_A : HSYNC_B;
+    int vtot_reg = (pipe == 0) ? VTOTAL_A : VTOTAL_B;
+    int vblank_reg = (pipe == 0) ? VBLANK_A : VBLANK_B;
+    int vsync_reg = (pipe == 0) ? VSYNC_A : VSYNC_B;
+    int dspsize_reg = (pipe == 0) ? DSPASIZE : DSPBSIZE;
+	int dspstride_reg = (pipe == 0) ? DSPASTRIDE : DSPBSTRIDE;
+    int dsppos_reg = (pipe == 0) ? DSPAPOS : DSPBPOS;
+    int pipesrc_reg = (pipe == 0) ? PIPEASRC : PIPEBSRC;
+    ex_intel_clock_t clock;
+    u32 dpll = 0, fp = 0, dspcntr, pipeconf;
+    bool ok, is_sdvo = FALSE;
+    int centerX = 0, centerY = 0;
+    u32 ulPortMultiplier, ulTemp, ulDotClock;
+    int sdvo_pixel_multiply;
+	u32 dotclock;
+
+    /* Set up some convenient bools for what outputs are connected to
+     * our pipe, used in DPLL setup.
+     */
+	if (!crtc->fb) {
+		DRM_ERROR("Can't set mode without attached fb\n");
+		return;
+	}
+    is_sdvo = TRUE;
+    ok = TRUE;
+    ulDotClock = mode->clock * 1000 / 1000;	/*xiaolin, fixme, do i need to by 1k hz */
+    for (ulPortMultiplier = 1; ulPortMultiplier <= 5; ulPortMultiplier++) {
+	ulTemp = ulDotClock * ulPortMultiplier;
+	if ((ulTemp >= 100000) && (ulTemp <= 200000)) {
+	    if ((ulPortMultiplier == 3) || (ulPortMultiplier == 5))
+		continue;
+	    else
+		break;
+	}
+    }
+    /* ulPortMultiplier is 2, dotclok is 1babc, fall into the first one case */
+    /* add two to each m and n value -- optimizes (slightly) the search algo. */
+    dotclock = ulPortMultiplier * (mode->clock * 1000) / 1000;
+	DRM_DEBUG("mode->clock is %x, dotclock is %x,!\n", mode->clock,dotclock);
+
+    if ((dotclock >= 100000) && (dotclock < 140500)) {
+	DRM_DEBUG("dotclock is between 10000 and 140500!\n");
+	clock.p1 = 0x2;
+	clock.p2 = 0x00;
+	clock.n = 0x3;
+	clock.m1 = 0x10;
+	clock.m2 = 0x8;
+    } else if ((dotclock >= 140500) && (dotclock <= 200000)) {
+
+	DRM_DEBUG("dotclock is between 140500 and 200000!\n");
+	clock.p1 = 0x1;
+	/*CG was using 0x10 from spreadsheet it should be 0 */
+	/*pClock_Data->Clk_P2 = 0x10; */
+	clock.p2 = 0x00;
+	clock.n = 0x6;
+	clock.m1 = 0xC;
+	clock.m2 = 0x8;
+    } else
+	ok = FALSE;
+
+    if (!ok)
+	DRM_DEBUG("Couldn't find PLL settings for mode!\n");
+
+    fp = clock.n << 16 | clock.m1 << 8 | clock.m2;
+
+    dpll = DPLL_VGA_MODE_DIS | DPLL_CLOCK_PHASE_9;
+
+    dpll |= DPLLB_MODE_DAC_SERIAL;
+
+    sdvo_pixel_multiply = ulPortMultiplier;
+    dpll |= DPLL_DVO_HIGH_SPEED;
+    dpll |= (sdvo_pixel_multiply - 1) << SDVO_MULTIPLIER_SHIFT_HIRES;
+
+    /* compute bitmask from p1 value */
+    dpll |= (clock.p1 << 16);
+    dpll |= (clock.p2 << 24);
+
+    dpll |= PLL_REF_INPUT_TVCLKINBC;
+
+    /* Set up the display plane register */
+    dspcntr = DISPPLANE_GAMMA_ENABLE;
+    switch (crtc->fb->bits_per_pixel) {
+    case 8:
+	dspcntr |= DISPPLANE_8BPP;
+	break;
+    case 16:
+	if (crtc->fb->depth == 15)
+	    dspcntr |= DISPPLANE_15_16BPP;
+	else
+	    dspcntr |= DISPPLANE_16BPP;
+	break;
+    case 32:
+	dspcntr |= DISPPLANE_32BPP_NO_ALPHA;
+	break;
+    default:
+	DRM_DEBUG("unknown display bpp\n");
+    }
+
+    if (pipe == 0)
+	dspcntr |= DISPPLANE_SEL_PIPE_A;
+    else
+	dspcntr |= DISPPLANE_SEL_PIPE_B;
+
+    pipeconf = I915_READ(pipeconf_reg);
+    if (pipe == 0) {
+	/* Enable pixel doubling when the dot clock is > 90% of the (display)
+	 * core speed.
+	 *
+	 * XXX: No double-wide on 915GM pipe B. Is that the only reason for the
+	 * pipe == 0 check?
+	 */
+	if (mode->clock * 1000 > (intel_get_core_clock_speed(dev)) * 9 / 10)	/*xiaolin, fixme, do i need to by 1k hz */
+	   { pipeconf |= PIPEACONF_DOUBLE_WIDE; DRM_DEBUG("PIPEACONF_DOUBLE_WIDE\n");}
+	else
+	   { pipeconf &= ~PIPEACONF_DOUBLE_WIDE; DRM_DEBUG("non PIPEACONF_DOUBLE_WIDE\n");}
+    }
+
+	dspcntr |= DISPLAY_PLANE_ENABLE;
+    pipeconf |= PIPEACONF_ENABLE;
+    dpll |= DPLL_VCO_ENABLE;
+
+    /* Disable the panel fitter if it was on our pipe */
+    if (intel_panel_fitter_pipe(dev) == pipe)
+	I915_WRITE(PFIT_CONTROL, 0);
+
+    print_Pll("chosen", &clock);
+	DRM_DEBUG("Mode for pipe %c:\n", pipe == 0 ? 'A' : 'B');
+	drm_mode_debug_printmodeline(dev, mode);
+	DRM_DEBUG("Modeline %d:\"%s\" %d %d %d %d %d %d %d %d\n",
+		  mode->mode_id, mode->name, mode->crtc_htotal, mode->crtc_hdisplay,
+		  mode->crtc_hblank_end, mode->crtc_hblank_start,
+		  mode->crtc_vtotal, mode->crtc_vdisplay,
+		  mode->crtc_vblank_end, mode->crtc_vblank_start);
+    DRM_DEBUG("clock regs: 0x%08x, 0x%08x,dspntr is 0x%8x, pipeconf is 0x%8x\n", (int)dpll,
+	      (int)fp,(int)dspcntr,(int)pipeconf);
+
+    if (dpll & DPLL_VCO_ENABLE) {
+	I915_WRITE(fp_reg, fp);
+	I915_WRITE(dpll_reg, dpll & ~DPLL_VCO_ENABLE);
+	(void)I915_READ(dpll_reg);
+	udelay(150);
+    }
+    I915_WRITE(fp_reg, fp);
+    I915_WRITE(dpll_reg, dpll);
+    (void)I915_READ(dpll_reg);
+    /* Wait for the clocks to stabilize. */
+    udelay(150);
+
+	/* write it again -- the BIOS does, after all */
+	I915_WRITE(dpll_reg, dpll);
+	I915_READ(dpll_reg);
+	/* Wait for the clocks to stabilize. */
+	udelay(150);
+
+    I915_WRITE(htot_reg, (mode->crtc_hdisplay - 1) |
+		((mode->crtc_htotal - 1) << 16));
+    I915_WRITE(hblank_reg, (mode->crtc_hblank_start - 1) |
+		((mode->crtc_hblank_end - 1) << 16));
+    I915_WRITE(hsync_reg, (mode->crtc_hsync_start - 1) |
+		((mode->crtc_hsync_end - 1) << 16));
+    I915_WRITE(vtot_reg, (mode->crtc_vdisplay - 1) |
+		((mode->crtc_vtotal - 1) << 16));
+    I915_WRITE(vblank_reg, (mode->crtc_vblank_start - 1) |
+		((mode->crtc_vblank_end - 1) << 16));
+    I915_WRITE(vsync_reg, (mode->crtc_vsync_start - 1) |
+		((mode->crtc_vsync_end - 1) << 16));
+	I915_WRITE(dspstride_reg, crtc->fb->pitch);
+
+    if (0) {
+
+	centerX = (adjusted_mode->crtc_hdisplay - mode->hdisplay) / 2;
+	centerY = (adjusted_mode->crtc_vdisplay - mode->vdisplay) / 2;
+	I915_WRITE(dspsize_reg,
+		    ((mode->vdisplay - 1) << 16) | (mode->hdisplay - 1));
+
+	I915_WRITE(dsppos_reg, centerY << 16 | centerX);
+	I915_WRITE(pipesrc_reg,
+		    ((adjusted_mode->crtc_hdisplay -
+		      1) << 16) | (adjusted_mode->crtc_vdisplay - 1));
+    } else {
+	/* pipesrc and dspsize control the size that is scaled from, which should
+	 * always be the user's requested size.
+	 */
+	I915_WRITE(dspsize_reg,
+		    ((mode->vdisplay - 1) << 16) | (mode->hdisplay - 1));
+	I915_WRITE(dsppos_reg, 0);
+	I915_WRITE(pipesrc_reg,
+		    ((mode->hdisplay - 1) << 16) | (mode->vdisplay - 1));
+
+    }
+    I915_WRITE(pipeconf_reg, pipeconf);
+    I915_READ(pipeconf_reg);
+
+    intel_wait_for_vblank(dev);
+
+    I915_WRITE(dspcntr_reg, dspcntr);
+	/* Flush the plane changes */
+	//intel_pipe_set_base(crtc, 0, 0);
+	/* Disable the VGA plane that we never use */
+	//I915_WRITE(VGACNTRL, VGA_DISP_DISABLE);
+    //intel_wait_for_vblank(dev);
+
+}
+
+static void intel_sdvo_mode_set(struct drm_output *output,
+				struct drm_display_mode *mode,
+				struct drm_display_mode *adjusted_mode)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc = output->crtc;
+	struct intel_crtc *intel_crtc = crtc->driver_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+	u32 sdvox;
+	struct intel_sdvo_dtd output_dtd;
+	int sdvo_pixel_multiply;
+	bool success;
+	struct drm_display_mode * save_mode;
+	DRM_DEBUG("xxintel_sdvo_mode_set\n");
+
+	if (!mode)
+		return;
+
+    if (sdvo_priv->ActiveDevice == SDVO_DEVICE_TV) {
+	if (!i830_tv_mode_check_support(output, mode)) {
+	    DRM_DEBUG("mode setting failed, use the forced mode\n");
+	    mode = &tv_modes[0].mode_entry;
+		drm_mode_set_crtcinfo(mode, 0);
+	}
+    }
+    save_mode = mode;
+#if 0
+	width = mode->crtc_hdisplay;
+	height = mode->crtc_vdisplay;
+
+	/* do some mode translations */
+	h_blank_len = mode->crtc_hblank_end - mode->crtc_hblank_start;
+	h_sync_len = mode->crtc_hsync_end - mode->crtc_hsync_start;
+
+	v_blank_len = mode->crtc_vblank_end - mode->crtc_vblank_start;
+	v_sync_len = mode->crtc_vsync_end - mode->crtc_vsync_start;
+
+	h_sync_offset = mode->crtc_hsync_start - mode->crtc_hblank_start;
+	v_sync_offset = mode->crtc_vsync_start - mode->crtc_vblank_start;
+
+	output_dtd.part1.clock = mode->clock / 10;
+	output_dtd.part1.h_active = width & 0xff;
+	output_dtd.part1.h_blank = h_blank_len & 0xff;
+	output_dtd.part1.h_high = (((width >> 8) & 0xf) << 4) |
+		((h_blank_len >> 8) & 0xf);
+	output_dtd.part1.v_active = height & 0xff;
+	output_dtd.part1.v_blank = v_blank_len & 0xff;
+	output_dtd.part1.v_high = (((height >> 8) & 0xf) << 4) |
+		((v_blank_len >> 8) & 0xf);
+
+	output_dtd.part2.h_sync_off = h_sync_offset;
+	output_dtd.part2.h_sync_width = h_sync_len & 0xff;
+	output_dtd.part2.v_sync_off_width = (v_sync_offset & 0xf) << 4 |
+		(v_sync_len & 0xf);
+	output_dtd.part2.sync_off_width_high = ((h_sync_offset & 0x300) >> 2) |
+		((h_sync_len & 0x300) >> 4) | ((v_sync_offset & 0x30) >> 2) |
+		((v_sync_len & 0x30) >> 4);
+
+	output_dtd.part2.dtd_flags = 0x18;
+	if (mode->flags & V_PHSYNC)
+		output_dtd.part2.dtd_flags |= 0x2;
+	if (mode->flags & V_PVSYNC)
+		output_dtd.part2.dtd_flags |= 0x4;
+
+	output_dtd.part2.sdvo_flags = 0;
+	output_dtd.part2.v_sync_off_high = v_sync_offset & 0xc0;
+	output_dtd.part2.reserved = 0;
+#else
+    /* disable and enable the display output */
+    intel_sdvo_set_target_output(output, 0);
+
+    //intel_sdvo_set_active_outputs(output, sdvo_priv->active_outputs);
+    memset(&output_dtd, 0, sizeof(struct intel_sdvo_dtd));
+    /* check if this mode can be supported or not */
+
+    i830_translate_timing2dtd(mode, &output_dtd);
+#endif
+    intel_sdvo_set_target_output(output, 0);
+    /* set the target input & output first */
+	/* Set the input timing to the screen. Assume always input 0. */
+	intel_sdvo_set_target_output(output, sdvo_priv->active_outputs);
+	intel_sdvo_set_output_timing(output, &output_dtd);
+	intel_sdvo_set_target_input(output, true, false);
+
+    if (sdvo_priv->ActiveDevice == SDVO_DEVICE_TV) {
+	i830_tv_set_overscan_parameters(output);
+	/* Set TV standard */
+	#if 0
+	if (sdvo_priv->TVMode == TVMODE_HDTV)
+	    i830_sdvo_map_hdtvstd_bitmask(output);
+	else
+	    i830_sdvo_map_sdtvstd_bitmask(output);
+	#endif
+	/* Set TV format */
+	i830_sdvo_set_tvoutputs_formats(output);
+	/* We would like to use i830_sdvo_create_preferred_input_timing() to
+	 * provide the device with a timing it can support, if it supports that
+	 * feature.  However, presumably we would need to adjust the CRTC to output
+	 * the preferred timing, and we don't support that currently.
+	 */
+	success = i830_sdvo_create_preferred_input_timing(output, mode);
+	if (success) {
+	    i830_sdvo_get_preferred_input_timing(output, &output_dtd);
+	}
+	/* Set the overscan values now as input timing is dependent on overscan values */
+
+    }
+
+
+	/* We would like to use i830_sdvo_create_preferred_input_timing() to
+	 * provide the device with a timing it can support, if it supports that
+	 * feature.  However, presumably we would need to adjust the CRTC to
+	 * output the preferred timing, and we don't support that currently.
+	 */
+#if 0
+	success = intel_sdvo_create_preferred_input_timing(output, clock,
+							   width, height);
+	if (success) {
+		struct intel_sdvo_dtd *input_dtd;
+
+		intel_sdvo_get_preferred_input_timing(output, &input_dtd);
+		intel_sdvo_set_input_timing(output, &input_dtd);
+	}
+#else
+    /* Set input timing (in DTD) */
+	intel_sdvo_set_input_timing(output, &output_dtd);
+#endif
+    if (sdvo_priv->ActiveDevice == SDVO_DEVICE_TV) {
+
+	DRM_DEBUG("xxintel_sdvo_mode_set tv path\n");
+	i830_tv_program_display_params(output);
+	/* translate dtd 2 timing */
+	i830_translate_dtd2timing(mode, &output_dtd);
+	/* Program clock rate multiplier, 2x,clock is = 0x360b730 */
+	if ((mode->clock * 1000 >= 24000000)
+	    && (mode->clock * 1000 < 50000000)) {
+	    intel_sdvo_set_clock_rate_mult(output, SDVO_CLOCK_RATE_MULT_4X);
+	} else if ((mode->clock * 1000 >= 50000000)
+		   && (mode->clock * 1000 < 100000000)) {
+	    intel_sdvo_set_clock_rate_mult(output, SDVO_CLOCK_RATE_MULT_2X);
+	} else if ((mode->clock * 1000 >= 100000000)
+		   && (mode->clock * 1000 < 200000000)) {
+	    intel_sdvo_set_clock_rate_mult(output, SDVO_CLOCK_RATE_MULT_1X);
+	} else
+	    DRM_DEBUG("i830_sdvo_set_clock_rate is failed\n");
+
+	i830_sdvo_tv_settiming(output->crtc, mode, adjusted_mode);
+	//intel_crtc_mode_set(output->crtc, mode,adjusted_mode,0,0);
+	mode = save_mode;
+    } else {
+	DRM_DEBUG("xxintel_sdvo_mode_set - non tv path\n");
+	switch (intel_sdvo_get_pixel_multiplier(mode)) {
+	case 1:
+		intel_sdvo_set_clock_rate_mult(output,
+					       SDVO_CLOCK_RATE_MULT_1X);
+		break;
+	case 2:
+		intel_sdvo_set_clock_rate_mult(output,
+					       SDVO_CLOCK_RATE_MULT_2X);
+		break;
+	case 4:
+		intel_sdvo_set_clock_rate_mult(output,
+					       SDVO_CLOCK_RATE_MULT_4X);
+		break;
+	}
+    }
+	/* Set the SDVO control regs. */
+        if (0/*IS_I965GM(dev)*/) {
+                sdvox = SDVO_BORDER_ENABLE;
+        } else {
+                sdvox = I915_READ(sdvo_priv->output_device);
+                switch (sdvo_priv->output_device) {
+                case SDVOB:
+                        sdvox &= SDVOB_PRESERVE_MASK;
+                        break;
+                case SDVOC:
+                        sdvox &= SDVOC_PRESERVE_MASK;
+                        break;
+                }
+                sdvox |= (9 << 19) | SDVO_BORDER_ENABLE;
+        }
+	if (intel_crtc->pipe == 1)
+		sdvox |= SDVO_PIPE_B_SELECT;
+
+	sdvo_pixel_multiply = intel_sdvo_get_pixel_multiplier(mode);
+	if (IS_I965G(dev)) {
+		/* done in crtc_mode_set as the dpll_md reg must be written
+		   early */
+	} else if (IS_POULSBO(dev) || IS_I945G(dev) || IS_I945GM(dev)) {
+		/* done in crtc_mode_set as it lives inside the
+		   dpll register */
+	} else {
+		sdvox |= (sdvo_pixel_multiply - 1) << SDVO_PORT_MULTIPLY_SHIFT;
+	}
+
+	intel_sdvo_write_sdvox(output, sdvox);
+	i830_sdvo_set_iomap(output);
+}
+
+static void intel_sdvo_dpms(struct drm_output *output, int mode)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	u32 temp;
+
+	DRM_DEBUG("xxintel_sdvo_dpms, dpms mode is %d, active output is %d\n",mode,sdvo_priv->active_outputs);
+
+#ifdef SII_1392_WA
+	if((SII_1392==1) && (drm_psb_no_fb ==1)) {
+		DRM_DEBUG("don't touch 1392 card when no_fb=1\n");
+		return;
+	}
+#endif
+
+	if (mode != DPMSModeOn) {
+		intel_sdvo_set_active_outputs(output, sdvo_priv->output_device);
+		if (0)
+			intel_sdvo_set_encoder_power_state(output, mode);
+
+		if (mode == DPMSModeOff) {
+			temp = I915_READ(sdvo_priv->output_device);
+			if ((temp & SDVO_ENABLE) != 0) {
+				intel_sdvo_write_sdvox(output, temp & ~SDVO_ENABLE);
+			}
+		}
+	} else {
+		bool input1, input2;
+		int i;
+		u8 status;
+
+		temp = I915_READ(sdvo_priv->output_device);
+		if ((temp & SDVO_ENABLE) == 0)
+			intel_sdvo_write_sdvox(output, temp | SDVO_ENABLE);
+		for (i = 0; i < 2; i++)
+		  intel_wait_for_vblank(dev);
+
+		status = intel_sdvo_get_trained_inputs(output, &input1,
+						       &input2);
+
+
+		/* Warn if the device reported failure to sync.
+		 * A lot of SDVO devices fail to notify of sync, but it's
+		 * a given it the status is a success, we succeeded.
+		 */
+		if (status == SDVO_CMD_STATUS_SUCCESS && !input1) {
+			DRM_DEBUG("First %s output reported failure to sync\n",
+				   SDVO_NAME(sdvo_priv));
+		}
+
+		if (0)
+			intel_sdvo_set_encoder_power_state(output, mode);
+
+		DRM_DEBUG("xiaolin active output is %d\n",sdvo_priv->active_outputs);
+		intel_sdvo_set_active_outputs(output, sdvo_priv->active_outputs);
+	}
+	return;
+}
+
+static void intel_sdvo_save(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+	DRM_DEBUG("xxintel_sdvo_save\n");
+
+	sdvo_priv->save_sdvo_mult = intel_sdvo_get_clock_rate_mult(output);
+	intel_sdvo_get_active_outputs(output, &sdvo_priv->save_active_outputs);
+
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x1) {
+		intel_sdvo_set_target_input(output, true, false);
+		intel_sdvo_get_input_timing(output,
+					    &sdvo_priv->save_input_dtd_1);
+	}
+
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x2) {
+		intel_sdvo_set_target_input(output, false, true);
+		intel_sdvo_get_input_timing(output,
+					    &sdvo_priv->save_input_dtd_2);
+	}
+
+    intel_sdvo_set_target_output(output, sdvo_priv->active_outputs);
+    intel_sdvo_get_output_timing(output,
+				&sdvo_priv->save_output_dtd[sdvo_priv->active_outputs]);
+	sdvo_priv->save_SDVOX = I915_READ(sdvo_priv->output_device);
+}
+
+static void intel_sdvo_restore(struct drm_output *output)
+{
+	struct drm_device *dev = output->dev;
+	DRM_DRIVER_PRIVATE_T *dev_priv = dev->dev_private;
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	int i;
+	bool input1, input2;
+	u8 status;
+	DRM_DEBUG("xxintel_sdvo_restore\n");
+
+	intel_sdvo_set_active_outputs(output, 0);
+
+    intel_sdvo_set_target_output(output, sdvo_priv->save_active_outputs);
+    intel_sdvo_set_output_timing(output,
+				&sdvo_priv->save_output_dtd[sdvo_priv->save_active_outputs]);
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x1) {
+		intel_sdvo_set_target_input(output, true, false);
+		intel_sdvo_set_input_timing(output, &sdvo_priv->save_input_dtd_1);
+	}
+
+	if (sdvo_priv->caps.sdvo_inputs_mask & 0x2) {
+		intel_sdvo_set_target_input(output, false, true);
+		intel_sdvo_set_input_timing(output, &sdvo_priv->save_input_dtd_2);
+	}
+
+	intel_sdvo_set_clock_rate_mult(output, sdvo_priv->save_sdvo_mult);
+
+	I915_WRITE(sdvo_priv->output_device, sdvo_priv->save_SDVOX);
+
+	if (sdvo_priv->save_SDVOX & SDVO_ENABLE)
+	{
+		for (i = 0; i < 2; i++)
+			intel_wait_for_vblank(dev);
+		status = intel_sdvo_get_trained_inputs(output, &input1, &input2);
+		if (status == SDVO_CMD_STATUS_SUCCESS && !input1)
+			DRM_DEBUG("First %s output reported failure to sync\n",
+				   SDVO_NAME(sdvo_priv));
+	}
+
+    i830_sdvo_set_iomap(output);
+	intel_sdvo_set_active_outputs(output, sdvo_priv->save_active_outputs);
+}
+
+static bool i830_tv_mode_find(struct drm_output * output,struct drm_display_mode * pMode)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+    bool find = FALSE;
+    int i;
+
+    DRM_DEBUG("i830_tv_mode_find,0x%x\n", sdvo_priv->TVStandard);
+
+    for (i = 0; i < NUM_TV_MODES; i++)
+    {
+       const tv_mode_t *tv_mode = &tv_modes[i];
+       if (strcmp (tv_mode->mode_entry.name, pMode->name) == 0
+                           && (pMode->type & M_T_TV)) {
+           find = TRUE;
+           break;
+       }
+    }
+    return find;
+}
+
+
+static int intel_sdvo_mode_valid(struct drm_output *output,
+				 struct drm_display_mode *mode)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+	bool status = TRUE;
+	DRM_DEBUG("xxintel_sdvo_mode_valid\n");
+
+	if (sdvo_priv->ActiveDevice == SDVO_DEVICE_TV) {
+	   status = i830_tv_mode_check_support(output, mode);
+	   if (status) {
+		   if(i830_tv_mode_find(output,mode)) {
+			   DRM_DEBUG("%s is ok\n", mode->name);
+			   return MODE_OK;
+		   }
+		   else
+			   return MODE_CLOCK_RANGE;
+	   } else {
+		   DRM_DEBUG("%s is failed\n",
+				 mode->name);
+		   return MODE_CLOCK_RANGE;
+	   }
+	}
+
+	if (mode->flags & V_DBLSCAN)
+		return MODE_NO_DBLESCAN;
+
+	if (sdvo_priv->pixel_clock_min > mode->clock)
+		return MODE_CLOCK_LOW;
+
+	if (sdvo_priv->pixel_clock_max < mode->clock)
+		return MODE_CLOCK_HIGH;
+
+	return MODE_OK;
+}
+
+static bool intel_sdvo_get_capabilities(struct drm_output *output, struct intel_sdvo_caps *caps)
+{
+	u8 status;
+
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_DEVICE_CAPS, NULL, 0);
+	status = intel_sdvo_read_response(output, caps, sizeof(*caps));
+	if (status != SDVO_CMD_STATUS_SUCCESS)
+		return false;
+
+	return true;
+}
+
+void i830_tv_get_default_params(struct drm_output * output)
+{
+    u32 dwSupportedSDTVBitMask = 0;
+    u32 dwSupportedHDTVBitMask = 0;
+	u32 dwTVStdBitmask = 0;
+
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+
+    /* Get supported TV Standard */
+    i830_sdvo_get_supported_tvoutput_formats(output, &dwSupportedSDTVBitMask,
+					     &dwSupportedHDTVBitMask,&dwTVStdBitmask);
+
+    sdvo_priv->dwSDVOSDTVBitMask = dwSupportedSDTVBitMask;
+    sdvo_priv->dwSDVOHDTVBitMask = dwSupportedHDTVBitMask;
+	sdvo_priv->TVStdBitmask = dwTVStdBitmask;
+
+}
+
+static enum drm_output_status intel_sdvo_detect(struct drm_output *output)
+{
+	u8 response[2];
+	u8 status;
+	u8 count = 5;
+
+    char deviceName[256];
+    char *name_suffix;
+	char *name_prefix;
+	unsigned char bytes[2];
+
+	struct drm_device *dev = output->dev;
+
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+	DRM_DEBUG("xxintel_sdvo_detect\n");
+    intel_sdvo_dpms(output, DPMSModeOn);
+
+    if (!intel_sdvo_get_capabilities(output, &sdvo_priv->caps)) {
+        /*No SDVO support, power down the pipe */
+        intel_sdvo_dpms(output, DPMSModeOff);
+        return output_status_disconnected;
+    }
+
+#ifdef SII_1392_WA
+	if ((sdvo_priv->caps.vendor_id == 0x04) && (sdvo_priv->caps.device_id==0xAE)){
+	    /*Leave the control of 1392 to X server*/
+		SII_1392=1;
+		printk("%s: detect 1392 card, leave the setting to up level\n", __FUNCTION__);
+		if (drm_psb_no_fb == 0)
+			intel_sdvo_dpms(output, DPMSModeOff);
+		return output_status_disconnected;
+	}
+#endif
+    while (count--) {
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_ATTACHED_DISPLAYS, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, 2);
+
+	if(count >3 && status == SDVO_CMD_STATUS_PENDING) {
+		intel_sdvo_write_cmd(output,SDVO_CMD_RESET,NULL,0);
+		intel_sdvo_read_response(output, &response, 2);
+		continue;
+	}
+
+	if ((status != SDVO_CMD_STATUS_SUCCESS) || (response[0] == 0 && response[1] == 0)) {
+	    udelay(500);
+	    continue;
+    } else
+     	break;
+    }
+    if (response[0] != 0 || response[1] != 0) {
+	/*Check what device types are connected to the hardware CRT/HDTV/S-Video/Composite */
+	/*in case of CRT and multiple TV's attached give preference in the order mentioned below */
+	/* 1. RGB */
+	/* 2. HDTV */
+	/* 3. S-Video */
+	/* 4. composite */
+	if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_TMDS0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_TMDS0;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "TMDS";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_TMDS;
+	} else if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_TMDS1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_TMDS1;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "TMDS";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_TMDS;
+	} else if (response[0] & SDVO_OUTPUT_RGB0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_RGB0;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "RGB0";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_CRT;
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_RGB1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_RGB1;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "RGB1";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_CRT;
+	} else if (response[0] & SDVO_OUTPUT_YPRPB0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_YPRPB0;
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_YPRPB1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_YPRPB1;
+	}
+	/* SCART is given Second preference */
+	else if (response[0] & SDVO_OUTPUT_SCART0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_SCART0;
+
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_SCART1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_SCART1;
+	}
+	/* if S-Video type TV is connected along with Composite type TV give preference to S-Video */
+	else if (response[0] & SDVO_OUTPUT_SVID0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_SVID0;
+
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_SVID1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_SVID1;
+	}
+	/* Composite is given least preference */
+	else if (response[0] & SDVO_OUTPUT_CVBS0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_CVBS0;
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_CVBS1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_CVBS1;
+	} else {
+	    DRM_DEBUG("no display attached\n");
+
+	    memcpy(bytes, &sdvo_priv->caps.output_flags, 2);
+	    DRM_DEBUG("%s: No active TMDS or RGB outputs (0x%02x%02x) 0x%08x\n",
+		       SDVO_NAME(sdvo_priv), bytes[0], bytes[1],
+		       sdvo_priv->caps.output_flags);
+	    name_prefix = "Unknown";
+	}
+
+	/* init para for TV connector */
+	if (sdvo_priv->active_outputs & SDVO_OUTPUT_TV0) {
+		DRM_INFO("TV is attaced\n");
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "TV0";
+	    /* Init TV mode setting para */
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_TV;
+	    sdvo_priv->bGetClk = TRUE;
+            if (sdvo_priv->active_outputs == SDVO_OUTPUT_YPRPB0 ||
+                                         sdvo_priv->active_outputs == SDVO_OUTPUT_YPRPB1) {
+            /*sdvo_priv->TVStandard = HDTV_SMPTE_274M_1080i60;*/
+                sdvo_priv->TVMode = TVMODE_HDTV;
+            } else {
+            /*sdvo_priv->TVStandard = TVSTANDARD_NTSC_M;*/
+                sdvo_priv->TVMode = TVMODE_SDTV;
+            }
+
+	    /*intel_output->pDevice->TVEnabled = TRUE;*/
+
+		i830_tv_get_default_params(output);
+	    /*Init Display parameter for TV */
+	    sdvo_priv->OverScanX.Value = 0xffffffff;
+	    sdvo_priv->OverScanY.Value = 0xffffffff;
+	    sdvo_priv->dispParams.Brightness.Value = 0x80;
+	    sdvo_priv->dispParams.FlickerFilter.Value = 0xffffffff;
+	    sdvo_priv->dispParams.AdaptiveFF.Value = 7;
+	    sdvo_priv->dispParams.TwoD_FlickerFilter.Value = 0xffffffff;
+	    sdvo_priv->dispParams.Contrast.Value = 0x40;
+	    sdvo_priv->dispParams.PositionX.Value = 0x200;
+	    sdvo_priv->dispParams.PositionY.Value = 0x200;
+	    sdvo_priv->dispParams.DotCrawl.Value = 1;
+	    sdvo_priv->dispParams.ChromaFilter.Value = 1;
+	    sdvo_priv->dispParams.LumaFilter.Value = 2;
+	    sdvo_priv->dispParams.Sharpness.Value = 4;
+	    sdvo_priv->dispParams.Saturation.Value = 0x45;
+	    sdvo_priv->dispParams.Hue.Value = 0x40;
+	    sdvo_priv->dispParams.Dither.Value = 0;
+
+	}
+	else {
+            name_prefix = "RGB0";
+	    DRM_INFO("non TV is attaced\n");
+	}
+        if (sdvo_priv->output_device == SDVOB) {
+	    name_suffix = "-1";
+	} else {
+	    name_suffix = "-2";
+	}
+
+	strcpy(deviceName, name_prefix);
+	strcat(deviceName, name_suffix);
+
+	if(output->name && (strcmp(output->name,deviceName) != 0)){
+	    DRM_DEBUG("change the output name to %s\n", deviceName);
+	    if (!drm_output_rename(output, deviceName)) {
+		drm_output_destroy(output);
+		return output_status_disconnected;
+	    }
+
+	}
+	i830_sdvo_set_iomap(output);
+
+	DRM_INFO("get attached displays=0x%x,0x%x,connectedouputs=0x%x\n",
+		  response[0], response[1], sdvo_priv->active_outputs);
+		return output_status_connected;
+    } else {
+        /*No SDVO display device attached */
+        intel_sdvo_dpms(output, DPMSModeOff);
+		sdvo_priv->ActiveDevice = SDVO_DEVICE_NONE;
+		return output_status_disconnected;
+    }
+}
+
+static int i830_sdvo_get_tvmode_from_table(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+	struct drm_device *dev = output->dev;
+
+	int i, modes = 0;
+
+	for (i = 0; i < NUM_TV_MODES; i++)
+		if (((sdvo_priv->TVMode == TVMODE_HDTV) && /*hdtv mode list */
+		(tv_modes[i].dwSupportedHDTVvss & TVSTANDARD_HDTV_ALL)) ||
+		((sdvo_priv->TVMode == TVMODE_SDTV) && /*sdtv mode list */
+		(tv_modes[i].dwSupportedSDTVvss & TVSTANDARD_SDTV_ALL))) {
+			struct drm_display_mode *newmode;
+			newmode = drm_mode_duplicate(dev, &tv_modes[i].mode_entry);
+			drm_mode_set_crtcinfo(newmode,0);
+			drm_mode_probed_add(output, newmode);
+			modes++;
+		}
+
+	return modes;
+
+}
+
+static int intel_sdvo_get_modes(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	struct intel_sdvo_priv *sdvo_priv = intel_output->dev_priv;
+
+	DRM_DEBUG("xxintel_sdvo_get_modes\n");
+
+	if (sdvo_priv->ActiveDevice == SDVO_DEVICE_TV) {
+		DRM_DEBUG("SDVO_DEVICE_TV\n");
+		i830_sdvo_get_tvmode_from_table(output);
+		if (list_empty(&output->probed_modes))
+			return 0;
+		return 1;
+
+	} else {
+	/* set the bus switch and get the modes */
+	intel_sdvo_set_control_bus_switch(output, SDVO_CONTROL_BUS_DDC2);
+	intel_ddc_get_modes(output);
+
+	if (list_empty(&output->probed_modes))
+		return 0;
+	return 1;
+	}
+#if 0
+	/* Mac mini hack.  On this device, I get DDC through the analog, which
+	 * load-detects as disconnected.  I fail to DDC through the SDVO DDC,
+	 * but it does load-detect as connected.  So, just steal the DDC bits
+	 * from analog when we fail at finding it the right way.
+	 */
+	/* TODO */
+	return NULL;
+
+	return NULL;
+#endif
+}
+
+static void intel_sdvo_destroy(struct drm_output *output)
+{
+	struct intel_output *intel_output = output->driver_private;
+	DRM_DEBUG("xxintel_sdvo_destroy\n");
+
+	if (intel_output->i2c_bus)
+		intel_i2c_destroy(intel_output->i2c_bus);
+
+	if (intel_output) {
+		kfree(intel_output);
+		output->driver_private = NULL;
+	}
+}
+
+static const struct drm_output_funcs intel_sdvo_output_funcs = {
+	.dpms = intel_sdvo_dpms,
+	.save = intel_sdvo_save,
+	.restore = intel_sdvo_restore,
+	.mode_valid = intel_sdvo_mode_valid,
+	.mode_fixup = intel_sdvo_mode_fixup,
+	.prepare = intel_output_prepare,
+	.mode_set = intel_sdvo_mode_set,
+	.commit = intel_output_commit,
+	.detect = intel_sdvo_detect,
+	.get_modes = intel_sdvo_get_modes,
+	.cleanup = intel_sdvo_destroy
+};
+
+void intel_sdvo_init(struct drm_device *dev, int output_device)
+{
+	struct drm_output *output;
+	struct intel_output *intel_output;
+	struct intel_sdvo_priv *sdvo_priv;
+	struct intel_i2c_chan *i2cbus = NULL;
+	u8 ch[0x40];
+	int i;
+	char name[DRM_OUTPUT_LEN];
+	char *name_prefix;
+	char *name_suffix;
+
+    int count = 3;
+    u8 response[2];
+    u8 status;
+	unsigned char bytes[2];
+
+	DRM_DEBUG("xxintel_sdvo_init\n");
+
+		if (IS_POULSBO(dev)) {
+			struct pci_dev * pci_root = pci_get_bus_and_slot(0, 0);
+			u32 sku_value = 0;
+			bool sku_bSDVOEnable = true;
+			if(pci_root)
+			{
+				pci_write_config_dword(pci_root, 0xD0, PCI_PORT5_REG80_FFUSE);
+				pci_read_config_dword(pci_root, 0xD4, &sku_value);
+				sku_bSDVOEnable = (sku_value & PCI_PORT5_REG80_SDVO_DISABLE)?false : true;
+				DRM_INFO("intel_sdvo_init: sku_value is 0x%08x\n", sku_value);
+				DRM_INFO("intel_sdvo_init: sku_bSDVOEnable is %d\n", sku_bSDVOEnable);
+				if (sku_bSDVOEnable == false)
+						return;
+			}
+		}
+
+	output = drm_output_create(dev, &intel_sdvo_output_funcs, NULL);
+	if (!output)
+		return;
+
+	intel_output = kcalloc(sizeof(struct intel_output)+sizeof(struct intel_sdvo_priv), 1, GFP_KERNEL);
+	if (!intel_output) {
+		drm_output_destroy(output);
+		return;
+	}
+
+	sdvo_priv = (struct intel_sdvo_priv *)(intel_output + 1);
+	intel_output->type = INTEL_OUTPUT_SDVO;
+	output->driver_private = intel_output;
+	output->interlace_allowed = 0;
+	output->doublescan_allowed = 0;
+
+	/* setup the DDC bus. */
+	if (output_device == SDVOB)
+		i2cbus = intel_i2c_create(dev, GPIOE, "SDVOCTRL_E for SDVOB");
+	else
+		i2cbus = intel_i2c_create(dev, GPIOE, "SDVOCTRL_E for SDVOC");
+
+	if (i2cbus == NULL) {
+		drm_output_destroy(output);
+		return;
+	}
+
+	sdvo_priv->i2c_bus = i2cbus;
+
+	if (output_device == SDVOB) {
+		name_suffix = "-1";
+		sdvo_priv->i2c_bus->slave_addr = 0x38;
+		sdvo_priv->byInputWiring = SDVOB_IN0;
+	} else {
+		name_suffix = "-2";
+		sdvo_priv->i2c_bus->slave_addr = 0x39;
+	}
+
+	sdvo_priv->output_device = output_device;
+	intel_output->i2c_bus = i2cbus;
+	intel_output->dev_priv = sdvo_priv;
+
+
+	/* Read the regs to test if we can talk to the device */
+	for (i = 0; i < 0x40; i++) {
+		if (!intel_sdvo_read_byte(output, i, &ch[i])) {
+			DRM_DEBUG("No SDVO device found on SDVO%c\n",
+				  output_device == SDVOB ? 'B' : 'C');
+			drm_output_destroy(output);
+			return;
+		}
+	}
+
+	intel_sdvo_get_capabilities(output, &sdvo_priv->caps);
+
+#ifdef SII_1392_WA
+	if ((sdvo_priv->caps.vendor_id == 0x04) && (sdvo_priv->caps.device_id==0xAE)){
+	    /*Leave the control of 1392 to X server*/
+		SII_1392=1;
+		printk("%s: detect 1392 card, leave the setting to up level\n", __FUNCTION__);
+		if (drm_psb_no_fb == 0)
+			intel_sdvo_dpms(output, DPMSModeOff);
+		sdvo_priv->active_outputs = 0;
+		output->subpixel_order = SubPixelHorizontalRGB;
+		name_prefix = "SDVO";
+		sdvo_priv->ActiveDevice = SDVO_DEVICE_NONE;
+		strcpy(name, name_prefix);
+		strcat(name, name_suffix);
+		if (!drm_output_rename(output, name)) {
+			drm_output_destroy(output);
+			return;
+        	}
+		return;
+	}
+#endif
+	memset(&sdvo_priv->active_outputs, 0, sizeof(sdvo_priv->active_outputs));
+
+    while (count--) {
+	intel_sdvo_write_cmd(output, SDVO_CMD_GET_ATTACHED_DISPLAYS, NULL, 0);
+	status = intel_sdvo_read_response(output, &response, 2);
+
+	if (status != SDVO_CMD_STATUS_SUCCESS) {
+	    udelay(1000);
+	    continue;
+	}
+	if (status == SDVO_CMD_STATUS_SUCCESS)
+		break;
+    }
+    if (response[0] != 0 || response[1] != 0) {
+	/*Check what device types are connected to the hardware CRT/HDTV/S-Video/Composite */
+	/*in case of CRT and multiple TV's attached give preference in the order mentioned below */
+	/* 1. RGB */
+	/* 2. HDTV */
+	/* 3. S-Video */
+	/* 4. composite */
+	if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_TMDS0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_TMDS0;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "TMDS";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_TMDS;
+	} else if (sdvo_priv->caps.output_flags & SDVO_OUTPUT_TMDS1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_TMDS1;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "TMDS";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_TMDS;
+	} else if (response[0] & SDVO_OUTPUT_RGB0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_RGB0;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "RGB0";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_CRT;
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_RGB1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_RGB1;
+	    output->subpixel_order = SubPixelHorizontalRGB;
+	    name_prefix = "RGB1";
+	    sdvo_priv->ActiveDevice = SDVO_DEVICE_CRT;
+	} else if (response[0] & SDVO_OUTPUT_YPRPB0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_YPRPB0;
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_YPRPB1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_YPRPB1;
+	}
+	/* SCART is given Second preference */
+	else if (response[0] & SDVO_OUTPUT_SCART0) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_SCART0;
+
+	} else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_SCART1) {
+	    sdvo_priv->active_outputs = SDVO_OUTPUT_SCART1;
+	}
+        /* if S-Video type TV is connected along with Composite type TV give preference to S-Video */
+        else if (response[0] & SDVO_OUTPUT_SVID0) {
+            sdvo_priv->active_outputs = SDVO_OUTPUT_SVID0;
+
+        } else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_SVID1) {
+            sdvo_priv->active_outputs = SDVO_OUTPUT_SVID1;
+        }
+        /* Composite is given least preference */
+        else if (response[0] & SDVO_OUTPUT_CVBS0) {
+            sdvo_priv->active_outputs = SDVO_OUTPUT_CVBS0;
+        } else if ((response[1] << 8 | response[0]) & SDVO_OUTPUT_CVBS1) {
+            sdvo_priv->active_outputs = SDVO_OUTPUT_CVBS1;
+        } else {
+            DRM_DEBUG("no display attached\n");
+
+            memcpy(bytes, &sdvo_priv->caps.output_flags, 2);
+            DRM_INFO("%s: No active TMDS or RGB outputs (0x%02x%02x) 0x%08x\n",
+                       SDVO_NAME(sdvo_priv), bytes[0], bytes[1],
+                       sdvo_priv->caps.output_flags);
+            name_prefix = "Unknown";
+        }
+
+         /* init para for TV connector */
+        if (sdvo_priv->active_outputs & SDVO_OUTPUT_TV0) {
+			DRM_INFO("TV is attaced\n");
+            output->subpixel_order = SubPixelHorizontalRGB;
+            name_prefix = "TV0";
+            /* Init TV mode setting para */
+            sdvo_priv->ActiveDevice = SDVO_DEVICE_TV;
+            sdvo_priv->bGetClk = TRUE;
+            if (sdvo_priv->active_outputs == SDVO_OUTPUT_YPRPB0 ||
+                                         sdvo_priv->active_outputs == SDVO_OUTPUT_YPRPB1) {
+                sdvo_priv->TVStandard = HDTV_SMPTE_274M_1080i60;
+                sdvo_priv->TVMode = TVMODE_HDTV;
+            } else {
+                sdvo_priv->TVStandard = TVSTANDARD_NTSC_M;
+                sdvo_priv->TVMode = TVMODE_SDTV;
+            }
+            /*intel_output->pDevice->TVEnabled = TRUE;*/
+             /*Init Display parameter for TV */
+            sdvo_priv->OverScanX.Value = 0xffffffff;
+            sdvo_priv->OverScanY.Value = 0xffffffff;
+            sdvo_priv->dispParams.Brightness.Value = 0x80;
+            sdvo_priv->dispParams.FlickerFilter.Value = 0xffffffff;
+            sdvo_priv->dispParams.AdaptiveFF.Value = 7;
+            sdvo_priv->dispParams.TwoD_FlickerFilter.Value = 0xffffffff;
+            sdvo_priv->dispParams.Contrast.Value = 0x40;
+            sdvo_priv->dispParams.PositionX.Value = 0x200;
+            sdvo_priv->dispParams.PositionY.Value = 0x200;
+            sdvo_priv->dispParams.DotCrawl.Value = 1;
+            sdvo_priv->dispParams.ChromaFilter.Value = 1;
+            sdvo_priv->dispParams.LumaFilter.Value = 2;
+            sdvo_priv->dispParams.Sharpness.Value = 4;
+            sdvo_priv->dispParams.Saturation.Value = 0x45;
+            sdvo_priv->dispParams.Hue.Value = 0x40;
+            sdvo_priv->dispParams.Dither.Value = 0;
+        }
+        else {
+            name_prefix = "RGB0";
+	    DRM_INFO("non TV is attaced\n");
+        }
+
+        strcpy(name, name_prefix);
+        strcat(name, name_suffix);
+        if (!drm_output_rename(output, name)) {
+            drm_output_destroy(output);
+            return;
+        }
+    } else {
+        /*No SDVO display device attached */
+        intel_sdvo_dpms(output, DPMSModeOff);
+        sdvo_priv->active_outputs = 0;
+        output->subpixel_order = SubPixelHorizontalRGB;
+        name_prefix = "SDVO";
+        sdvo_priv->ActiveDevice = SDVO_DEVICE_NONE;
+        strcpy(name, name_prefix);
+        strcat(name, name_suffix);
+        if (!drm_output_rename(output, name)) {
+            drm_output_destroy(output);
+            return;
+        }
+
+    }
+
+	/*(void)intel_sdvo_set_active_outputs(output, sdvo_priv->active_outputs);*/
+
+	/* Set the input timing to the screen. Assume always input 0. */
+	intel_sdvo_set_target_input(output, true, false);
+
+	intel_sdvo_get_input_pixel_clock_range(output,
+					       &sdvo_priv->pixel_clock_min,
+					       &sdvo_priv->pixel_clock_max);
+
+
+	DRM_DEBUG("%s device VID/DID: %02X:%02X.%02X, "
+		  "clock range %dMHz - %dMHz, "
+		  "input 1: %c, input 2: %c, "
+		  "output 1: %c, output 2: %c\n",
+		  SDVO_NAME(sdvo_priv),
+		  sdvo_priv->caps.vendor_id, sdvo_priv->caps.device_id,
+		  sdvo_priv->caps.device_rev_id,
+		  sdvo_priv->pixel_clock_min / 1000,
+		  sdvo_priv->pixel_clock_max / 1000,
+		  (sdvo_priv->caps.sdvo_inputs_mask & 0x1) ? 'Y' : 'N',
+		  (sdvo_priv->caps.sdvo_inputs_mask & 0x2) ? 'Y' : 'N',
+		  /* check currently supported outputs */
+		  sdvo_priv->caps.output_flags &
+		  	(SDVO_OUTPUT_TMDS0 | SDVO_OUTPUT_RGB0) ? 'Y' : 'N',
+		  sdvo_priv->caps.output_flags &
+		  	(SDVO_OUTPUT_TMDS1 | SDVO_OUTPUT_RGB1) ? 'Y' : 'N');
+
+	intel_output->ddc_bus = i2cbus;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/intel_sdvo_regs.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/intel_sdvo_regs.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,580 @@
+/*
+ * Copyright ?2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+
+/**
+ * @file SDVO command definitions and structures.
+ */
+
+#define SDVO_OUTPUT_FIRST   (0)
+#define SDVO_OUTPUT_TMDS0   (1 << 0)
+#define SDVO_OUTPUT_RGB0    (1 << 1)
+#define SDVO_OUTPUT_CVBS0   (1 << 2)
+#define SDVO_OUTPUT_SVID0   (1 << 3)
+#define SDVO_OUTPUT_YPRPB0  (1 << 4)
+#define SDVO_OUTPUT_SCART0  (1 << 5)
+#define SDVO_OUTPUT_LVDS0   (1 << 6)
+#define SDVO_OUTPUT_TMDS1   (1 << 8)
+#define SDVO_OUTPUT_RGB1    (1 << 9)
+#define SDVO_OUTPUT_CVBS1   (1 << 10)
+#define SDVO_OUTPUT_SVID1   (1 << 11)
+#define SDVO_OUTPUT_YPRPB1  (1 << 12)
+#define SDVO_OUTPUT_SCART1  (1 << 13)
+#define SDVO_OUTPUT_LVDS1   (1 << 14)
+#define SDVO_OUTPUT_LAST    (14)
+
+struct intel_sdvo_caps {
+    u8 vendor_id;
+    u8 device_id;
+    u8 device_rev_id;
+    u8 sdvo_version_major;
+    u8 sdvo_version_minor;
+    unsigned int sdvo_inputs_mask:2;
+    unsigned int smooth_scaling:1;
+    unsigned int sharp_scaling:1;
+    unsigned int up_scaling:1;
+    unsigned int down_scaling:1;
+    unsigned int stall_support:1;
+    unsigned int pad:1;
+    u16 output_flags;
+} __attribute__((packed));
+
+/** This matches the EDID DTD structure, more or less */
+struct intel_sdvo_dtd {
+    struct {
+	u16 clock;		/**< pixel clock, in 10kHz units */
+	u8 h_active;		/**< lower 8 bits (pixels) */
+	u8 h_blank;		/**< lower 8 bits (pixels) */
+	u8 h_high;		/**< upper 4 bits each h_active, h_blank */
+	u8 v_active;		/**< lower 8 bits (lines) */
+	u8 v_blank;		/**< lower 8 bits (lines) */
+	u8 v_high;		/**< upper 4 bits each v_active, v_blank */
+    } part1;
+
+    struct {
+	u8 h_sync_off;	/**< lower 8 bits, from hblank start */
+	u8 h_sync_width;	/**< lower 8 bits (pixels) */
+	/** lower 4 bits each vsync offset, vsync width */
+	u8 v_sync_off_width;
+	/**
+	 * 2 high bits of hsync offset, 2 high bits of hsync width,
+	 * bits 4-5 of vsync offset, and 2 high bits of vsync width.
+	 */
+	u8 sync_off_width_high;
+	u8 dtd_flags;
+	u8 sdvo_flags;
+	/** bits 6-7 of vsync offset at bits 6-7 */
+	u8 v_sync_off_high;
+	u8 reserved;
+    } part2;
+} __attribute__((packed));
+
+struct intel_sdvo_pixel_clock_range {
+    u16 min;			/**< pixel clock, in 10kHz units */
+    u16 max;			/**< pixel clock, in 10kHz units */
+} __attribute__((packed));
+
+struct intel_sdvo_preferred_input_timing_args {
+    u16 clock;
+    u16 width;
+    u16 height;
+} __attribute__((packed));
+
+/* I2C registers for SDVO */
+#define SDVO_I2C_ARG_0				0x07
+#define SDVO_I2C_ARG_1				0x06
+#define SDVO_I2C_ARG_2				0x05
+#define SDVO_I2C_ARG_3				0x04
+#define SDVO_I2C_ARG_4				0x03
+#define SDVO_I2C_ARG_5				0x02
+#define SDVO_I2C_ARG_6				0x01
+#define SDVO_I2C_ARG_7				0x00
+#define SDVO_I2C_OPCODE				0x08
+#define SDVO_I2C_CMD_STATUS			0x09
+#define SDVO_I2C_RETURN_0			0x0a
+#define SDVO_I2C_RETURN_1			0x0b
+#define SDVO_I2C_RETURN_2			0x0c
+#define SDVO_I2C_RETURN_3			0x0d
+#define SDVO_I2C_RETURN_4			0x0e
+#define SDVO_I2C_RETURN_5			0x0f
+#define SDVO_I2C_RETURN_6			0x10
+#define SDVO_I2C_RETURN_7			0x11
+#define SDVO_I2C_VENDOR_BEGIN			0x20
+
+/* Status results */
+#define SDVO_CMD_STATUS_POWER_ON		0x0
+#define SDVO_CMD_STATUS_SUCCESS			0x1
+#define SDVO_CMD_STATUS_NOTSUPP			0x2
+#define SDVO_CMD_STATUS_INVALID_ARG		0x3
+#define SDVO_CMD_STATUS_PENDING			0x4
+#define SDVO_CMD_STATUS_TARGET_NOT_SPECIFIED	0x5
+#define SDVO_CMD_STATUS_SCALING_NOT_SUPP	0x6
+
+/* SDVO commands, argument/result registers */
+
+#define SDVO_CMD_RESET					0x01
+
+/** Returns a struct intel_sdvo_caps */
+#define SDVO_CMD_GET_DEVICE_CAPS			0x02
+
+#define SDVO_CMD_GET_FIRMWARE_REV			0x86
+# define SDVO_DEVICE_FIRMWARE_MINOR			SDVO_I2C_RETURN_0
+# define SDVO_DEVICE_FIRMWARE_MAJOR			SDVO_I2C_RETURN_1
+# define SDVO_DEVICE_FIRMWARE_PATCH			SDVO_I2C_RETURN_2
+
+/**
+ * Reports which inputs are trained (managed to sync).
+ *
+ * Devices must have trained within 2 vsyncs of a mode change.
+ */
+#define SDVO_CMD_GET_TRAINED_INPUTS			0x03
+struct intel_sdvo_get_trained_inputs_response {
+    unsigned int input0_trained:1;
+    unsigned int input1_trained:1;
+    unsigned int pad:6;
+} __attribute__((packed));
+
+/** Returns a struct intel_sdvo_output_flags of active outputs. */
+#define SDVO_CMD_GET_ACTIVE_OUTPUTS			0x04
+
+/**
+ * Sets the current set of active outputs.
+ *
+ * Takes a struct intel_sdvo_output_flags.  Must be preceded by a SET_IN_OUT_MAP
+ * on multi-output devices.
+ */
+#define SDVO_CMD_SET_ACTIVE_OUTPUTS			0x05
+
+/**
+ * Returns the current mapping of SDVO inputs to outputs on the device.
+ *
+ * Returns two struct intel_sdvo_output_flags structures.
+ */
+#define SDVO_CMD_GET_IN_OUT_MAP				0x06
+
+/**
+ * Sets the current mapping of SDVO inputs to outputs on the device.
+ *
+ * Takes two struct i380_sdvo_output_flags structures.
+ */
+#define SDVO_CMD_SET_IN_OUT_MAP				0x07
+
+/**
+ * Returns a struct intel_sdvo_output_flags of attached displays.
+ */
+#define SDVO_CMD_GET_ATTACHED_DISPLAYS			0x0b
+
+/**
+ * Returns a struct intel_sdvo_ouptut_flags of displays supporting hot plugging.
+ */
+#define SDVO_CMD_GET_HOT_PLUG_SUPPORT			0x0c
+
+/**
+ * Takes a struct intel_sdvo_output_flags.
+ */
+#define SDVO_CMD_SET_ACTIVE_HOT_PLUG			0x0d
+
+/**
+ * Returns a struct intel_sdvo_output_flags of displays with hot plug
+ * interrupts enabled.
+ */
+#define SDVO_CMD_GET_ACTIVE_HOT_PLUG			0x0e
+
+#define SDVO_CMD_GET_INTERRUPT_EVENT_SOURCE		0x0f
+struct intel_sdvo_get_interrupt_event_source_response {
+    u16 interrupt_status;
+    unsigned int ambient_light_interrupt:1;
+    unsigned int pad:7;
+} __attribute__((packed));
+
+/**
+ * Selects which input is affected by future input commands.
+ *
+ * Commands affected include SET_INPUT_TIMINGS_PART[12],
+ * GET_INPUT_TIMINGS_PART[12], GET_PREFERRED_INPUT_TIMINGS_PART[12],
+ * GET_INPUT_PIXEL_CLOCK_RANGE, and CREATE_PREFERRED_INPUT_TIMINGS.
+ */
+#define SDVO_CMD_SET_TARGET_INPUT			0x10
+struct intel_sdvo_set_target_input_args {
+    unsigned int target_1:1;
+    unsigned int pad:7;
+} __attribute__((packed));
+
+/**
+ * Takes a struct intel_sdvo_output_flags of which outputs are targetted by
+ * future output commands.
+ *
+ * Affected commands inclue SET_OUTPUT_TIMINGS_PART[12],
+ * GET_OUTPUT_TIMINGS_PART[12], and GET_OUTPUT_PIXEL_CLOCK_RANGE.
+ */
+#define SDVO_CMD_SET_TARGET_OUTPUT			0x11
+
+#define SDVO_CMD_GET_INPUT_TIMINGS_PART1		0x12
+#define SDVO_CMD_GET_INPUT_TIMINGS_PART2		0x13
+#define SDVO_CMD_SET_INPUT_TIMINGS_PART1		0x14
+#define SDVO_CMD_SET_INPUT_TIMINGS_PART2		0x15
+#define SDVO_CMD_SET_OUTPUT_TIMINGS_PART1		0x16
+#define SDVO_CMD_SET_OUTPUT_TIMINGS_PART2		0x17
+#define SDVO_CMD_GET_OUTPUT_TIMINGS_PART1		0x18
+#define SDVO_CMD_GET_OUTPUT_TIMINGS_PART2		0x19
+/* Part 1 */
+# define SDVO_DTD_CLOCK_LOW				SDVO_I2C_ARG_0
+# define SDVO_DTD_CLOCK_HIGH				SDVO_I2C_ARG_1
+# define SDVO_DTD_H_ACTIVE				SDVO_I2C_ARG_2
+# define SDVO_DTD_H_BLANK				SDVO_I2C_ARG_3
+# define SDVO_DTD_H_HIGH				SDVO_I2C_ARG_4
+# define SDVO_DTD_V_ACTIVE				SDVO_I2C_ARG_5
+# define SDVO_DTD_V_BLANK				SDVO_I2C_ARG_6
+# define SDVO_DTD_V_HIGH				SDVO_I2C_ARG_7
+/* Part 2 */
+# define SDVO_DTD_HSYNC_OFF				SDVO_I2C_ARG_0
+# define SDVO_DTD_HSYNC_WIDTH				SDVO_I2C_ARG_1
+# define SDVO_DTD_VSYNC_OFF_WIDTH			SDVO_I2C_ARG_2
+# define SDVO_DTD_SYNC_OFF_WIDTH_HIGH			SDVO_I2C_ARG_3
+# define SDVO_DTD_DTD_FLAGS				SDVO_I2C_ARG_4
+# define SDVO_DTD_DTD_FLAG_INTERLACED				(1 << 7)
+# define SDVO_DTD_DTD_FLAG_STEREO_MASK				(3 << 5)
+# define SDVO_DTD_DTD_FLAG_INPUT_MASK				(3 << 3)
+# define SDVO_DTD_DTD_FLAG_SYNC_MASK				(3 << 1)
+# define SDVO_DTD_SDVO_FLAS				SDVO_I2C_ARG_5
+# define SDVO_DTD_SDVO_FLAG_STALL				(1 << 7)
+# define SDVO_DTD_SDVO_FLAG_CENTERED				(0 << 6)
+# define SDVO_DTD_SDVO_FLAG_UPPER_LEFT				(1 << 6)
+# define SDVO_DTD_SDVO_FLAG_SCALING_MASK			(3 << 4)
+# define SDVO_DTD_SDVO_FLAG_SCALING_NONE			(0 << 4)
+# define SDVO_DTD_SDVO_FLAG_SCALING_SHARP			(1 << 4)
+# define SDVO_DTD_SDVO_FLAG_SCALING_SMOOTH			(2 << 4)
+# define SDVO_DTD_VSYNC_OFF_HIGH			SDVO_I2C_ARG_6
+
+/**
+ * Generates a DTD based on the given width, height, and flags.
+ *
+ * This will be supported by any device supporting scaling or interlaced
+ * modes.
+ */
+#define SDVO_CMD_CREATE_PREFERRED_INPUT_TIMING		0x1a
+# define SDVO_PREFERRED_INPUT_TIMING_CLOCK_LOW		SDVO_I2C_ARG_0
+# define SDVO_PREFERRED_INPUT_TIMING_CLOCK_HIGH		SDVO_I2C_ARG_1
+# define SDVO_PREFERRED_INPUT_TIMING_WIDTH_LOW		SDVO_I2C_ARG_2
+# define SDVO_PREFERRED_INPUT_TIMING_WIDTH_HIGH		SDVO_I2C_ARG_3
+# define SDVO_PREFERRED_INPUT_TIMING_HEIGHT_LOW		SDVO_I2C_ARG_4
+# define SDVO_PREFERRED_INPUT_TIMING_HEIGHT_HIGH	SDVO_I2C_ARG_5
+# define SDVO_PREFERRED_INPUT_TIMING_FLAGS		SDVO_I2C_ARG_6
+# define SDVO_PREFERRED_INPUT_TIMING_FLAGS_INTERLACED		(1 << 0)
+# define SDVO_PREFERRED_INPUT_TIMING_FLAGS_SCALED		(1 << 1)
+
+#define SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART1	0x1b
+#define SDVO_CMD_GET_PREFERRED_INPUT_TIMING_PART2	0x1c
+
+/** Returns a struct intel_sdvo_pixel_clock_range */
+#define SDVO_CMD_GET_INPUT_PIXEL_CLOCK_RANGE		0x1d
+/** Returns a struct intel_sdvo_pixel_clock_range */
+#define SDVO_CMD_GET_OUTPUT_PIXEL_CLOCK_RANGE		0x1e
+
+/** Returns a byte bitfield containing SDVO_CLOCK_RATE_MULT_* flags */
+#define SDVO_CMD_GET_SUPPORTED_CLOCK_RATE_MULTS		0x1f
+
+/** Returns a byte containing a SDVO_CLOCK_RATE_MULT_* flag */
+#define SDVO_CMD_GET_CLOCK_RATE_MULT			0x20
+/** Takes a byte containing a SDVO_CLOCK_RATE_MULT_* flag */
+#define SDVO_CMD_SET_CLOCK_RATE_MULT			0x21
+# define SDVO_CLOCK_RATE_MULT_1X				(1 << 0)
+# define SDVO_CLOCK_RATE_MULT_2X				(1 << 1)
+# define SDVO_CLOCK_RATE_MULT_4X				(1 << 3)
+
+#define SDVO_CMD_GET_SUPPORTED_TV_FORMATS		0x27
+
+#define SDVO_CMD_GET_TV_FORMAT				0x28
+
+#define SDVO_CMD_SET_TV_FORMAT				0x29
+
+#define SDVO_CMD_GET_SUPPORTED_POWER_STATES		0x2a
+#define SDVO_CMD_GET_ENCODER_POWER_STATE		0x2b
+#define SDVO_CMD_SET_ENCODER_POWER_STATE		0x2c
+# define SDVO_ENCODER_STATE_ON					(1 << 0)
+# define SDVO_ENCODER_STATE_STANDBY				(1 << 1)
+# define SDVO_ENCODER_STATE_SUSPEND				(1 << 2)
+# define SDVO_ENCODER_STATE_OFF					(1 << 3)
+
+#define SDVO_CMD_SET_TV_RESOLUTION_SUPPORT		0x93
+
+#define SDVO_CMD_SET_CONTROL_BUS_SWITCH			0x7a
+# define SDVO_CONTROL_BUS_PROM				0x0
+# define SDVO_CONTROL_BUS_DDC1				0x1
+# define SDVO_CONTROL_BUS_DDC2				0x2
+# define SDVO_CONTROL_BUS_DDC3				0x3
+
+/* xiaolin, to support add-on SDVO TV Encoder */
+/* SDVO Bus & SDVO Inputs wiring details*/
+/* Bit 0: Is SDVOB connected to In0 (1 = yes, 0 = no*/
+/* Bit 1: Is SDVOB connected to In1 (1 = yes, 0 = no*/
+/* Bit 2: Is SDVOC connected to In0 (1 = yes, 0 = no*/
+/* Bit 3: Is SDVOC connected to In1 (1 = yes, 0 = no*/
+#define SDVOB_IN0 0x01
+#define SDVOB_IN1 0x02
+#define SDVOC_IN0 0x04
+#define SDVOC_IN1 0x08
+
+#define SDVO_OUTPUT_TV0     0x003C
+#define SDVO_OUTPUT_TV1     0x3C00
+#define SDVO_OUTPUT_LAST    (14)
+
+#define SDVO_OUTPUT_CRT     (SDVO_OUTPUT_RGB0 | SDVO_OUTPUT_RGB1 )
+#define SDVO_OUTPUT_TV      (SDVO_OUTPUT_TV0 | SDVO_OUTPUT_TV1)
+#define SDVO_OUTPUT_LVDS    (SDVO_OUTPUT_LVDS0 | SDVO_OUTPUT_LVDS1)
+#define SDVO_OUTPUT_TMDS    (SDVO_OUTPUT_TMDS0 | SDVO_OUTPUT_TMDS1)
+
+
+
+#define SDVO_DEVICE_NONE 0x00
+#define	SDVO_DEVICE_CRT 0x01
+#define	SDVO_DEVICE_TV 0x02
+#define	SDVO_DEVICE_LVDS 0x04
+#define	SDVO_DEVICE_TMDS 0x08
+
+/* Different TV mode*/
+#define	TVMODE_OFF		0x0000
+#define	TVMODE_SDTV	0x0001
+#define	TVMODE_HDTV	0x0002
+
+#define TVSTANDARD_NONE 0x00
+#define TVSTANDARD_NTSC_M   0x0001  //        75 IRE Setup
+#define TVSTANDARD_NTSC_M_J 0x0002  // Japan,  0 IRE Setup
+#define TVSTANDARD_PAL_B    0x0004
+#define TVSTANDARD_PAL_D    0x0008
+#define TVSTANDARD_PAL_H    0x0010
+#define TVSTANDARD_PAL_I    0x0020
+#define TVSTANDARD_PAL_M    0x0040
+#define TVSTANDARD_PAL_N    0x0080
+#define TVSTANDARD_SECAM_B  0x0100
+#define TVSTANDARD_SECAM_D  0x0200
+#define TVSTANDARD_SECAM_G  0x0400
+#define TVSTANDARD_SECAM_H  0x0800
+#define TVSTANDARD_SECAM_K  0x1000
+#define TVSTANDARD_SECAM_K1 0x2000
+#define TVSTANDARD_SECAM_L  0x4000
+#define TVSTANDARD_WIN_VGA  0x8000
+/*and the rest*/
+#define TVSTANDARD_NTSC_433 0x00010000
+#define TVSTANDARD_PAL_G    0x00020000
+#define TVSTANDARD_PAL_60   0x00040000
+#define TVSTANDARD_SECAM_L1 0x00080000
+#define TVSTANDARD_SDTV_ALL 0x000FFFFF
+
+
+/*HDTV standard defination added using the unused upper 12 bits of dwTVStandard*/
+#define HDTV_SMPTE_170M_480i59		0x00100000
+#define HDTV_SMPTE_293M_480p60		0x00200000
+#define HDTV_SMPTE_293M_480p59		0x00400000
+#define HDTV_ITURBT601_576i50		0x00800000
+#define HDTV_ITURBT601_576p50		0x01000000
+#define HDTV_SMPTE_296M_720p50		0x02000000
+#define HDTV_SMPTE_296M_720p59		0x04000000
+#define HDTV_SMPTE_296M_720p60		0x08000000
+#define HDTV_SMPTE_274M_1080i50		0x10000000
+#define HDTV_SMPTE_274M_1080i59		0x20000000
+#define HDTV_SMPTE_274M_1080i60		0x40000000
+#define HDTV_SMPTE_274M_1080p60		0x80000000
+#define TVSTANDARD_HDTV_ALL			0xFFF00000
+
+
+#define TVSTANDARD_NTSC 0x01
+#define TVSTANDARD_PAL 0x02
+
+#define TVOUTPUT_NONE 0x00
+#define TVOUTPUT_COMPOSITE 0x01
+#define TVOUTPUT_SVIDEO 0x02
+#define TVOUTPUT_RGB 0x04
+#define TVOUTPUT_YCBCR 0x08
+#define TVOUTPUT_SC 0x16
+
+/* Encoder supported TV standard bit mask per SDVO ED*/
+#define SDVO_NTSC_M                                     0x00000001
+#define SDVO_NTSC_M_J                                   0x00000002
+#define SDVO_NTSC_433                                   0x00000004
+#define SDVO_PAL_B                                      0x00000008
+#define SDVO_PAL_D                                      0x00000010
+#define SDVO_PAL_G                                      0x00000020
+#define SDVO_PAL_H                                      0x00000040
+#define SDVO_PAL_I                                      0x00000080
+#define SDVO_PAL_M                                      0x00000100
+#define SDVO_PAL_N                                      0x00000200
+#define SDVO_PAL_NC                                     0x00000400
+#define SDVO_PAL_60                                     0x00000800
+#define SDVO_SECAM_B                                    0x00001000
+#define SDVO_SECAM_D                                    0x00002000
+#define SDVO_SECAM_G                                    0x00004000
+#define SDVO_SECAM_K                                    0x00008000
+#define SDVO_SECAM_K1                                   0x00010000
+#define SDVO_SECAM_L                                    0x00020000
+#define SDVO_SECAM_60									0x00040000
+
+/* Number of SDTV format*/
+#define SDTV_NUM_STANDARDS                              19
+
+/* Encoder supported HDTV standard bit mask per SDVO ED*/
+#define SDVO_HDTV_STD_240M_1080i59                      0x00000008
+#define SDVO_HDTV_STD_240M_1080i60                      0x00000010
+#define SDVO_HDTV_STD_260M_1080i59                      0x00000020
+#define SDVO_HDTV_STD_260M_1080i60                      0x00000040
+#define SDVO_HDTV_STD_274M_1080i50                      0x00000080
+#define SDVO_HDTV_STD_274M_1080i59                      0x00000100
+#define SDVO_HDTV_STD_274M_1080i60                      0x00000200
+#define SDVO_HDTV_STD_274M_1080p23                      0x00000400
+#define SDVO_HDTV_STD_274M_1080p24                      0x00000800
+#define SDVO_HDTV_STD_274M_1080p25                      0x00001000
+#define SDVO_HDTV_STD_274M_1080p29                      0x00002000
+#define SDVO_HDTV_STD_274M_1080p30                      0x00004000
+#define SDVO_HDTV_STD_274M_1080p50                      0x00008000
+#define SDVO_HDTV_STD_274M_1080p59                      0x00010000
+#define SDVO_HDTV_STD_274M_1080p60                      0x00020000
+#define SDVO_HDTV_STD_295M_1080i50                      0x00040000
+#define SDVO_HDTV_STD_295M_1080p50                      0x00080000
+#define SDVO_HDTV_STD_296M_720p59                       0x00100000
+#define SDVO_HDTV_STD_296M_720p60                       0x00200000
+#define SDVO_HDTV_STD_296M_720p50	                    0x00400000
+#define SDVO_HDTV_STD_293M_480p59                       0x00800000
+#define SDVO_HDTV_STD_170M_480i59                       0x01000000
+#define SDVO_HDTV_STD_ITURBT601_576i50			        0x02000000
+#define SDVO_HDTV_STD_ITURBT601_576p50			        0x04000000
+#define SDVO_HDTV_STD_EIA_7702A_480i60                  0x08000000
+#define SDVO_HDTV_STD_EIA_7702A_480p60                  0x10000000
+
+/* SDTV resolution*/
+#define SDVO_SDTV_320x200                               0x00000001
+#define SDVO_SDTV_320x240                               0x00000002
+#define SDVO_SDTV_400x300                               0x00000004
+#define SDVO_SDTV_640x350                               0x00000008
+#define SDVO_SDTV_640x400                               0x00000010
+#define SDVO_SDTV_640x480                               0x00000020
+#define SDVO_SDTV_704x480                               0x00000040
+#define SDVO_SDTV_704x576                               0x00000080
+#define SDVO_SDTV_720x350                               0x00000100
+#define SDVO_SDTV_720x400                               0x00000200
+#define SDVO_SDTV_720x480                               0x00000400
+#define SDVO_SDTV_720x540                               0x00000800
+#define SDVO_SDTV_720x576                               0x00001000
+#define SDVO_SDTV_768x576                               0x00002000
+#define SDVO_SDTV_800x600                               0x00004000
+#define SDVO_SDTV_832x624                               0x00008000
+#define SDVO_SDTV_920x766                               0x00010000
+#define SDVO_SDTV_1024x768                              0x00020000
+#define SDVO_SDTV_1280x1024								0x00040000
+
+
+#define SDVO_HDTV_640x480                               0x00000001
+#define SDVO_HDTV_800x600                               0x00000002
+#define SDVO_HDTV_1024x768                              0x00000004
+#define SDVO_HDTV_1064x600                              0x00020000
+#define SDVO_HDTV_1280x720                              0x00040000
+#define SDVO_HDTV_1704x960                              0x00100000
+#define SDVO_HDTV_1864x1050                             0x00200000
+#define SDVO_HDTV_1920x1080                             0x00400000
+#define SDVO_HDTV_640x400                               0x02000000
+
+/* Number of SDTV mode*/
+#define SDTV_NUM_MODES                                  19
+
+/* sdvo cmd for sdvo tv */
+#define SDVO_CMD_CREATE_PREFERRED_INPUT_TIMINGS 0x1A
+#define SDVO_CMD_GET_SUPPORTED_TV_FORMATS	0x27
+#define SDVO_CMD_GET_TV_FORMATS				0x28
+#define SDVO_CMD_SET_TV_FORMATS				0x29
+
+#define SDVO_CMD_GET_SUPPORTED_POWER_STATES		0x2a
+#define SDVO_CMD_GET_ENCODER_POWER_STATE		0x2b
+#define SDVO_CMD_SET_ENCODER_POWER_STATE		0x2c
+#define SDVO_ENCODER_STATE_ON					(1 << 0)
+#define SDVO_ENCODER_STATE_STANDBY				(1 << 1)
+#define SDVO_ENCODER_STATE_SUSPEND				(1 << 2)
+#define SDVO_ENCODER_STATE_OFF					(1 << 3)
+
+/* Bit mask of picture enhancement*/
+#define SDVO_FLICKER_FILTER                             0x00000001
+#define SDVO_ADAPTIVE_FLICKER_FILTER                    0x00000002
+#define SDVO_2D_FLICKER_FILTER                          0x00000004
+#define SDVO_SATURATION                                 0x00000008
+#define SDVO_HUE                                        0x00000010
+#define SDVO_BRIGHTNESS                                 0x00000020
+#define SDVO_CONTRAST                                   0x00000040
+#define SDVO_HORIZONTAL_OVERSCAN                        0x00000080
+#define SDVO_VERTICAL_OVERSCAN                          0x00000100
+#define SDVO_HORIZONTAL_POSITION                        0x00000200
+#define SDVO_VERTICAL_POSITION                          0x00000400
+#define SDVO_SHARPNESS                                  0x00000800
+#define SDVO_DOT_CRAWL                                  0x00001000
+#define SDVO_DITHER                                     0x00002000
+#define SDVO_MAX_TV_CHROMA_FILTER                       0x00004000
+#define SDVO_TV_MAX_LUMA_FILTER                         0x00008000
+
+#define SDVO_CMD_GET_ANCILLARY_VIDEO_INFORMATION        0x3A
+#define SDVO_CMD_SET_ANCILLARY_VIDEO_INFORMATION        0x3B
+
+#define SDVO_CMD_GET_SUPPORTED_ENHANCEMENTS              0x84
+#define SDVO_CMD_GET_MAX_FLICKER_FILTER                  0x4D
+#define SDVO_CMD_GET_FLICKER_FILTER                      0x4E
+#define SDVO_CMD_SET_FLICKER_FILTER                      0x4F
+#define SDVO_CMD_GET_ADAPTIVE_FLICKER_FILTER             0x50
+#define SDVO_CMD_SET_ADAPTIVE_FLICKER_FILTER             0x51
+#define SDVO_CMD_GET_MAX_2D_FLICKER_FILTER               0x52
+#define SDVO_CMD_GET_2D_FLICKER_FILTER                   0x53
+#define SDVO_CMD_SET_2D_FLICKER_FILTER                   0x54
+#define SDVO_CMD_GET_MAX_SATURATION                      0x55
+#define SDVO_CMD_GET_SATURATION                          0x56
+#define SDVO_CMD_SET_SATURATION                          0x57
+#define SDVO_CMD_GET_MAX_HUE                             0x58
+#define SDVO_CMD_GET_HUE                                 0x59
+#define SDVO_CMD_SET_HUE                                 0x5A
+#define SDVO_CMD_GET_MAX_BRIGHTNESS                      0x5B
+#define SDVO_CMD_GET_BRIGHTNESS                          0x5C
+#define SDVO_CMD_SET_BRIGHTNESS                          0x5D
+#define SDVO_CMD_GET_MAX_CONTRAST                        0x5E
+#define SDVO_CMD_GET_CONTRAST                            0x5F
+#define SDVO_CMD_SET_CONTRAST                            0x60
+
+#define SDVO_CMD_GET_MAX_HORIZONTAL_OVERSCAN             0x61
+#define SDVO_CMD_GET_HORIZONTAL_OVERSCAN                 0x62
+#define SDVO_CMD_SET_HORIZONTAL_OVERSCAN                 0x63
+#define SDVO_CMD_GET_MAX_VERTICAL_OVERSCAN               0x64
+#define SDVO_CMD_GET_VERTICAL_OVERSCAN                   0x65
+#define SDVO_CMD_SET_VERTICAL_OVERSCAN                   0x66
+#define SDVO_CMD_GET_MAX_HORIZONTAL_POSITION             0x67
+#define SDVO_CMD_GET_HORIZONTAL_POSITION                 0x68
+#define SDVO_CMD_SET_HORIZONTAL_POSITION                 0x69
+#define SDVO_CMD_GET_MAX_VERTICAL_POSITION               0x6A
+#define SDVO_CMD_GET_VERTICAL_POSITION                   0x6B
+#define SDVO_CMD_SET_VERTICAL_POSITION                   0x6C
+#define SDVO_CMD_GET_MAX_SHARPNESS                       0x6D
+#define SDVO_CMD_GET_SHARPNESS                           0x6E
+#define SDVO_CMD_SET_SHARPNESS                           0x6F
+#define SDVO_CMD_GET_DOT_CRAWL                           0x70
+#define SDVO_CMD_SET_DOT_CRAWL                           0x71
+#define SDVO_CMD_GET_MAX_TV_CHROMA_FILTER                0x74
+#define SDVO_CMD_GET_TV_CHROMA_FILTER                    0x75
+#define SDVO_CMD_SET_TV_CHROMA_FILTER                    0x76
+#define SDVO_CMD_GET_MAX_TV_LUMA_FILTER                  0x77
+#define SDVO_CMD_GET_TV_LUMA_FILTER                      0x78
+#define SDVO_CMD_SET_TV_LUMA_FILTER                      0x79
+#define SDVO_CMD_GET_MAX_ADAPTIVE_FLICKER_FILTER         0x7B
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_buffer.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_buffer.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,437 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics.com>
+ */
+#include "drmP.h"
+#include "psb_drv.h"
+#include "psb_schedule.h"
+
+struct drm_psb_ttm_backend {
+	struct drm_ttm_backend base;
+	struct page **pages;
+	unsigned int desired_tile_stride;
+	unsigned int hw_tile_stride;
+	int mem_type;
+	unsigned long offset;
+	unsigned long num_pages;
+};
+
+int psb_fence_types(struct drm_buffer_object *bo, uint32_t * class,
+		    uint32_t * type)
+{
+	switch (*class) {
+	case PSB_ENGINE_TA:
+		*type = DRM_FENCE_TYPE_EXE |
+		    _PSB_FENCE_TYPE_TA_DONE | _PSB_FENCE_TYPE_RASTER_DONE;
+		if (bo->mem.mask & PSB_BO_FLAG_TA)
+			*type &= ~_PSB_FENCE_TYPE_RASTER_DONE;
+		if (bo->mem.mask & PSB_BO_FLAG_SCENE)
+			*type |= _PSB_FENCE_TYPE_SCENE_DONE;
+		if (bo->mem.mask & PSB_BO_FLAG_FEEDBACK)
+			*type |= _PSB_FENCE_TYPE_FEEDBACK;
+		break;
+	default:
+		*type = DRM_FENCE_TYPE_EXE;
+	}
+	return 0;
+}
+
+static inline size_t drm_size_align(size_t size)
+{
+	size_t tmpSize = 4;
+	if (size > PAGE_SIZE)
+		return PAGE_ALIGN(size);
+	while (tmpSize < size)
+		tmpSize <<= 1;
+
+	return (size_t) tmpSize;
+}
+
+/*
+ * Poulsbo GPU virtual space looks like this
+ * (We currently use only one MMU context).
+ *
+ * gatt_start = Start of GATT aperture in bus space.
+ * stolen_end = End of GATT populated by stolen memory in bus space.
+ * gatt_end   = End of GATT
+ * twod_end   = MIN(gatt_start + 256_MEM, gatt_end)
+ *
+ * 0x00000000 -> 0x10000000 Temporary mapping space for tiling- and copy operations.
+ *                          This space is not managed and is protected by the
+ *                          temp_mem mutex.
+ *
+ * 0x10000000 -> 0x20000000 DRM_PSB_MEM_KERNEL For kernel buffers.
+ *
+ * 0x20000000 -> gatt_start DRM_PSB_MEM_MMU    For generic MMU-only use.
+ *
+ * gatt_start -> stolen_end DRM_BO_MEM_VRAM    Pre-populated GATT pages.
+ *
+ * stolen_end -> twod_end   DRM_BO_MEM_TT      GATT memory usable by 2D engine.
+ *
+ * twod_end -> gatt_end     DRM_BO_MEM_APER    GATT memory not usable by 2D engine.
+ *
+ * gatt_end ->   0xffffffff Currently unused.
+ */
+
+int psb_init_mem_type(struct drm_device *dev, uint32_t type,
+		      struct drm_mem_type_manager *man)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct psb_gtt *pg = dev_priv->pg;
+
+	switch (type) {
+	case DRM_BO_MEM_LOCAL:
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CACHED;
+		man->drm_bus_maptype = 0;
+		break;
+	case DRM_PSB_MEM_KERNEL:
+		man->io_offset = 0x00000000;
+		man->io_size = 0x00000000;
+		man->io_addr = NULL;
+		man->drm_bus_maptype = _DRM_TTM;
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_MEMTYPE_CMA;
+		man->gpu_offset = PSB_MEM_KERNEL_START;
+		break;
+	case DRM_PSB_MEM_MMU:
+		man->io_offset = 0x00000000;
+		man->io_size = 0x00000000;
+		man->io_addr = NULL;
+		man->drm_bus_maptype = _DRM_TTM;
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_MEMTYPE_CMA;
+		man->gpu_offset = PSB_MEM_MMU_START;
+		break;
+	case DRM_PSB_MEM_PDS:
+		man->io_offset = 0x00000000;
+		man->io_size = 0x00000000;
+		man->io_addr = NULL;
+		man->drm_bus_maptype = _DRM_TTM;
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_MEMTYPE_CMA;
+		man->gpu_offset = PSB_MEM_PDS_START;
+		break;
+	case DRM_PSB_MEM_RASTGEOM:
+		man->io_offset = 0x00000000;
+		man->io_size = 0x00000000;
+		man->io_addr = NULL;
+		man->drm_bus_maptype = _DRM_TTM;
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_MEMTYPE_CMA;
+		man->gpu_offset = PSB_MEM_RASTGEOM_START;
+		break;
+	case DRM_BO_MEM_VRAM:
+		man->io_addr = NULL;
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_FIXED | _DRM_FLAG_NEEDS_IOREMAP;
+#ifdef PSB_WORKING_HOST_MMU_ACCESS
+		man->drm_bus_maptype = _DRM_AGP;
+		man->io_offset = pg->gatt_start;
+		man->io_size = pg->gatt_pages << PAGE_SHIFT;
+#else
+		man->drm_bus_maptype = _DRM_TTM;	/* Forces uncached */
+		man->io_offset = pg->stolen_base;
+		man->io_size = pg->stolen_size;
+#endif
+		man->gpu_offset = pg->gatt_start;
+		break;
+	case DRM_BO_MEM_TT:	/* Mappable GATT memory */
+		man->io_offset = pg->gatt_start;
+		man->io_size = pg->gatt_pages << PAGE_SHIFT;
+		man->io_addr = NULL;
+#ifdef PSB_WORKING_HOST_MMU_ACCESS
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_NEEDS_IOREMAP;
+		man->drm_bus_maptype = _DRM_AGP;
+#else
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_MEMTYPE_CMA;
+		man->drm_bus_maptype = _DRM_TTM;
+#endif
+		man->gpu_offset = pg->gatt_start;
+		break;
+	case DRM_PSB_MEM_APER:	/*MMU memory. Mappable. Not usable for 2D. */
+		man->io_offset = pg->gatt_start;
+		man->io_size = pg->gatt_pages << PAGE_SHIFT;
+		man->io_addr = NULL;
+#ifdef PSB_WORKING_HOST_MMU_ACCESS
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_NEEDS_IOREMAP;
+		man->drm_bus_maptype = _DRM_AGP;
+#else
+		man->flags = _DRM_FLAG_MEMTYPE_MAPPABLE |
+		    _DRM_FLAG_MEMTYPE_CSELECT | _DRM_FLAG_MEMTYPE_CMA;
+		man->drm_bus_maptype = _DRM_TTM;
+#endif
+		man->gpu_offset = pg->gatt_start;
+		break;
+	default:
+		DRM_ERROR("Unsupported memory type %u\n", (unsigned)type);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+uint32_t psb_evict_mask(struct drm_buffer_object * bo)
+{
+	switch (bo->mem.mem_type) {
+	case DRM_BO_MEM_VRAM:
+		return DRM_BO_FLAG_MEM_TT;
+	default:
+		return DRM_BO_FLAG_MEM_LOCAL;
+	}
+}
+
+int psb_invalidate_caches(struct drm_device *dev, uint64_t flags)
+{
+	return 0;
+}
+
+static int psb_move_blit(struct drm_buffer_object *bo,
+			 int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+	int dir = 0;
+
+	if ((old_mem->mem_type == new_mem->mem_type) &&
+	    (new_mem->mm_node->start <
+	     old_mem->mm_node->start + old_mem->mm_node->size)) {
+		dir = 1;
+	}
+
+	psb_emit_2d_copy_blit(bo->dev,
+			      old_mem->mm_node->start << PAGE_SHIFT,
+			      new_mem->mm_node->start << PAGE_SHIFT,
+			      new_mem->num_pages, dir);
+
+	return drm_bo_move_accel_cleanup(bo, evict, no_wait, 0,
+					 DRM_FENCE_TYPE_EXE, 0, new_mem);
+}
+
+/*
+ * Flip destination ttm into cached-coherent GATT,
+ * then blit and subsequently move out again.
+ */
+
+static int psb_move_flip(struct drm_buffer_object *bo,
+			 int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_device *dev = bo->dev;
+	struct drm_bo_mem_reg tmp_mem;
+	int ret;
+
+	tmp_mem = *new_mem;
+	tmp_mem.mm_node = NULL;
+	tmp_mem.mask = DRM_BO_FLAG_MEM_TT |
+	    DRM_BO_FLAG_CACHED | DRM_BO_FLAG_FORCE_CACHING;
+
+	ret = drm_bo_mem_space(bo, &tmp_mem, no_wait);
+	if (ret)
+		return ret;
+	ret = drm_bind_ttm(bo->ttm, &tmp_mem);
+	if (ret)
+		goto out_cleanup;
+	ret = psb_move_blit(bo, 1, no_wait, &tmp_mem);
+	if (ret)
+		goto out_cleanup;
+
+	ret = drm_bo_move_ttm(bo, evict, no_wait, new_mem);
+      out_cleanup:
+	if (tmp_mem.mm_node) {
+		mutex_lock(&dev->struct_mutex);
+		if (tmp_mem.mm_node != bo->pinned_node)
+			drm_mm_put_block(tmp_mem.mm_node);
+		tmp_mem.mm_node = NULL;
+		mutex_unlock(&dev->struct_mutex);
+	}
+	return ret;
+}
+
+int psb_move(struct drm_buffer_object *bo,
+	     int evict, int no_wait, struct drm_bo_mem_reg *new_mem)
+{
+	struct drm_bo_mem_reg *old_mem = &bo->mem;
+
+	if (old_mem->mem_type == DRM_BO_MEM_LOCAL) {
+		return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	} else if (new_mem->mem_type == DRM_BO_MEM_LOCAL) {
+		if (psb_move_flip(bo, evict, no_wait, new_mem))
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	} else {
+		if (psb_move_blit(bo, evict, no_wait, new_mem))
+			return drm_bo_move_memcpy(bo, evict, no_wait, new_mem);
+	}
+	return 0;
+}
+
+static int drm_psb_tbe_nca(struct drm_ttm_backend *backend)
+{
+	return ((backend->flags & DRM_BE_FLAG_BOUND_CACHED) ? 0 : 1);
+}
+
+static int drm_psb_tbe_populate(struct drm_ttm_backend *backend,
+				unsigned long num_pages, struct page **pages)
+{
+	struct drm_psb_ttm_backend *psb_be =
+	    container_of(backend, struct drm_psb_ttm_backend, base);
+
+	psb_be->pages = pages;
+	return 0;
+}
+
+static int drm_psb_tbe_unbind(struct drm_ttm_backend *backend)
+{
+	struct drm_device *dev = backend->dev;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_psb_ttm_backend *psb_be =
+	    container_of(backend, struct drm_psb_ttm_backend, base);
+	struct psb_mmu_pd *pd = psb_mmu_get_default_pd(dev_priv->mmu);
+	struct drm_mem_type_manager *man = &dev->bm.man[psb_be->mem_type];
+
+	PSB_DEBUG_RENDER("MMU unbind.\n");
+
+	if (psb_be->mem_type == DRM_BO_MEM_TT) {
+		uint32_t gatt_p_offset = (psb_be->offset - man->gpu_offset) >>
+		    PAGE_SHIFT;
+
+		(void)psb_gtt_remove_pages(dev_priv->pg, gatt_p_offset,
+					   psb_be->num_pages,
+					   psb_be->desired_tile_stride,
+					   psb_be->hw_tile_stride);
+	}
+
+	psb_mmu_remove_pages(pd, psb_be->offset,
+			     psb_be->num_pages,
+			     psb_be->desired_tile_stride,
+			     psb_be->hw_tile_stride);
+
+	return 0;
+}
+
+static int drm_psb_tbe_bind(struct drm_ttm_backend *backend,
+			    struct drm_bo_mem_reg *bo_mem)
+{
+	struct drm_device *dev = backend->dev;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_psb_ttm_backend *psb_be =
+	    container_of(backend, struct drm_psb_ttm_backend, base);
+	struct psb_mmu_pd *pd = psb_mmu_get_default_pd(dev_priv->mmu);
+	struct drm_mem_type_manager *man = &dev->bm.man[bo_mem->mem_type];
+	int type;
+	int ret = 0;
+
+	psb_be->mem_type = bo_mem->mem_type;
+	psb_be->num_pages = bo_mem->num_pages;
+	psb_be->desired_tile_stride = bo_mem->desired_tile_stride;
+	psb_be->hw_tile_stride = bo_mem->hw_tile_stride;
+	psb_be->desired_tile_stride = 0;
+	psb_be->hw_tile_stride = 0;
+	psb_be->offset = (bo_mem->mm_node->start << PAGE_SHIFT) +
+	    man->gpu_offset;
+
+	type = (bo_mem->flags & DRM_BO_FLAG_CACHED) ? PSB_MMU_CACHED_MEMORY : 0;
+
+	PSB_DEBUG_RENDER("MMU bind.\n");
+	if (psb_be->mem_type == DRM_BO_MEM_TT) {
+		uint32_t gatt_p_offset = (psb_be->offset - man->gpu_offset) >>
+		    PAGE_SHIFT;
+
+		ret = psb_gtt_insert_pages(dev_priv->pg, psb_be->pages,
+					   gatt_p_offset,
+					   psb_be->num_pages,
+					   psb_be->desired_tile_stride,
+					   psb_be->hw_tile_stride, type);
+	}
+
+	ret = psb_mmu_insert_pages(pd, psb_be->pages,
+				   psb_be->offset, psb_be->num_pages,
+				   psb_be->desired_tile_stride,
+				   psb_be->hw_tile_stride, type);
+	if (ret)
+		goto out_err;
+
+	DRM_FLAG_MASKED(backend->flags, (bo_mem->flags & DRM_BO_FLAG_CACHED) ?
+			DRM_BE_FLAG_BOUND_CACHED : 0, DRM_BE_FLAG_BOUND_CACHED);
+
+	return 0;
+      out_err:
+	drm_psb_tbe_unbind(backend);
+	return ret;
+
+}
+
+static void drm_psb_tbe_clear(struct drm_ttm_backend *backend)
+{
+	struct drm_psb_ttm_backend *psb_be =
+	    container_of(backend, struct drm_psb_ttm_backend, base);
+
+	psb_be->pages = NULL;
+	return;
+}
+
+static void drm_psb_tbe_destroy(struct drm_ttm_backend *backend)
+{
+	struct drm_psb_ttm_backend *psb_be =
+	    container_of(backend, struct drm_psb_ttm_backend, base);
+
+	if (backend)
+		drm_free(psb_be, sizeof(*psb_be), DRM_MEM_TTM);
+}
+
+static struct drm_ttm_backend_func psb_ttm_backend = {
+	.needs_ub_cache_adjust = drm_psb_tbe_nca,
+	.populate = drm_psb_tbe_populate,
+	.clear = drm_psb_tbe_clear,
+	.bind = drm_psb_tbe_bind,
+	.unbind = drm_psb_tbe_unbind,
+	.destroy = drm_psb_tbe_destroy,
+};
+
+struct drm_ttm_backend *drm_psb_tbe_init(struct drm_device *dev)
+{
+	struct drm_psb_ttm_backend *psb_be;
+
+	psb_be = drm_calloc(1, sizeof(*psb_be), DRM_MEM_TTM);
+	if (!psb_be)
+		return NULL;
+	psb_be->pages = NULL;
+	psb_be->base.func = &psb_ttm_backend;
+	psb_be->base.dev = dev;
+
+	return &psb_be->base;
+}
+
+int psb_tbe_size(struct drm_device *dev, unsigned long num_pages)
+{
+	/*
+	 * Return the size of the structures themselves and the
+	 * estimated size of the pagedir and pagetable entries.
+	 */
+
+	return drm_size_align(sizeof(struct drm_psb_ttm_backend)) +
+		8*num_pages;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_drm.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_drm.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,370 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ */
+
+#ifndef _PSB_DRM_H_
+#define _PSB_DRM_H_
+
+#if defined(__linux__) && !defined(__KERNEL__)
+#include<stdint.h>
+#endif
+
+/*
+ * Intel Poulsbo driver package version.
+ *
+ */
+/* #define PSB_PACKAGE_VERSION "ED"__DATE__*/
+#define PSB_PACKAGE_VERSION "2.1.0.32L.0019"
+
+#define DRM_PSB_SAREA_MAJOR 0
+#define DRM_PSB_SAREA_MINOR 1
+#define PSB_FIXED_SHIFT 16
+
+/*
+ * Public memory types.
+ */
+
+#define DRM_PSB_MEM_MMU DRM_BO_MEM_PRIV1
+#define DRM_PSB_FLAG_MEM_MMU DRM_BO_FLAG_MEM_PRIV1
+#define DRM_PSB_MEM_PDS DRM_BO_MEM_PRIV2
+#define DRM_PSB_FLAG_MEM_PDS DRM_BO_FLAG_MEM_PRIV2
+#define DRM_PSB_MEM_APER DRM_BO_MEM_PRIV3
+#define DRM_PSB_FLAG_MEM_APER DRM_BO_FLAG_MEM_PRIV3
+#define DRM_PSB_MEM_RASTGEOM DRM_BO_MEM_PRIV4
+#define DRM_PSB_FLAG_MEM_RASTGEOM DRM_BO_FLAG_MEM_PRIV4
+#define PSB_MEM_RASTGEOM_START   0x30000000
+
+typedef int32_t psb_fixed;
+typedef uint32_t psb_ufixed;
+
+static inline psb_fixed psb_int_to_fixed(int a)
+{
+	return a * (1 << PSB_FIXED_SHIFT);
+}
+
+static inline psb_ufixed psb_unsigned_to_ufixed(unsigned int a)
+{
+	return a << PSB_FIXED_SHIFT;
+}
+
+/*Status of the command sent to the gfx device.*/
+typedef enum {
+	DRM_CMD_SUCCESS,
+	DRM_CMD_FAILED,
+	DRM_CMD_HANG
+} drm_cmd_status_t;
+
+struct drm_psb_scanout {
+	uint32_t buffer_id;	/* DRM buffer object ID */
+	uint32_t rotation;	/* Rotation as in RR_rotation definitions */
+	uint32_t stride;	/* Buffer stride in bytes */
+	uint32_t depth;		/* Buffer depth in bits (NOT) bpp */
+	uint32_t width;		/* Buffer width in pixels */
+	uint32_t height;	/* Buffer height in lines */
+	psb_fixed transform[3][3];	/* Buffer composite transform */
+	/* (scaling, rot, reflect) */
+};
+
+#define DRM_PSB_SAREA_OWNERS 16
+#define DRM_PSB_SAREA_OWNER_2D 0
+#define DRM_PSB_SAREA_OWNER_3D 1
+
+#define DRM_PSB_SAREA_SCANOUTS 3
+
+struct drm_psb_sarea {
+	/* Track changes of this data structure */
+
+	uint32_t major;
+	uint32_t minor;
+
+	/* Last context to touch part of hw */
+	uint32_t ctx_owners[DRM_PSB_SAREA_OWNERS];
+
+	/* Definition of front- and rotated buffers */
+	uint32_t num_scanouts;
+	struct drm_psb_scanout scanouts[DRM_PSB_SAREA_SCANOUTS];
+
+	int planeA_x;
+	int planeA_y;
+	int planeA_w;
+	int planeA_h;
+	int planeB_x;
+	int planeB_y;
+	int planeB_w;
+	int planeB_h;
+	uint32_t msvdx_state;
+	uint32_t msvdx_context;
+};
+
+#define PSB_RELOC_MAGIC         0x67676767
+#define PSB_RELOC_SHIFT_MASK    0x0000FFFF
+#define PSB_RELOC_SHIFT_SHIFT   0
+#define PSB_RELOC_ALSHIFT_MASK  0xFFFF0000
+#define PSB_RELOC_ALSHIFT_SHIFT 16
+
+#define PSB_RELOC_OP_OFFSET     0	/* Offset of the indicated
+					 * buffer
+					 */
+#define PSB_RELOC_OP_2D_OFFSET  1	/* Offset of the indicated
+					 *  buffer, relative to 2D
+					 *  base address
+					 */
+#define PSB_RELOC_OP_PDS_OFFSET 2	/* Offset of the indicated buffer,
+					 *  relative to PDS base address
+					 */
+#define PSB_RELOC_OP_STRIDE     3	/* Stride of the indicated
+					 * buffer (for tiling)
+					 */
+#define PSB_RELOC_OP_USE_OFFSET 4	/* Offset of USE buffer
+					 * relative to base reg
+					 */
+#define PSB_RELOC_OP_USE_REG    5	/* Base reg of USE buffer */
+
+struct drm_psb_reloc {
+	uint32_t reloc_op;
+	uint32_t where;		/* offset in destination buffer */
+	uint32_t buffer;	/* Buffer reloc applies to */
+	uint32_t mask;		/* Destination format: */
+	uint32_t shift;		/* Destination format: */
+	uint32_t pre_add;	/* Destination format: */
+	uint32_t background;	/* Destination add */
+	uint32_t dst_buffer;	/* Destination buffer. Index into buffer_list */
+	uint32_t arg0;		/* Reloc-op dependant */
+	uint32_t arg1;
+};
+
+#define PSB_BO_FLAG_TA              (1ULL << 48)
+#define PSB_BO_FLAG_SCENE           (1ULL << 49)
+#define PSB_BO_FLAG_FEEDBACK        (1ULL << 50)
+#define PSB_BO_FLAG_USSE            (1ULL << 51)
+
+#define PSB_ENGINE_2D 0
+#define PSB_ENGINE_VIDEO 1
+#define PSB_ENGINE_RASTERIZER 2
+#define PSB_ENGINE_TA 3
+#define PSB_ENGINE_HPRAST 4
+
+/*
+ * For this fence class we have a couple of
+ * fence types.
+ */
+
+#define _PSB_FENCE_EXE_SHIFT           0
+#define _PSB_FENCE_TA_DONE_SHIFT       1
+#define _PSB_FENCE_RASTER_DONE_SHIFT   2
+#define _PSB_FENCE_SCENE_DONE_SHIFT    3
+#define _PSB_FENCE_FEEDBACK_SHIFT      4
+
+#define _PSB_ENGINE_TA_FENCE_TYPES   5
+#define _PSB_FENCE_TYPE_TA_DONE     (1 << _PSB_FENCE_TA_DONE_SHIFT)
+#define _PSB_FENCE_TYPE_RASTER_DONE (1 << _PSB_FENCE_RASTER_DONE_SHIFT)
+#define _PSB_FENCE_TYPE_SCENE_DONE  (1 << _PSB_FENCE_SCENE_DONE_SHIFT)
+#define _PSB_FENCE_TYPE_FEEDBACK    (1 << _PSB_FENCE_FEEDBACK_SHIFT)
+
+#define PSB_ENGINE_HPRAST 4
+#define PSB_NUM_ENGINES 5
+
+#define PSB_TA_FLAG_FIRSTPASS    (1 << 0)
+#define PSB_TA_FLAG_LASTPASS     (1 << 1)
+
+#define PSB_FEEDBACK_OP_VISTEST (1 << 0)
+
+struct drm_psb_scene {
+	int handle_valid;
+	uint32_t handle;
+	uint32_t w;
+	uint32_t h;
+	uint32_t num_buffers;
+};
+
+struct drm_psb_hw_info
+{
+        uint32_t rev_id;
+        uint32_t caps;
+};
+
+typedef struct drm_psb_cmdbuf_arg {
+	uint64_t buffer_list;	/* List of buffers to validate */
+	uint64_t clip_rects;	/* See i915 counterpart */
+	uint64_t scene_arg;
+	uint64_t fence_arg;
+
+	uint32_t ta_flags;
+
+	uint32_t ta_handle;	/* TA reg-value pairs */
+	uint32_t ta_offset;
+	uint32_t ta_size;
+
+	uint32_t oom_handle;
+	uint32_t oom_offset;
+	uint32_t oom_size;
+
+	uint32_t cmdbuf_handle;	/* 2D Command buffer object or, */
+	uint32_t cmdbuf_offset;	/* rasterizer reg-value pairs */
+	uint32_t cmdbuf_size;
+
+	uint32_t reloc_handle;	/* Reloc buffer object */
+	uint32_t reloc_offset;
+	uint32_t num_relocs;
+
+	int32_t damage;		/* Damage front buffer with cliprects */
+	/* Not implemented yet */
+	uint32_t fence_flags;
+	uint32_t engine;
+
+	/*
+	 * Feedback;
+	 */
+
+	uint32_t feedback_ops;
+	uint32_t feedback_handle;
+	uint32_t feedback_offset;
+	uint32_t feedback_breakpoints;
+	uint32_t feedback_size;
+} drm_psb_cmdbuf_arg_t;
+
+struct drm_psb_xhw_init_arg {
+	uint32_t operation;
+	uint32_t buffer_handle;
+};
+
+/*
+ * Feedback components:
+ */
+
+/*
+ * Vistest component. The number of these in the feedback buffer
+ * equals the number of vistest breakpoints + 1.
+ * This is currently the only feedback component.
+ */
+
+struct drm_psb_vistest {
+	uint32_t vt[8];
+};
+
+#define PSB_HW_COOKIE_SIZE 16
+#define PSB_HW_FEEDBACK_SIZE 8
+#define PSB_HW_OOM_CMD_SIZE 6
+
+struct drm_psb_xhw_arg {
+	uint32_t op;
+	int ret;
+	uint32_t irq_op;
+	uint32_t issue_irq;
+	uint32_t cookie[PSB_HW_COOKIE_SIZE];
+	union {
+		struct {
+			uint32_t w;
+			uint32_t h;
+			uint32_t size;
+			uint32_t clear_p_start;
+			uint32_t clear_num_pages;
+		} si;
+		struct {
+			uint32_t fire_flags;
+			uint32_t hw_context;
+			uint32_t offset;
+			uint32_t engine;
+			uint32_t flags;
+			uint32_t rca;
+			uint32_t num_oom_cmds;
+			uint32_t oom_cmds[PSB_HW_OOM_CMD_SIZE];
+		} sb;
+		struct {
+			uint32_t pages;
+			uint32_t size;
+		} bi;
+		struct {
+			uint32_t bca;
+			uint32_t rca;
+			uint32_t flags;
+		} oom;
+		struct {
+			uint32_t pt_offset;
+			uint32_t param_offset;
+			uint32_t flags;
+		} bl;
+		struct {
+			uint32_t value;
+		} cl;
+		uint32_t feedback[PSB_HW_FEEDBACK_SIZE];
+	} arg;
+};
+
+#define DRM_PSB_CMDBUF          0x00
+#define DRM_PSB_XHW_INIT        0x01
+#define DRM_PSB_XHW             0x02
+#define DRM_PSB_SCENE_UNREF     0x03
+/* Controlling the kernel modesetting buffers */
+#define DRM_PSB_KMS_OFF		0x04
+#define DRM_PSB_KMS_ON		0x05
+#define DRM_PSB_HW_INFO         0x06
+
+#define PSB_XHW_INIT            0x00
+#define PSB_XHW_TAKEDOWN        0x01
+
+#define PSB_XHW_FIRE_RASTER     0x00
+#define PSB_XHW_SCENE_INFO      0x01
+#define PSB_XHW_SCENE_BIND_FIRE 0x02
+#define PSB_XHW_TA_MEM_INFO     0x03
+#define PSB_XHW_RESET_DPM       0x04
+#define PSB_XHW_OOM             0x05
+#define PSB_XHW_TERMINATE       0x06
+#define PSB_XHW_VISTEST         0x07
+#define PSB_XHW_RESUME          0x08
+#define PSB_XHW_TA_MEM_LOAD	0x09
+#define PSB_XHW_CHECK_LOCKUP    0x0a
+
+#define PSB_SCENE_FLAG_DIRTY       (1 << 0)
+#define PSB_SCENE_FLAG_COMPLETE    (1 << 1)
+#define PSB_SCENE_FLAG_SETUP       (1 << 2)
+#define PSB_SCENE_FLAG_SETUP_ONLY  (1 << 3)
+#define PSB_SCENE_FLAG_CLEARED     (1 << 4)
+
+#define PSB_TA_MEM_FLAG_TA            (1 << 0)
+#define PSB_TA_MEM_FLAG_RASTER        (1 << 1)
+#define PSB_TA_MEM_FLAG_HOSTA         (1 << 2)
+#define PSB_TA_MEM_FLAG_HOSTD         (1 << 3)
+#define PSB_TA_MEM_FLAG_INIT          (1 << 4)
+#define PSB_TA_MEM_FLAG_NEW_PT_OFFSET (1 << 5)
+
+/*Raster fire will deallocate memory */
+#define PSB_FIRE_FLAG_RASTER_DEALLOC  (1 << 0)
+/*Isp reset needed due to change in ZLS format */
+#define PSB_FIRE_FLAG_NEEDS_ISP_RESET (1 << 1)
+/*These are set by Xpsb. */
+#define PSB_FIRE_FLAG_XHW_MASK        0xff000000
+/*The task has had at least one OOM and Xpsb will
+  send back messages on each fire. */
+#define PSB_FIRE_FLAG_XHW_OOM         (1 << 24)
+
+#define PSB_SCENE_ENGINE_TA    0
+#define PSB_SCENE_ENGINE_RASTER    1
+#define PSB_SCENE_NUM_ENGINES      2
+
+struct drm_psb_dev_info_arg {
+	uint32_t num_use_attribute_registers;
+};
+#define DRM_PSB_DEVINFO         0x01
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_drv.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_drv.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,1006 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ */
+
+#include "drmP.h"
+#include "drm.h"
+#include "psb_drm.h"
+#include "psb_drv.h"
+#include "psb_reg.h"
+#include "i915_reg.h"
+#include "psb_msvdx.h"
+#include "drm_pciids.h"
+#include "psb_scene.h"
+#include <linux/cpu.h>
+#include <linux/notifier.h>
+#include <linux/fb.h>
+
+int drm_psb_debug = 0;
+EXPORT_SYMBOL(drm_psb_debug);
+static int drm_psb_trap_pagefaults = 0;
+static int drm_psb_clock_gating = 0;
+static int drm_psb_ta_mem_size = 32 * 1024;
+int drm_psb_disable_vsync = 1;
+int drm_psb_no_fb = 0;
+int drm_psb_force_pipeb = 0;
+char* psb_init_mode;
+int psb_init_xres;
+int psb_init_yres;
+/*
+ *
+ */
+#define SII_1392_WA
+#ifdef SII_1392_WA
+extern int SII_1392;
+#endif
+
+MODULE_PARM_DESC(debug, "Enable debug output");
+MODULE_PARM_DESC(clock_gating, "clock gating");
+MODULE_PARM_DESC(no_fb, "Disable FBdev");
+MODULE_PARM_DESC(trap_pagefaults, "Error and reset on MMU pagefaults");
+MODULE_PARM_DESC(disable_vsync, "Disable vsync interrupts");
+MODULE_PARM_DESC(force_pipeb, "Forces PIPEB to become primary fb");
+MODULE_PARM_DESC(ta_mem_size, "TA memory size in kiB");
+MODULE_PARM_DESC(mode, "initial mode name");
+MODULE_PARM_DESC(xres, "initial mode width");
+MODULE_PARM_DESC(yres, "initial mode height");
+
+module_param_named(debug, drm_psb_debug, int, 0600);
+module_param_named(clock_gating, drm_psb_clock_gating, int, 0600);
+module_param_named(no_fb, drm_psb_no_fb, int, 0600);
+module_param_named(trap_pagefaults, drm_psb_trap_pagefaults, int, 0600);
+module_param_named(disable_vsync, drm_psb_disable_vsync, int, 0600);
+module_param_named(force_pipeb, drm_psb_force_pipeb, int, 0600);
+module_param_named(ta_mem_size, drm_psb_ta_mem_size, int, 0600);
+module_param_named(mode, psb_init_mode, charp, 0600);
+module_param_named(xres, psb_init_xres, int, 0600);
+module_param_named(yres, psb_init_yres, int, 0600);
+
+static struct pci_device_id pciidlist[] = {
+	psb_PCI_IDS
+};
+
+#define DRM_PSB_CMDBUF_IOCTL    DRM_IOW(DRM_PSB_CMDBUF, \
+					struct drm_psb_cmdbuf_arg)
+#define DRM_PSB_XHW_INIT_IOCTL  DRM_IOR(DRM_PSB_XHW_INIT, \
+					struct drm_psb_xhw_init_arg)
+#define DRM_PSB_XHW_IOCTL       DRM_IO(DRM_PSB_XHW)
+
+#define DRM_PSB_SCENE_UNREF_IOCTL DRM_IOWR(DRM_PSB_SCENE_UNREF, \
+					   struct drm_psb_scene)
+#define DRM_PSB_HW_INFO_IOCTL DRM_IOR(DRM_PSB_HW_INFO, \
+                                           struct drm_psb_hw_info)
+
+#define DRM_PSB_KMS_OFF_IOCTL	DRM_IO(DRM_PSB_KMS_OFF)
+#define DRM_PSB_KMS_ON_IOCTL	DRM_IO(DRM_PSB_KMS_ON)
+
+static struct drm_ioctl_desc psb_ioctls[] = {
+	DRM_IOCTL_DEF(DRM_PSB_CMDBUF_IOCTL, psb_cmdbuf_ioctl, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_PSB_XHW_INIT_IOCTL, psb_xhw_init_ioctl,
+		      DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_PSB_XHW_IOCTL, psb_xhw_ioctl, DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_PSB_SCENE_UNREF_IOCTL, drm_psb_scene_unref_ioctl,
+		      DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_PSB_KMS_OFF_IOCTL, psbfb_kms_off_ioctl,
+		      DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_PSB_KMS_ON_IOCTL, psbfb_kms_on_ioctl, DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_PSB_HW_INFO_IOCTL, psb_hw_info_ioctl, DRM_AUTH),
+};
+static int psb_max_ioctl = DRM_ARRAY_SIZE(psb_ioctls);
+
+static int probe(struct pci_dev *pdev, const struct pci_device_id *ent);
+
+#ifdef USE_PAT_WC
+#warning Init pat
+static int __cpuinit psb_cpu_callback(struct notifier_block *nfb,
+			    unsigned long action,
+			    void *hcpu)
+{
+	if (action == CPU_ONLINE)
+		drm_init_pat();
+
+	return 0;
+}
+
+static struct notifier_block __cpuinitdata psb_nb = {
+	.notifier_call = psb_cpu_callback,
+	.priority = 1
+};
+#endif
+
+static int dri_library_name(struct drm_device *dev, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "psb\n");
+}
+
+static void psb_set_uopt(struct drm_psb_uopt *uopt)
+{
+	uopt->clock_gating = drm_psb_clock_gating;
+}
+
+static void psb_lastclose(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	if (!dev->dev_private)
+		return;
+
+	mutex_lock(&dev->struct_mutex);
+	if (dev_priv->ta_mem)
+		psb_ta_mem_unref_devlocked(&dev_priv->ta_mem);
+	mutex_unlock(&dev->struct_mutex);
+	mutex_lock(&dev_priv->cmdbuf_mutex);
+	if (dev_priv->buffers) {
+		vfree(dev_priv->buffers);
+		dev_priv->buffers = NULL;
+	}
+	mutex_unlock(&dev_priv->cmdbuf_mutex);
+}
+
+static void psb_do_takedown(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	mutex_lock(&dev->struct_mutex);
+	if (dev->bm.initialized) {
+		if (dev_priv->have_mem_rastgeom) {
+			drm_bo_clean_mm(dev, DRM_PSB_MEM_RASTGEOM);
+			dev_priv->have_mem_rastgeom = 0;
+		}
+		if (dev_priv->have_mem_mmu) {
+			drm_bo_clean_mm(dev, DRM_PSB_MEM_MMU);
+			dev_priv->have_mem_mmu = 0;
+		}
+		if (dev_priv->have_mem_aper) {
+			drm_bo_clean_mm(dev, DRM_PSB_MEM_APER);
+			dev_priv->have_mem_aper = 0;
+		}
+		if (dev_priv->have_tt) {
+			drm_bo_clean_mm(dev, DRM_BO_MEM_TT);
+			dev_priv->have_tt = 0;
+		}
+		if (dev_priv->have_vram) {
+			drm_bo_clean_mm(dev, DRM_BO_MEM_VRAM);
+			dev_priv->have_vram = 0;
+		}
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	if (dev_priv->has_msvdx)
+		psb_msvdx_uninit(dev);
+
+	if (dev_priv->comm) {
+		kunmap(dev_priv->comm_page);
+		dev_priv->comm = NULL;
+	}
+	if (dev_priv->comm_page) {
+		__free_page(dev_priv->comm_page);
+		dev_priv->comm_page = NULL;
+	}
+}
+
+void psb_clockgating(struct drm_psb_private *dev_priv)
+{
+	uint32_t clock_gating;
+
+	if (dev_priv->uopt.clock_gating == 1) {
+		PSB_DEBUG_INIT("Disabling clock gating.\n");
+
+		clock_gating = (_PSB_C_CLKGATECTL_CLKG_DISABLED <<
+				_PSB_C_CLKGATECTL_2D_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_DISABLED <<
+		     _PSB_C_CLKGATECTL_ISP_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_DISABLED <<
+		     _PSB_C_CLKGATECTL_TSP_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_DISABLED <<
+		     _PSB_C_CLKGATECTL_TA_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_DISABLED <<
+		     _PSB_C_CLKGATECTL_DPM_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_DISABLED <<
+		     _PSB_C_CLKGATECTL_USE_CLKG_SHIFT);
+
+	} else if (dev_priv->uopt.clock_gating == 2) {
+		PSB_DEBUG_INIT("Enabling clock gating.\n");
+
+		clock_gating = (_PSB_C_CLKGATECTL_CLKG_AUTO <<
+				_PSB_C_CLKGATECTL_2D_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_AUTO <<
+		     _PSB_C_CLKGATECTL_ISP_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_AUTO <<
+		     _PSB_C_CLKGATECTL_TSP_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_AUTO <<
+		     _PSB_C_CLKGATECTL_TA_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_AUTO <<
+		     _PSB_C_CLKGATECTL_DPM_CLKG_SHIFT) |
+		    (_PSB_C_CLKGATECTL_CLKG_AUTO <<
+		     _PSB_C_CLKGATECTL_USE_CLKG_SHIFT);
+	} else
+		clock_gating = PSB_RSGX32(PSB_CR_CLKGATECTL);
+
+#ifdef FIX_TG_2D_CLOCKGATE
+	clock_gating &= ~_PSB_C_CLKGATECTL_2D_CLKG_MASK;
+	clock_gating |= (_PSB_C_CLKGATECTL_CLKG_DISABLED <<
+			 _PSB_C_CLKGATECTL_2D_CLKG_SHIFT);
+#endif
+	PSB_WSGX32(clock_gating, PSB_CR_CLKGATECTL);
+	(void)PSB_RSGX32(PSB_CR_CLKGATECTL);
+}
+
+static int psb_do_init(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct psb_gtt *pg = dev_priv->pg;
+
+	uint32_t stolen_gtt;
+	uint32_t tt_start;
+	uint32_t tt_pages;
+
+	int ret = -ENOMEM;
+
+	DRM_ERROR("Debug is 0x%08x\n", drm_psb_debug);
+
+	dev_priv->ta_mem_pages =
+	    PSB_ALIGN_TO(drm_psb_ta_mem_size * 1024, PAGE_SIZE) >> PAGE_SHIFT;
+	dev_priv->comm_page = alloc_page(GFP_KERNEL);
+	if (!dev_priv->comm_page)
+		goto out_err;
+
+	dev_priv->comm = kmap(dev_priv->comm_page);
+	memset((void *)dev_priv->comm, 0, PAGE_SIZE);
+
+	dev_priv->has_msvdx = 1;
+	if (psb_msvdx_init(dev))
+		dev_priv->has_msvdx = 0;
+
+	/*
+	 * Initialize sequence numbers for the different command
+	 * submission mechanisms.
+	 */
+
+	dev_priv->sequence[PSB_ENGINE_2D] = 0;
+	dev_priv->sequence[PSB_ENGINE_RASTERIZER] = 0;
+	dev_priv->sequence[PSB_ENGINE_TA] = 0;
+	dev_priv->sequence[PSB_ENGINE_HPRAST] = 0;
+
+	if (pg->gatt_start & 0x0FFFFFFF) {
+		DRM_ERROR("Gatt must be 256M aligned. This is a bug.\n");
+		ret = -EINVAL;
+		goto out_err;
+	}
+
+	stolen_gtt = (pg->stolen_size >> PAGE_SHIFT) * 4;
+	stolen_gtt = (stolen_gtt + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	stolen_gtt = (stolen_gtt < pg->gtt_pages) ? stolen_gtt : pg->gtt_pages;
+
+	dev_priv->gatt_free_offset = pg->gatt_start +
+	    (stolen_gtt << PAGE_SHIFT) * 1024;
+
+	/*
+	 * Insert a cache-coherent communications page in mmu space
+	 * just after the stolen area. Will be used for fencing etc.
+	 */
+
+	dev_priv->comm_mmu_offset = dev_priv->gatt_free_offset;
+	dev_priv->gatt_free_offset += PAGE_SIZE;
+
+	ret = psb_mmu_insert_pages(psb_mmu_get_default_pd(dev_priv->mmu),
+				   &dev_priv->comm_page,
+				   dev_priv->comm_mmu_offset, 1, 0, 0,
+				   PSB_MMU_CACHED_MEMORY);
+
+	if (ret)
+		goto out_err;
+
+	if (1 || drm_debug) {
+		uint32_t core_id = PSB_RSGX32(PSB_CR_CORE_ID);
+		uint32_t core_rev = PSB_RSGX32(PSB_CR_CORE_REVISION);
+		DRM_INFO("SGX core id = 0x%08x\n", core_id);
+		DRM_INFO("SGX core rev major = 0x%02x, minor = 0x%02x\n",
+			 (core_rev & _PSB_CC_REVISION_MAJOR_MASK) >>
+			 _PSB_CC_REVISION_MAJOR_SHIFT,
+			 (core_rev & _PSB_CC_REVISION_MINOR_MASK) >>
+			 _PSB_CC_REVISION_MINOR_SHIFT);
+		DRM_INFO
+		    ("SGX core rev maintenance = 0x%02x, designer = 0x%02x\n",
+		     (core_rev & _PSB_CC_REVISION_MAINTENANCE_MASK) >>
+		     _PSB_CC_REVISION_MAINTENANCE_SHIFT,
+		     (core_rev & _PSB_CC_REVISION_DESIGNER_MASK) >>
+		     _PSB_CC_REVISION_DESIGNER_SHIFT);
+	}
+
+	dev_priv->irqmask_lock = SPIN_LOCK_UNLOCKED;
+	dev_priv->fence0_irq_on = 0;
+
+	tt_pages = (pg->gatt_pages < PSB_TT_PRIV0_PLIMIT) ?
+	    pg->gatt_pages : PSB_TT_PRIV0_PLIMIT;
+	tt_start = dev_priv->gatt_free_offset - pg->gatt_start;
+	tt_pages -= tt_start >> PAGE_SHIFT;
+
+	mutex_lock(&dev->struct_mutex);
+
+	if (!drm_bo_init_mm(dev, DRM_BO_MEM_VRAM, 0,
+			    pg->stolen_size >> PAGE_SHIFT)) {
+		dev_priv->have_vram = 1;
+	}
+
+	if (!drm_bo_init_mm(dev, DRM_BO_MEM_TT, tt_start >> PAGE_SHIFT,
+			    tt_pages)) {
+		dev_priv->have_tt = 1;
+	}
+
+	if (!drm_bo_init_mm(dev, DRM_PSB_MEM_MMU, 0x00000000,
+			    (pg->gatt_start -
+			     PSB_MEM_MMU_START) >> PAGE_SHIFT)) {
+		dev_priv->have_mem_mmu = 1;
+	}
+
+	if (!drm_bo_init_mm(dev, DRM_PSB_MEM_RASTGEOM, 0x00000000,
+			    (PSB_MEM_MMU_START -
+			     PSB_MEM_RASTGEOM_START) >> PAGE_SHIFT)) {
+		dev_priv->have_mem_rastgeom = 1;
+	}
+#if 0
+	if (pg->gatt_pages > PSB_TT_PRIV0_PLIMIT) {
+		if (!drm_bo_init_mm(dev, DRM_PSB_MEM_APER, PSB_TT_PRIV0_PLIMIT,
+				    pg->gatt_pages - PSB_TT_PRIV0_PLIMIT)) {
+			dev_priv->have_mem_aper = 1;
+		}
+	}
+#endif
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+      out_err:
+	psb_do_takedown(dev);
+	return ret;
+}
+
+static int psb_driver_unload(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	intel_modeset_cleanup(dev);
+
+	if (dev_priv) {
+		psb_watchdog_takedown(dev_priv);
+		psb_do_takedown(dev);
+		psb_xhw_takedown(dev_priv);
+		psb_scheduler_takedown(&dev_priv->scheduler);
+
+		mutex_lock(&dev->struct_mutex);
+		if (dev_priv->have_mem_pds) {
+			drm_bo_clean_mm(dev, DRM_PSB_MEM_PDS);
+			dev_priv->have_mem_pds = 0;
+		}
+		if (dev_priv->have_mem_kernel) {
+			drm_bo_clean_mm(dev, DRM_PSB_MEM_KERNEL);
+			dev_priv->have_mem_kernel = 0;
+		}
+		mutex_unlock(&dev->struct_mutex);
+
+		(void)drm_bo_driver_finish(dev);
+
+		if (dev_priv->pf_pd) {
+			psb_mmu_free_pagedir(dev_priv->pf_pd);
+			dev_priv->pf_pd = NULL;
+		}
+		if (dev_priv->mmu) {
+			struct psb_gtt *pg = dev_priv->pg;
+
+			down_read(&pg->sem);
+			psb_mmu_remove_pfn_sequence(psb_mmu_get_default_pd
+						    (dev_priv->mmu),
+						    pg->gatt_start,
+						    pg->
+						    stolen_size >> PAGE_SHIFT);
+			up_read(&pg->sem);
+			psb_mmu_driver_takedown(dev_priv->mmu);
+			dev_priv->mmu = NULL;
+		}
+		psb_gtt_takedown(dev_priv->pg, 1);
+		if (dev_priv->scratch_page) {
+			__free_page(dev_priv->scratch_page);
+			dev_priv->scratch_page = NULL;
+		}
+		psb_takedown_use_base(dev_priv);
+		if (dev_priv->vdc_reg) {
+			iounmap(dev_priv->vdc_reg);
+			dev_priv->vdc_reg = NULL;
+		}
+		if (dev_priv->sgx_reg) {
+			iounmap(dev_priv->sgx_reg);
+			dev_priv->sgx_reg = NULL;
+		}
+		if (dev_priv->msvdx_reg) {
+			iounmap(dev_priv->msvdx_reg);
+			dev_priv->msvdx_reg = NULL;
+		}
+
+		drm_free(dev_priv, sizeof(*dev_priv), DRM_MEM_DRIVER);
+		dev->dev_private = NULL;
+	}
+	return 0;
+}
+
+extern int drm_crtc_probe_output_modes(struct drm_device *dev, int, int);
+extern int drm_pick_crtcs(struct drm_device *dev);
+extern char drm_init_mode[32];
+extern int drm_init_xres;
+extern int drm_init_yres;
+
+static int psb_initial_config(struct drm_device *dev, bool can_grow)
+{
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	struct drm_output *output;
+	struct drm_crtc *crtc;
+	int ret = false;
+
+	mutex_lock(&dev->mode_config.mutex);
+
+	drm_crtc_probe_output_modes(dev, 2048, 2048);
+
+	/* strncpy(drm_init_mode, psb_init_mode, strlen(psb_init_mode)); */
+	drm_init_xres = psb_init_xres;
+	drm_init_yres = psb_init_yres;
+
+	drm_pick_crtcs(dev);
+
+	if ((I915_READ(PIPEACONF) & PIPEACONF_ENABLE) && !drm_psb_force_pipeb)
+		list_for_each_entry(crtc, &dev->mode_config.crtc_list, head) {
+		if (!crtc->desired_mode)
+			continue;
+
+		dev->driver->fb_probe(dev, crtc);
+	} else
+		list_for_each_entry_reverse(crtc, &dev->mode_config.crtc_list,
+					    head) {
+		if (!crtc->desired_mode)
+			continue;
+
+		dev->driver->fb_probe(dev, crtc);
+		}
+
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+
+		if (!output->crtc || !output->crtc->desired_mode)
+			continue;
+
+		if (output->crtc->fb)
+			drm_crtc_set_mode(output->crtc,
+					  output->crtc->desired_mode, 0, 0);
+	}
+
+#ifdef SII_1392_WA
+	if((SII_1392 != 1) || (drm_psb_no_fb==0))
+		drm_disable_unused_functions(dev);
+#else
+	drm_disable_unused_functions(dev);
+#endif
+
+
+	mutex_unlock(&dev->mode_config.mutex);
+
+	return ret;
+
+}
+
+static int psb_driver_load(struct drm_device *dev, unsigned long chipset)
+{
+	struct drm_psb_private *dev_priv;
+	unsigned long resource_start;
+	struct psb_gtt *pg;
+	int ret = -ENOMEM;
+
+	DRM_INFO("psb - %s\n", PSB_PACKAGE_VERSION);
+	dev_priv = drm_calloc(1, sizeof(*dev_priv), DRM_MEM_DRIVER);
+	if (dev_priv == NULL)
+		return -ENOMEM;
+
+	mutex_init(&dev_priv->temp_mem);
+	mutex_init(&dev_priv->cmdbuf_mutex);
+	mutex_init(&dev_priv->reset_mutex);
+	psb_init_disallowed();
+
+	atomic_set(&dev_priv->msvdx_mmu_invaldc, 0);
+
+#ifdef FIX_TG_16
+	atomic_set(&dev_priv->lock_2d, 0);
+	atomic_set(&dev_priv->ta_wait_2d, 0);
+	atomic_set(&dev_priv->ta_wait_2d_irq, 0);
+	atomic_set(&dev_priv->waiters_2d, 0);;
+	DRM_INIT_WAITQUEUE(&dev_priv->queue_2d);
+#else
+	mutex_init(&dev_priv->mutex_2d);
+#endif
+
+	spin_lock_init(&dev_priv->reloc_lock);
+
+	DRM_INIT_WAITQUEUE(&dev_priv->rel_mapped_queue);
+	DRM_INIT_WAITQUEUE(&dev_priv->event_2d_queue);
+
+	dev->dev_private = (void *)dev_priv;
+	dev_priv->chipset = chipset;
+	psb_set_uopt(&dev_priv->uopt);
+
+	psb_watchdog_init(dev_priv);
+	psb_scheduler_init(dev, &dev_priv->scheduler);
+
+	resource_start = pci_resource_start(dev->pdev, PSB_MMIO_RESOURCE);
+
+	dev_priv->msvdx_reg =
+	    ioremap(resource_start + PSB_MSVDX_OFFSET, PSB_MSVDX_SIZE);
+	if (!dev_priv->msvdx_reg)
+		goto out_err;
+
+	dev_priv->vdc_reg =
+	    ioremap(resource_start + PSB_VDC_OFFSET, PSB_VDC_SIZE);
+	if (!dev_priv->vdc_reg)
+		goto out_err;
+
+	dev_priv->sgx_reg =
+	    ioremap(resource_start + PSB_SGX_OFFSET, PSB_SGX_SIZE);
+	if (!dev_priv->sgx_reg)
+		goto out_err;
+
+	psb_clockgating(dev_priv);
+	if (psb_init_use_base(dev_priv, 3, 13))
+		goto out_err;
+
+	dev_priv->scratch_page = alloc_page(GFP_DMA32 | __GFP_ZERO);
+	if (!dev_priv->scratch_page)
+		goto out_err;
+
+	dev_priv->pg = psb_gtt_alloc(dev);
+	if (!dev_priv->pg)
+		goto out_err;
+
+	ret = psb_gtt_init(dev_priv->pg, 0);
+	if (ret)
+		goto out_err;
+
+	dev_priv->mmu = psb_mmu_driver_init(dev_priv->sgx_reg,
+					    drm_psb_trap_pagefaults, 0,
+					    &dev_priv->msvdx_mmu_invaldc);
+	if (!dev_priv->mmu)
+		goto out_err;
+
+	pg = dev_priv->pg;
+
+	/*
+	 * Make sgx MMU aware of the stolen memory area we call VRAM.
+	 */
+
+	down_read(&pg->sem);
+	ret =
+	    psb_mmu_insert_pfn_sequence(psb_mmu_get_default_pd(dev_priv->mmu),
+					pg->stolen_base >> PAGE_SHIFT,
+					pg->gatt_start,
+					pg->stolen_size >> PAGE_SHIFT, 0);
+	up_read(&pg->sem);
+	if (ret)
+		goto out_err;
+
+	dev_priv->pf_pd = psb_mmu_alloc_pd(dev_priv->mmu, 1, 0);
+	if (!dev_priv->pf_pd)
+		goto out_err;
+
+	/*
+	 * Make all presumably unused requestors page-fault by making them
+	 * use context 1 which does not have any valid mappings.
+	 */
+
+	PSB_WSGX32(0x00000000, PSB_CR_BIF_BANK0);
+	PSB_WSGX32(0x00000000, PSB_CR_BIF_BANK1);
+	PSB_RSGX32(PSB_CR_BIF_BANK1);
+
+	psb_mmu_set_pd_context(psb_mmu_get_default_pd(dev_priv->mmu), 0);
+	psb_mmu_set_pd_context(dev_priv->pf_pd, 1);
+	psb_mmu_enable_requestor(dev_priv->mmu, _PSB_MMU_ER_MASK);
+
+	psb_init_2d(dev_priv);
+
+	ret = drm_bo_driver_init(dev);
+	if (ret)
+		goto out_err;
+
+	ret = drm_bo_init_mm(dev, DRM_PSB_MEM_KERNEL, 0x00000000,
+			     (PSB_MEM_PDS_START - PSB_MEM_KERNEL_START)
+			     >> PAGE_SHIFT);
+	if (ret)
+		goto out_err;
+	dev_priv->have_mem_kernel = 1;
+
+	ret = drm_bo_init_mm(dev, DRM_PSB_MEM_PDS, 0x00000000,
+			     (PSB_MEM_RASTGEOM_START - PSB_MEM_PDS_START)
+			     >> PAGE_SHIFT);
+	if (ret)
+		goto out_err;
+	dev_priv->have_mem_pds = 1;
+
+	ret = psb_do_init(dev);
+	if (ret)
+		return ret;
+
+	ret = psb_xhw_init(dev);
+	if (ret)
+		return ret;
+
+	PSB_WSGX32(PSB_MEM_PDS_START, PSB_CR_PDS_EXEC_BASE);
+	PSB_WSGX32(PSB_MEM_RASTGEOM_START, PSB_CR_BIF_3D_REQ_BASE);
+
+	intel_modeset_init(dev);
+	psb_initial_config(dev, false);
+
+#ifdef USE_PAT_WC
+#warning Init pat
+	register_cpu_notifier(&psb_nb);
+#endif
+
+	return 0;
+      out_err:
+	psb_driver_unload(dev);
+	return ret;
+}
+
+int psb_driver_device_is_agp(struct drm_device *dev)
+{
+	return 0;
+}
+
+static int psb_prepare_msvdx_suspend(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[PSB_ENGINE_VIDEO];
+	struct drm_fence_object *fence;
+	int ret = 0;
+	int signaled = 0;
+	int count = 0;
+	unsigned long _end = jiffies + 3 * DRM_HZ;
+
+	PSB_DEBUG_GENERAL("MSVDXACPI Entering psb_prepare_msvdx_suspend....\n");
+
+	/*set the msvdx-reset flag here.. */
+	dev_priv->msvdx_needs_reset = 1;
+
+	/*Ensure that all pending IRQs are serviced, */
+	list_for_each_entry(fence, &fc->ring, ring) {
+		count++;
+		do {
+			DRM_WAIT_ON(ret, fc->fence_queue, 3 * DRM_HZ,
+				    (signaled =
+				     drm_fence_object_signaled(fence,
+							       DRM_FENCE_TYPE_EXE)));
+			if (signaled)
+				break;
+			if (time_after_eq(jiffies, _end))
+				PSB_DEBUG_GENERAL
+				    ("MSVDXACPI: fence 0x%x didn't get signaled for 3 secs; we will suspend anyways\n",
+				     (unsigned int)fence);
+		} while (ret == -EINTR);
+
+	}
+
+	/* Issue software reset */
+        PSB_WMSVDX32 (msvdx_sw_reset_all, MSVDX_CONTROL);
+
+        ret = psb_wait_for_register (dev_priv, MSVDX_CONTROL, 0,
+                           MSVDX_CONTROL_CR_MSVDX_SOFT_RESET_MASK);
+
+	PSB_DEBUG_GENERAL("MSVDXACPI: All MSVDX IRQs (%d) serviced...\n",
+			  count);
+	return 0;
+}
+
+static int psb_suspend(struct pci_dev *pdev, pm_message_t state)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+        struct drm_output *output;
+
+	if (drm_psb_no_fb == 0)
+		psbfb_suspend(dev);
+#ifdef WA_NO_FB_GARBAGE_DISPLAY
+	else {
+		if(num_registered_fb)
+		{
+			list_for_each_entry(output, &dev->mode_config.output_list, head) {
+				if(output->crtc != NULL)
+					intel_crtc_mode_save(output->crtc);
+				//if(output->funcs->save)
+				//	output->funcs->save(output);
+			}
+		}
+	}
+#endif
+
+	dev_priv->saveCLOCKGATING = PSB_RSGX32(PSB_CR_CLKGATECTL);
+	(void)psb_idle_3d(dev);
+	(void)psb_idle_2d(dev);
+	flush_scheduled_work();
+
+	psb_takedown_use_base(dev_priv);
+
+	if (dev_priv->has_msvdx)
+		psb_prepare_msvdx_suspend(dev);
+
+	pci_save_state(pdev);
+	pci_disable_device(pdev);
+	pci_set_power_state(pdev, PCI_D3hot);
+
+	return 0;
+}
+
+static int psb_resume(struct pci_dev *pdev)
+{
+	struct drm_device *dev = pci_get_drvdata(pdev);
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct psb_gtt *pg = dev_priv->pg;
+        struct drm_output *output;
+	int ret;
+
+	pci_set_power_state(pdev, PCI_D0);
+	pci_restore_state(pdev);
+	ret = pci_enable_device(pdev);
+	if (ret)
+		return ret;
+
+#ifdef USE_PAT_WC
+#warning Init pat
+	/* for single CPU's we do it here, then for more than one CPU we
+	 * use the CPU notifier to reinit PAT on those CPU's.
+	 */
+	drm_init_pat();
+#endif
+
+	INIT_LIST_HEAD(&dev_priv->resume_buf.head);
+	dev_priv->msvdx_needs_reset = 1;
+
+	PSB_WVDC32(pg->pge_ctl | _PSB_PGETBL_ENABLED, PSB_PGETBL_CTL);
+	pci_write_config_word(pdev, PSB_GMCH_CTRL,
+			      pg->gmch_ctrl | _PSB_GMCH_ENABLED);
+
+	/*
+	 * The GTT page tables are probably not saved.
+	 * However, TT and VRAM is empty at this point.
+	 */
+
+	psb_gtt_init(dev_priv->pg, 1);
+
+	/*
+	 * The SGX loses it's register contents.
+	 * Restore BIF registers. The MMU page tables are
+	 * "normal" pages, so their contents should be kept.
+	 */
+
+	PSB_WSGX32(dev_priv->saveCLOCKGATING, PSB_CR_CLKGATECTL);
+	PSB_WSGX32(0x00000000, PSB_CR_BIF_BANK0);
+	PSB_WSGX32(0x00000000, PSB_CR_BIF_BANK1);
+	PSB_RSGX32(PSB_CR_BIF_BANK1);
+
+	psb_mmu_set_pd_context(psb_mmu_get_default_pd(dev_priv->mmu), 0);
+	psb_mmu_set_pd_context(dev_priv->pf_pd, 1);
+	psb_mmu_enable_requestor(dev_priv->mmu, _PSB_MMU_ER_MASK);
+
+	/*
+	 * 2D Base registers..
+	 */
+	psb_init_2d(dev_priv);
+
+	if (drm_psb_no_fb == 0) {
+            list_for_each_entry(output, &dev->mode_config.output_list, head) {
+                if(output->crtc != NULL)
+                    drm_crtc_set_mode(output->crtc, &output->crtc->mode,
+                              output->crtc->x, output->crtc->y);
+            }
+        }
+
+	/*
+	 * Persistant 3D base registers and USSE base registers..
+	 */
+
+	PSB_WSGX32(PSB_MEM_PDS_START, PSB_CR_PDS_EXEC_BASE);
+	PSB_WSGX32(PSB_MEM_RASTGEOM_START, PSB_CR_BIF_3D_REQ_BASE);
+	psb_init_use_base(dev_priv, 3, 13);
+
+	/*
+	 * Now, re-initialize the 3D engine.
+	 */
+
+	psb_xhw_resume(dev_priv, &dev_priv->resume_buf);
+
+	psb_scheduler_ta_mem_check(dev_priv);
+	if (dev_priv->ta_mem && !dev_priv->force_ta_mem_load) {
+		psb_xhw_ta_mem_load(dev_priv, &dev_priv->resume_buf,
+				    PSB_TA_MEM_FLAG_TA |
+				    PSB_TA_MEM_FLAG_RASTER |
+				    PSB_TA_MEM_FLAG_HOSTA |
+				    PSB_TA_MEM_FLAG_HOSTD |
+				    PSB_TA_MEM_FLAG_INIT,
+				    dev_priv->ta_mem->ta_memory->offset,
+				    dev_priv->ta_mem->hw_data->offset,
+				    dev_priv->ta_mem->hw_cookie);
+	}
+
+	if (drm_psb_no_fb == 0)
+		psbfb_resume(dev);
+#ifdef WA_NO_FB_GARBAGE_DISPLAY
+	else {
+		if(num_registered_fb)
+		{
+			struct fb_info *fb_info=registered_fb[0];
+			list_for_each_entry(output, &dev->mode_config.output_list, head) {
+				if(output->crtc != NULL)
+					intel_crtc_mode_restore(output->crtc);
+			}
+			if(fb_info)
+			{
+				fb_set_suspend(fb_info, 0);
+				printk("set the fb_set_suspend resume end\n");
+			}
+		}
+	}
+#endif
+
+	return 0;
+}
+
+/* always available as we are SIGIO'd */
+static unsigned int psb_poll(struct file *filp, struct poll_table_struct *wait)
+{
+	return (POLLIN | POLLRDNORM);
+}
+
+static int psb_release(struct inode *inode, struct file *filp)
+{
+	struct drm_file *file_priv = (struct drm_file *)filp->private_data;
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	if (dev_priv && dev_priv->xhw_file) {
+		psb_xhw_init_takedown(dev_priv, file_priv, 1);
+	}
+	return drm_release(inode, filp);
+}
+
+extern struct drm_fence_driver psb_fence_driver;
+
+/*
+ * Use this memory type priority if no eviction is needed.
+ */
+static uint32_t psb_mem_prios[] = { DRM_BO_MEM_VRAM,
+	DRM_BO_MEM_TT,
+	DRM_PSB_MEM_KERNEL,
+	DRM_PSB_MEM_MMU,
+	DRM_PSB_MEM_RASTGEOM,
+	DRM_PSB_MEM_PDS,
+	DRM_PSB_MEM_APER,
+	DRM_BO_MEM_LOCAL
+};
+
+/*
+ * Use this memory type priority if need to evict.
+ */
+static uint32_t psb_busy_prios[] = { DRM_BO_MEM_TT,
+	DRM_BO_MEM_VRAM,
+	DRM_PSB_MEM_KERNEL,
+	DRM_PSB_MEM_MMU,
+	DRM_PSB_MEM_RASTGEOM,
+	DRM_PSB_MEM_PDS,
+	DRM_PSB_MEM_APER,
+	DRM_BO_MEM_LOCAL
+};
+
+static struct drm_bo_driver psb_bo_driver = {
+	.mem_type_prio = psb_mem_prios,
+	.mem_busy_prio = psb_busy_prios,
+	.num_mem_type_prio = ARRAY_SIZE(psb_mem_prios),
+	.num_mem_busy_prio = ARRAY_SIZE(psb_busy_prios),
+	.create_ttm_backend_entry = drm_psb_tbe_init,
+	.fence_type = psb_fence_types,
+	.invalidate_caches = psb_invalidate_caches,
+	.init_mem_type = psb_init_mem_type,
+	.evict_mask = psb_evict_mask,
+	.move = psb_move,
+	.backend_size = psb_tbe_size,
+	.command_stream_barrier = NULL,
+};
+
+static struct drm_driver driver = {
+	.driver_features = DRIVER_HAVE_IRQ | DRIVER_IRQ_SHARED |
+	    DRIVER_IRQ_VBL | DRIVER_IRQ_VBL2,
+	.load = psb_driver_load,
+	.unload = psb_driver_unload,
+	.dri_library_name = dri_library_name,
+	.get_reg_ofs = drm_core_get_reg_ofs,
+	.ioctls = psb_ioctls,
+	.device_is_agp = psb_driver_device_is_agp,
+	.vblank_wait = psb_vblank_wait,
+	.vblank_wait2 = psb_vblank_wait2,
+	.irq_preinstall = psb_irq_preinstall,
+	.irq_postinstall = psb_irq_postinstall,
+	.irq_uninstall = psb_irq_uninstall,
+	.irq_handler = psb_irq_handler,
+	.fb_probe = psbfb_probe,
+	.fb_remove = psbfb_remove,
+	.firstopen = NULL,
+	.lastclose = psb_lastclose,
+	.fops = {
+		 .owner = THIS_MODULE,
+		 .open = drm_open,
+		 .release = psb_release,
+		 .ioctl = drm_ioctl,
+		 .mmap = drm_mmap,
+		 .poll = psb_poll,
+		 .fasync = drm_fasync,
+		 },
+	.pci_driver = {
+		       .name = DRIVER_NAME,
+		       .id_table = pciidlist,
+		       .probe = probe,
+		       .remove = __devexit_p(drm_cleanup_pci),
+		       .resume = psb_resume,
+		       .suspend = psb_suspend,
+		       },
+	.fence_driver = &psb_fence_driver,
+	.bo_driver = &psb_bo_driver,
+	.name = DRIVER_NAME,
+	.desc = DRIVER_DESC,
+	.date = PSB_DRM_DRIVER_DATE,
+	.major = PSB_DRM_DRIVER_MAJOR,
+	.minor = PSB_DRM_DRIVER_MINOR,
+	.patchlevel = PSB_DRM_DRIVER_PATCHLEVEL
+};
+
+static int probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	return drm_get_dev(pdev, ent, &driver);
+}
+
+static int __init psb_init(void)
+{
+	driver.num_ioctls = psb_max_ioctl;
+
+	return drm_init(&driver, pciidlist);
+}
+
+static void __exit psb_exit(void)
+{
+	drm_exit(&driver);
+}
+
+module_init(psb_init);
+module_exit(psb_exit);
+
+MODULE_AUTHOR(DRIVER_AUTHOR);
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_LICENSE("GPL");
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_drv.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_drv.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,775 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ */
+#ifndef _PSB_DRV_H_
+#define _PSB_DRV_H_
+
+#include "drmP.h"
+#include "psb_drm.h"
+#include "psb_reg.h"
+#include "psb_schedule.h"
+#include "intel_drv.h"
+
+enum {
+	CHIP_PSB_8108 = 0,
+	CHIP_PSB_8109 = 1
+};
+
+/*
+ * Hardware bugfixes
+ */
+
+#define FIX_TG_16
+#define FIX_TG_2D_CLOCKGATE
+
+#define DRIVER_NAME "psb"
+#define DRIVER_DESC "drm driver for the Intel GMA500"
+#define DRIVER_AUTHOR "Tungsten Graphics Inc."
+
+#define PSB_DRM_DRIVER_DATE "20080613"
+#define PSB_DRM_DRIVER_MAJOR 4
+#define PSB_DRM_DRIVER_MINOR 12
+#define PSB_DRM_DRIVER_PATCHLEVEL 0
+
+#define PSB_VDC_OFFSET           0x00000000
+#define PSB_VDC_SIZE             0x000080000
+#define PSB_SGX_SIZE             0x8000
+#define PSB_SGX_OFFSET           0x00040000
+#define PSB_MMIO_RESOURCE        0
+#define PSB_GATT_RESOURCE        2
+#define PSB_GTT_RESOURCE         3
+#define PSB_GMCH_CTRL            0x52
+#define PSB_BSM                  0x5C
+#define _PSB_GMCH_ENABLED        0x4
+#define PSB_PGETBL_CTL           0x2020
+#define _PSB_PGETBL_ENABLED      0x00000001
+#define PSB_SGX_2D_SLAVE_PORT    0x4000
+#define PSB_TT_PRIV0_LIMIT       (256*1024*1024)
+#define PSB_TT_PRIV0_PLIMIT      (PSB_TT_PRIV0_LIMIT >> PAGE_SHIFT)
+#define PSB_NUM_VALIDATE_BUFFERS 1024
+#define PSB_MEM_KERNEL_START     0x10000000
+#define PSB_MEM_PDS_START        0x20000000
+#define PSB_MEM_MMU_START        0x40000000
+
+#define DRM_PSB_MEM_KERNEL       DRM_BO_MEM_PRIV0
+#define DRM_PSB_FLAG_MEM_KERNEL  DRM_BO_FLAG_MEM_PRIV0
+
+/*
+ * Flags for external memory type field.
+ */
+
+#define PSB_MSVDX_OFFSET        0x50000	/*MSVDX Base offset */
+#define PSB_MSVDX_SIZE          0x8000	/*MSVDX MMIO region is 0x50000 - 0x57fff ==> 32KB */
+
+#define PSB_MMU_CACHED_MEMORY     0x0001	/* Bind to MMU only */
+#define PSB_MMU_RO_MEMORY         0x0002	/* MMU RO memory */
+#define PSB_MMU_WO_MEMORY         0x0004	/* MMU WO memory */
+
+/*
+ * PTE's and PDE's
+ */
+
+#define PSB_PDE_MASK              0x003FFFFF
+#define PSB_PDE_SHIFT             22
+#define PSB_PTE_SHIFT             12
+
+#define PSB_PTE_VALID             0x0001	/* PTE / PDE valid */
+#define PSB_PTE_WO                0x0002	/* Write only */
+#define PSB_PTE_RO                0x0004	/* Read only */
+#define PSB_PTE_CACHED            0x0008	/* CPU cache coherent */
+
+/*
+ * VDC registers and bits
+ */
+#define PSB_HWSTAM                0x2098
+#define PSB_INSTPM                0x20C0
+#define PSB_INT_IDENTITY_R        0x20A4
+#define _PSB_VSYNC_PIPEB_FLAG     (1<<5)
+#define _PSB_VSYNC_PIPEA_FLAG     (1<<7)
+#define _PSB_IRQ_SGX_FLAG         (1<<18)
+#define _PSB_IRQ_MSVDX_FLAG       (1<<19)
+#define PSB_INT_MASK_R            0x20A8
+#define PSB_INT_ENABLE_R          0x20A0
+#define PSB_PIPEASTAT             0x70024
+#define _PSB_VBLANK_INTERRUPT_ENABLE (1 << 17)
+#define _PSB_VBLANK_CLEAR         (1 << 1)
+#define PSB_PIPEBSTAT             0x71024
+
+#define _PSB_MMU_ER_MASK      0x0001FF00
+#define _PSB_MMU_ER_HOST      (1 << 16)
+#define GPIOA			0x5010
+#define GPIOB			0x5014
+#define GPIOC			0x5018
+#define GPIOD			0x501c
+#define GPIOE			0x5020
+#define GPIOF			0x5024
+#define GPIOG			0x5028
+#define GPIOH			0x502c
+#define GPIO_CLOCK_DIR_MASK		(1 << 0)
+#define GPIO_CLOCK_DIR_IN		(0 << 1)
+#define GPIO_CLOCK_DIR_OUT		(1 << 1)
+#define GPIO_CLOCK_VAL_MASK		(1 << 2)
+#define GPIO_CLOCK_VAL_OUT		(1 << 3)
+#define GPIO_CLOCK_VAL_IN		(1 << 4)
+#define GPIO_CLOCK_PULLUP_DISABLE	(1 << 5)
+#define GPIO_DATA_DIR_MASK		(1 << 8)
+#define GPIO_DATA_DIR_IN		(0 << 9)
+#define GPIO_DATA_DIR_OUT		(1 << 9)
+#define GPIO_DATA_VAL_MASK		(1 << 10)
+#define GPIO_DATA_VAL_OUT		(1 << 11)
+#define GPIO_DATA_VAL_IN		(1 << 12)
+#define GPIO_DATA_PULLUP_DISABLE	(1 << 13)
+
+#define VCLK_DIVISOR_VGA0   0x6000
+#define VCLK_DIVISOR_VGA1   0x6004
+#define VCLK_POST_DIV       0x6010
+
+#define DRM_DRIVER_PRIVATE_T struct drm_psb_private
+#define I915_WRITE(_offs, _val) \
+  iowrite32(_val, dev_priv->vdc_reg + (_offs))
+#define I915_READ(_offs) \
+  ioread32(dev_priv->vdc_reg + (_offs))
+
+#define PSB_COMM_2D (PSB_ENGINE_2D << 4)
+#define PSB_COMM_3D (PSB_ENGINE_3D << 4)
+#define PSB_COMM_TA (PSB_ENGINE_TA << 4)
+#define PSB_COMM_HP (PSB_ENGINE_HP << 4)
+#define PSB_COMM_USER_IRQ (1024 >> 2)
+#define PSB_COMM_USER_IRQ_LOST (PSB_COMM_USER_IRQ + 1)
+#define PSB_COMM_FW (2048 >> 2)
+
+#define PSB_UIRQ_VISTEST               1
+#define PSB_UIRQ_OOM_REPLY             2
+#define PSB_UIRQ_FIRE_TA_REPLY         3
+#define PSB_UIRQ_FIRE_RASTER_REPLY     4
+
+#define PSB_2D_SIZE (256*1024*1024)
+#define PSB_MAX_RELOC_PAGES 1024
+
+#define PSB_LOW_REG_OFFS 0x0204
+#define PSB_HIGH_REG_OFFS 0x0600
+
+#define PSB_NUM_VBLANKS 2
+
+#define PSB_COMM_2D (PSB_ENGINE_2D << 4)
+#define PSB_COMM_3D (PSB_ENGINE_3D << 4)
+#define PSB_COMM_TA (PSB_ENGINE_TA << 4)
+#define PSB_COMM_HP (PSB_ENGINE_HP << 4)
+#define PSB_COMM_FW (2048 >> 2)
+
+#define PSB_2D_SIZE (256*1024*1024)
+#define PSB_MAX_RELOC_PAGES 1024
+
+#define PSB_LOW_REG_OFFS 0x0204
+#define PSB_HIGH_REG_OFFS 0x0600
+
+#define PSB_NUM_VBLANKS 2
+#define PSB_WATCHDOG_DELAY (DRM_HZ / 10)
+
+/*
+ * User options.
+ */
+
+struct drm_psb_uopt {
+	int clock_gating;
+};
+
+struct psb_gtt {
+	struct drm_device *dev;
+	int initialized;
+	uint32_t gatt_start;
+	uint32_t gtt_start;
+	uint32_t gtt_phys_start;
+	unsigned gtt_pages;
+	unsigned gatt_pages;
+	uint32_t stolen_base;
+	uint32_t pge_ctl;
+	u16 gmch_ctrl;
+	unsigned long stolen_size;
+	uint32_t *gtt_map;
+	struct rw_semaphore sem;
+};
+
+struct psb_use_base {
+	struct list_head head;
+	struct drm_fence_object *fence;
+	unsigned int reg;
+	unsigned long offset;
+	unsigned int dm;
+};
+
+struct psb_buflist_item;
+
+struct psb_msvdx_cmd_queue {
+	struct list_head head;
+	void *cmd;
+	unsigned long cmd_size;
+	uint32_t sequence;
+};
+
+struct drm_psb_private {
+	unsigned long chipset;
+        uint8_t psb_rev_id;
+
+	struct psb_xhw_buf resume_buf;
+	struct drm_psb_dev_info_arg dev_info;
+	struct drm_psb_uopt uopt;
+
+	struct psb_gtt *pg;
+
+	struct page *scratch_page;
+	struct page *comm_page;
+
+	volatile uint32_t *comm;
+	uint32_t comm_mmu_offset;
+	uint32_t mmu_2d_offset;
+	uint32_t sequence[PSB_NUM_ENGINES];
+	uint32_t last_sequence[PSB_NUM_ENGINES];
+	int idle[PSB_NUM_ENGINES];
+	uint32_t last_submitted_seq[PSB_NUM_ENGINES];
+	int engine_lockup_2d;
+
+	struct psb_mmu_driver *mmu;
+	struct psb_mmu_pd *pf_pd;
+
+	uint8_t *sgx_reg;
+	uint8_t *vdc_reg;
+	uint8_t *msvdx_reg;
+
+	/*
+	 * MSVDX
+	 */
+	int msvdx_needs_reset;
+	int has_msvdx;
+	uint32_t gatt_free_offset;
+	atomic_t msvdx_mmu_invaldc;
+
+	/*
+	 * Fencing / irq.
+	 */
+
+	uint32_t sgx_irq_mask;
+	uint32_t sgx2_irq_mask;
+	uint32_t vdc_irq_mask;
+
+	spinlock_t irqmask_lock;
+	spinlock_t sequence_lock;
+	int fence0_irq_on;
+	int irq_enabled;
+	unsigned int irqen_count_2d;
+	wait_queue_head_t event_2d_queue;
+
+#ifdef FIX_TG_16
+	wait_queue_head_t queue_2d;
+	atomic_t lock_2d;
+	atomic_t ta_wait_2d;
+	atomic_t ta_wait_2d_irq;
+	atomic_t waiters_2d;
+#else
+	struct mutex mutex_2d;
+#endif
+	uint32_t msvdx_current_sequence;
+	uint32_t msvdx_last_sequence;
+#define MSVDX_MAX_IDELTIME HZ*30
+	uint32_t msvdx_finished_sequence;
+	uint32_t msvdx_start_idle;
+	unsigned long msvdx_idle_start_jiffies;
+
+	int fence2_irq_on;
+
+	/*
+	 * MSVDX Rendec Memory
+	 */
+	struct drm_buffer_object *ccb0;
+	uint32_t base_addr0;
+	struct drm_buffer_object *ccb1;
+	uint32_t base_addr1;
+
+	/*
+	 * Memory managers
+	 */
+
+	int have_vram;
+	int have_tt;
+	int have_mem_mmu;
+	int have_mem_aper;
+	int have_mem_kernel;
+	int have_mem_pds;
+	int have_mem_rastgeom;
+	struct mutex temp_mem;
+
+	/*
+	 * Relocation buffer mapping.
+	 */
+
+	spinlock_t reloc_lock;
+	unsigned int rel_mapped_pages;
+	wait_queue_head_t rel_mapped_queue;
+
+	/*
+	 * SAREA
+	 */
+	struct drm_psb_sarea *sarea_priv;
+
+	/*
+	 * LVDS info
+	 */
+	int backlight_duty_cycle;	/* restore backlight to this value */
+	bool panel_wants_dither;
+	struct drm_display_mode *panel_fixed_mode;
+
+	/*
+	 * Register state
+	 */
+	uint32_t saveDSPACNTR;
+	uint32_t saveDSPBCNTR;
+	uint32_t savePIPEACONF;
+	uint32_t savePIPEBCONF;
+	uint32_t savePIPEASRC;
+	uint32_t savePIPEBSRC;
+	uint32_t saveFPA0;
+	uint32_t saveFPA1;
+	uint32_t saveDPLL_A;
+	uint32_t saveDPLL_A_MD;
+	uint32_t saveHTOTAL_A;
+	uint32_t saveHBLANK_A;
+	uint32_t saveHSYNC_A;
+	uint32_t saveVTOTAL_A;
+	uint32_t saveVBLANK_A;
+	uint32_t saveVSYNC_A;
+	uint32_t saveDSPASTRIDE;
+	uint32_t saveDSPASIZE;
+	uint32_t saveDSPAPOS;
+	uint32_t saveDSPABASE;
+	uint32_t saveDSPASURF;
+	uint32_t saveFPB0;
+	uint32_t saveFPB1;
+	uint32_t saveDPLL_B;
+	uint32_t saveDPLL_B_MD;
+	uint32_t saveHTOTAL_B;
+	uint32_t saveHBLANK_B;
+	uint32_t saveHSYNC_B;
+	uint32_t saveVTOTAL_B;
+	uint32_t saveVBLANK_B;
+	uint32_t saveVSYNC_B;
+	uint32_t saveDSPBSTRIDE;
+	uint32_t saveDSPBSIZE;
+	uint32_t saveDSPBPOS;
+	uint32_t saveDSPBBASE;
+	uint32_t saveDSPBSURF;
+	uint32_t saveVCLK_DIVISOR_VGA0;
+	uint32_t saveVCLK_DIVISOR_VGA1;
+	uint32_t saveVCLK_POST_DIV;
+	uint32_t saveVGACNTRL;
+	uint32_t saveADPA;
+	uint32_t saveLVDS;
+	uint32_t saveDVOA;
+	uint32_t saveDVOB;
+	uint32_t saveDVOC;
+	uint32_t savePP_ON;
+	uint32_t savePP_OFF;
+	uint32_t savePP_CONTROL;
+	uint32_t savePP_CYCLE;
+	uint32_t savePFIT_CONTROL;
+	uint32_t savePaletteA[256];
+	uint32_t savePaletteB[256];
+	uint32_t saveBLC_PWM_CTL;
+	uint32_t saveCLOCKGATING;
+
+	/*
+	 * USE code base register management.
+	 */
+
+	struct drm_reg_manager use_manager;
+
+	/*
+	 * Xhw
+	 */
+
+	uint32_t *xhw;
+	struct drm_buffer_object *xhw_bo;
+	struct drm_bo_kmap_obj xhw_kmap;
+	struct list_head xhw_in;
+	spinlock_t xhw_lock;
+	atomic_t xhw_client;
+	struct drm_file *xhw_file;
+	wait_queue_head_t xhw_queue;
+	wait_queue_head_t xhw_caller_queue;
+	struct mutex xhw_mutex;
+	struct psb_xhw_buf *xhw_cur_buf;
+	int xhw_submit_ok;
+	int xhw_on;
+
+	/*
+	 * Scheduling.
+	 */
+
+	struct mutex reset_mutex;
+	struct mutex cmdbuf_mutex;
+	struct psb_scheduler scheduler;
+        struct psb_buflist_item *buffers;
+	uint32_t ta_mem_pages;
+	struct psb_ta_mem *ta_mem;
+	int force_ta_mem_load;
+
+	/*
+	 * Watchdog
+	 */
+
+	spinlock_t watchdog_lock;
+	struct timer_list watchdog_timer;
+	struct work_struct watchdog_wq;
+	struct work_struct msvdx_watchdog_wq;
+	int timer_available;
+
+	/*
+	 * msvdx command queue
+	 */
+	spinlock_t msvdx_lock;
+	struct mutex msvdx_mutex;
+	struct list_head msvdx_queue;
+	int msvdx_busy;
+
+};
+
+struct psb_mmu_driver;
+
+extern struct psb_mmu_driver *psb_mmu_driver_init(uint8_t __iomem * registers,
+						  int trap_pagefaults,
+						  int invalid_type,
+						  atomic_t *msvdx_mmu_invaldc);
+extern void psb_mmu_driver_takedown(struct psb_mmu_driver *driver);
+extern struct psb_mmu_pd *psb_mmu_get_default_pd(struct psb_mmu_driver *driver);
+extern void psb_mmu_mirror_gtt(struct psb_mmu_pd *pd, uint32_t mmu_offset,
+			       uint32_t gtt_start, uint32_t gtt_pages);
+extern void psb_mmu_test(struct psb_mmu_driver *driver, uint32_t offset);
+extern struct psb_mmu_pd *psb_mmu_alloc_pd(struct psb_mmu_driver *driver,
+					   int trap_pagefaults,
+					   int invalid_type);
+extern void psb_mmu_free_pagedir(struct psb_mmu_pd *pd);
+extern void psb_mmu_flush(struct psb_mmu_driver *driver);
+extern void psb_mmu_remove_pfn_sequence(struct psb_mmu_pd *pd,
+					unsigned long address,
+					uint32_t num_pages);
+extern int psb_mmu_insert_pfn_sequence(struct psb_mmu_pd *pd,
+				       uint32_t start_pfn,
+				       unsigned long address,
+				       uint32_t num_pages, int type);
+extern int psb_mmu_virtual_to_pfn(struct psb_mmu_pd *pd, uint32_t virtual,
+				  unsigned long *pfn);
+
+/*
+ * Enable / disable MMU for different requestors.
+ */
+
+extern void psb_mmu_enable_requestor(struct psb_mmu_driver *driver,
+				     uint32_t mask);
+extern void psb_mmu_disable_requestor(struct psb_mmu_driver *driver,
+				      uint32_t mask);
+extern void psb_mmu_set_pd_context(struct psb_mmu_pd *pd, int hw_context);
+extern int psb_mmu_insert_pages(struct psb_mmu_pd *pd, struct page **pages,
+				unsigned long address, uint32_t num_pages,
+				uint32_t desired_tile_stride,
+				uint32_t hw_tile_stride, int type);
+extern void psb_mmu_remove_pages(struct psb_mmu_pd *pd, unsigned long address,
+				 uint32_t num_pages,
+				 uint32_t desired_tile_stride,
+				 uint32_t hw_tile_stride);
+/*
+ * psb_sgx.c
+ */
+
+extern int psb_blit_sequence(struct drm_psb_private *dev_priv,
+			     uint32_t sequence);
+extern void psb_init_2d(struct drm_psb_private *dev_priv);
+extern int psb_idle_2d(struct drm_device *dev);
+extern int psb_idle_3d(struct drm_device *dev);
+extern int psb_emit_2d_copy_blit(struct drm_device *dev,
+				 uint32_t src_offset,
+				 uint32_t dst_offset, uint32_t pages,
+				 int direction);
+extern int psb_cmdbuf_ioctl(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv);
+extern int psb_reg_submit(struct drm_psb_private *dev_priv, uint32_t * regs,
+			  unsigned int cmds);
+extern int psb_submit_copy_cmdbuf(struct drm_device *dev,
+				  struct drm_buffer_object *cmd_buffer,
+				  unsigned long cmd_offset,
+				  unsigned long cmd_size, int engine,
+				  uint32_t * copy_buffer);
+extern void psb_fence_or_sync(struct drm_file *priv,
+			      int engine,
+			      struct drm_psb_cmdbuf_arg *arg,
+			      struct drm_fence_arg *fence_arg,
+			      struct drm_fence_object **fence_p);
+extern void psb_init_disallowed(void);
+
+/*
+ * psb_irq.c
+ */
+
+extern irqreturn_t psb_irq_handler(DRM_IRQ_ARGS);
+extern void psb_irq_preinstall(struct drm_device *dev);
+extern int psb_irq_postinstall(struct drm_device *dev);
+extern void psb_irq_uninstall(struct drm_device *dev);
+extern int psb_vblank_wait2(struct drm_device *dev, unsigned int *sequence);
+extern int psb_vblank_wait(struct drm_device *dev, unsigned int *sequence);
+
+/*
+ * psb_fence.c
+ */
+
+extern void psb_fence_handler(struct drm_device *dev, uint32_t class);
+extern void psb_2D_irq_off(struct drm_psb_private *dev_priv);
+extern void psb_2D_irq_on(struct drm_psb_private *dev_priv);
+extern uint32_t psb_fence_advance_sequence(struct drm_device *dev,
+					   uint32_t class);
+extern int psb_fence_emit_sequence(struct drm_device *dev, uint32_t fence_class,
+				   uint32_t flags, uint32_t * sequence,
+				   uint32_t * native_type);
+extern void psb_fence_error(struct drm_device *dev,
+			    uint32_t class,
+			    uint32_t sequence, uint32_t type, int error);
+
+/*MSVDX stuff*/
+extern void psb_msvdx_irq_off(struct drm_psb_private *dev_priv);
+extern void psb_msvdx_irq_on(struct drm_psb_private *dev_priv);
+extern int psb_hw_info_ioctl(struct drm_device *dev, void *data,
+                            struct drm_file *file_priv);
+
+/*
+ * psb_buffer.c
+ */
+extern struct drm_ttm_backend *drm_psb_tbe_init(struct drm_device *dev);
+extern int psb_fence_types(struct drm_buffer_object *bo, uint32_t * class,
+			   uint32_t * type);
+extern uint32_t psb_evict_mask(struct drm_buffer_object *bo);
+extern int psb_invalidate_caches(struct drm_device *dev, uint64_t flags);
+extern int psb_init_mem_type(struct drm_device *dev, uint32_t type,
+			     struct drm_mem_type_manager *man);
+extern int psb_move(struct drm_buffer_object *bo,
+		    int evict, int no_wait, struct drm_bo_mem_reg *new_mem);
+extern int psb_tbe_size(struct drm_device *dev, unsigned long num_pages);
+
+/*
+ * psb_gtt.c
+ */
+extern int psb_gtt_init(struct psb_gtt *pg, int resume);
+extern int psb_gtt_insert_pages(struct psb_gtt *pg, struct page **pages,
+				unsigned offset_pages, unsigned num_pages,
+				unsigned desired_tile_stride,
+				unsigned hw_tile_stride, int type);
+extern int psb_gtt_remove_pages(struct psb_gtt *pg, unsigned offset_pages,
+				unsigned num_pages,
+				unsigned desired_tile_stride,
+				unsigned hw_tile_stride);
+
+extern struct psb_gtt *psb_gtt_alloc(struct drm_device *dev);
+extern void psb_gtt_takedown(struct psb_gtt *pg, int free);
+
+/*
+ * psb_fb.c
+ */
+extern int psbfb_probe(struct drm_device *dev, struct drm_crtc *crtc);
+extern int psbfb_remove(struct drm_device *dev, struct drm_crtc *crtc);
+extern int psbfb_kms_off_ioctl(struct drm_device *dev, void *data,
+			       struct drm_file *file_priv);
+extern int psbfb_kms_on_ioctl(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv);
+extern void psbfb_suspend(struct drm_device *dev);
+extern void psbfb_resume(struct drm_device *dev);
+
+/*
+ * psb_reset.c
+ */
+
+extern void psb_reset(struct drm_psb_private *dev_priv, int reset_2d);
+extern void psb_schedule_watchdog(struct drm_psb_private *dev_priv);
+extern void psb_watchdog_init(struct drm_psb_private *dev_priv);
+extern void psb_watchdog_takedown(struct drm_psb_private *dev_priv);
+extern void psb_print_pagefault(struct drm_psb_private *dev_priv);
+
+/*
+ * psb_regman.c
+ */
+
+extern void psb_takedown_use_base(struct drm_psb_private *dev_priv);
+extern int psb_grab_use_base(struct drm_psb_private *dev_priv,
+			     unsigned long dev_virtual,
+			     unsigned long size,
+			     unsigned int data_master,
+			     uint32_t fence_class,
+			     uint32_t fence_type,
+			     int no_wait,
+			     int ignore_signals,
+			     int *r_reg, uint32_t * r_offset);
+extern int psb_init_use_base(struct drm_psb_private *dev_priv,
+			     unsigned int reg_start, unsigned int reg_num);
+
+/*
+ * psb_xhw.c
+ */
+
+extern int psb_xhw_ioctl(struct drm_device *dev, void *data,
+			 struct drm_file *file_priv);
+extern int psb_xhw_init_ioctl(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv);
+extern int psb_xhw_init(struct drm_device *dev);
+extern void psb_xhw_takedown(struct drm_psb_private *dev_priv);
+extern void psb_xhw_init_takedown(struct drm_psb_private *dev_priv,
+				  struct drm_file *file_priv, int closing);
+extern int psb_xhw_scene_bind_fire(struct drm_psb_private *dev_priv,
+				   struct psb_xhw_buf *buf,
+				   uint32_t fire_flags,
+				   uint32_t hw_context,
+				   uint32_t * cookie,
+				   uint32_t * oom_cmds,
+				   uint32_t num_oom_cmds,
+				   uint32_t offset,
+				   uint32_t engine, uint32_t flags);
+extern int psb_xhw_fire_raster(struct drm_psb_private *dev_priv,
+			       struct psb_xhw_buf *buf, uint32_t fire_flags);
+extern int psb_xhw_scene_info(struct drm_psb_private *dev_priv,
+			      struct psb_xhw_buf *buf,
+			      uint32_t w,
+			      uint32_t h,
+			      uint32_t * hw_cookie,
+			      uint32_t * bo_size,
+			      uint32_t * clear_p_start,
+			      uint32_t * clear_num_pages);
+
+extern int psb_xhw_reset_dpm(struct drm_psb_private *dev_priv,
+			     struct psb_xhw_buf *buf);
+extern int psb_xhw_check_lockup(struct drm_psb_private *dev_priv,
+				struct psb_xhw_buf *buf, uint32_t * value);
+extern int psb_xhw_ta_mem_info(struct drm_psb_private *dev_priv,
+			       struct psb_xhw_buf *buf,
+			       uint32_t pages,
+			       uint32_t * hw_cookie, uint32_t * size);
+extern int psb_xhw_ta_oom(struct drm_psb_private *dev_priv,
+			  struct psb_xhw_buf *buf, uint32_t * cookie);
+extern void psb_xhw_ta_oom_reply(struct drm_psb_private *dev_priv,
+				 struct psb_xhw_buf *buf,
+				 uint32_t * cookie,
+				 uint32_t * bca,
+				 uint32_t * rca, uint32_t * flags);
+extern int psb_xhw_vistest(struct drm_psb_private *dev_priv,
+			   struct psb_xhw_buf *buf);
+extern int psb_xhw_handler(struct drm_psb_private *dev_priv);
+extern int psb_xhw_resume(struct drm_psb_private *dev_priv,
+			  struct psb_xhw_buf *buf);
+extern void psb_xhw_fire_reply(struct drm_psb_private *dev_priv,
+			       struct psb_xhw_buf *buf, uint32_t * cookie);
+extern int psb_xhw_ta_mem_load(struct drm_psb_private *dev_priv,
+			       struct psb_xhw_buf *buf,
+			       uint32_t flags,
+			       uint32_t param_offset,
+			       uint32_t pt_offset, uint32_t * hw_cookie);
+extern void psb_xhw_clean_buf(struct drm_psb_private *dev_priv,
+			      struct psb_xhw_buf *buf);
+
+/*
+ * psb_schedule.c: HW bug fixing.
+ */
+
+#ifdef FIX_TG_16
+
+extern void psb_2d_unlock(struct drm_psb_private *dev_priv);
+extern void psb_2d_lock(struct drm_psb_private *dev_priv);
+extern void psb_resume_ta_2d_idle(struct drm_psb_private *dev_priv);
+
+#else
+
+#define psb_2d_lock(_dev_priv) mutex_lock(&(_dev_priv)->mutex_2d)
+#define psb_2d_unlock(_dev_priv) mutex_unlock(&(_dev_priv)->mutex_2d)
+
+#endif
+
+/*
+ * Utilities
+ */
+
+#define PSB_ALIGN_TO(_val, _align) \
+  (((_val) + ((_align) - 1)) & ~((_align) - 1))
+#define PSB_WVDC32(_val, _offs) \
+  iowrite32(_val, dev_priv->vdc_reg + (_offs))
+#define PSB_RVDC32(_offs) \
+  ioread32(dev_priv->vdc_reg + (_offs))
+#define PSB_WSGX32(_val, _offs) \
+  iowrite32(_val, dev_priv->sgx_reg + (_offs))
+#define PSB_RSGX32(_offs) \
+  ioread32(dev_priv->sgx_reg + (_offs))
+#define PSB_WMSVDX32(_val, _offs) \
+  iowrite32(_val, dev_priv->msvdx_reg + (_offs))
+#define PSB_RMSVDX32(_offs) \
+  ioread32(dev_priv->msvdx_reg + (_offs))
+
+#define PSB_ALPL(_val, _base)			\
+  (((_val) >> (_base ## _ALIGNSHIFT)) << (_base ## _SHIFT))
+#define PSB_ALPLM(_val, _base)			\
+  ((((_val) >> (_base ## _ALIGNSHIFT)) << (_base ## _SHIFT)) & (_base ## _MASK))
+
+#define PSB_D_RENDER  (1 << 16)
+
+#define PSB_D_GENERAL (1 << 0)
+#define PSB_D_INIT    (1 << 1)
+#define PSB_D_IRQ     (1 << 2)
+#define PSB_D_FW      (1 << 3)
+#define PSB_D_PERF    (1 << 4)
+#define PSB_D_TMP    (1 << 5)
+#define PSB_D_RELOC   (1 << 6)
+
+extern int drm_psb_debug;
+extern int drm_psb_no_fb;
+extern int drm_psb_disable_vsync;
+
+#define PSB_DEBUG_FW(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_FW, _fmt, ##_arg)
+#define PSB_DEBUG_GENERAL(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_GENERAL, _fmt, ##_arg)
+#define PSB_DEBUG_INIT(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_INIT, _fmt, ##_arg)
+#define PSB_DEBUG_IRQ(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_IRQ, _fmt, ##_arg)
+#define PSB_DEBUG_RENDER(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_RENDER, _fmt, ##_arg)
+#define PSB_DEBUG_PERF(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_PERF, _fmt, ##_arg)
+#define PSB_DEBUG_TMP(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_TMP, _fmt, ##_arg)
+#define PSB_DEBUG_RELOC(_fmt, _arg...) \
+	PSB_DEBUG(PSB_D_RELOC, _fmt, ##_arg)
+
+#if DRM_DEBUG_CODE
+#define PSB_DEBUG(_flag, _fmt, _arg...)					\
+	do {								\
+	  if (unlikely((_flag) & drm_psb_debug))			\
+			printk(KERN_DEBUG				\
+			       "[psb:0x%02x:%s] " _fmt , _flag,	\
+			       __FUNCTION__ , ##_arg);			\
+	} while (0)
+#else
+#define PSB_DEBUG(_fmt, _arg...)     do { } while (0)
+#endif
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_fb.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_fb.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,1330 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/mm.h>
+#include <linux/tty.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/fb.h>
+#include <linux/init.h>
+#include <linux/console.h>
+
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+#include "psb_drv.h"
+
+#define SII_1392_WA
+#ifdef SII_1392_WA
+extern int SII_1392;
+#endif
+
+struct psbfb_vm_info {
+	struct drm_buffer_object *bo;
+	struct address_space *f_mapping;
+	struct mutex vm_mutex;
+	atomic_t refcount;
+};
+
+struct psbfb_par {
+	struct drm_device *dev;
+	struct drm_crtc *crtc;
+	struct drm_output *output;
+	struct psbfb_vm_info *vi;
+	int dpms_state;
+};
+
+static void psbfb_vm_info_deref(struct psbfb_vm_info **vi)
+{
+	struct psbfb_vm_info *tmp = *vi;
+	*vi = NULL;
+	if (atomic_dec_and_test(&tmp->refcount)) {
+		drm_bo_usage_deref_unlocked(&tmp->bo);
+		drm_free(tmp, sizeof(*tmp), DRM_MEM_MAPS);
+	}
+}
+
+static struct psbfb_vm_info *psbfb_vm_info_ref(struct psbfb_vm_info *vi)
+{
+	atomic_inc(&vi->refcount);
+	return vi;
+}
+
+static struct psbfb_vm_info *psbfb_vm_info_create(void)
+{
+	struct psbfb_vm_info *vi;
+
+	vi = drm_calloc(1, sizeof(*vi), DRM_MEM_MAPS);
+	if (!vi)
+		return NULL;
+
+	mutex_init(&vi->vm_mutex);
+	atomic_set(&vi->refcount, 1);
+	return vi;
+}
+
+#define CMAP_TOHW(_val, _width) ((((_val) << (_width)) + 0x7FFF - (_val)) >> 16)
+
+static int psbfb_setcolreg(unsigned regno, unsigned red, unsigned green,
+			   unsigned blue, unsigned transp, struct fb_info *info)
+{
+	struct psbfb_par *par = info->par;
+	struct drm_crtc *crtc = par->crtc;
+	uint32_t v;
+
+	if (!crtc->fb)
+		return -ENOMEM;
+
+	if (regno > 255)
+		return 1;
+
+	if (crtc->funcs->gamma_set)
+		crtc->funcs->gamma_set(crtc, red, green, blue, regno);
+
+	red = CMAP_TOHW(red, info->var.red.length);
+	blue = CMAP_TOHW(blue, info->var.blue.length);
+	green = CMAP_TOHW(green, info->var.green.length);
+	transp = CMAP_TOHW(transp, info->var.transp.length);
+
+	v = (red << info->var.red.offset) |
+	    (green << info->var.green.offset) |
+	    (blue << info->var.blue.offset) |
+	    (transp << info->var.transp.offset);
+
+	switch (crtc->fb->bits_per_pixel) {
+	case 16:
+		((uint32_t *) info->pseudo_palette)[regno] = v;
+		break;
+	case 24:
+	case 32:
+		((uint32_t *) info->pseudo_palette)[regno] = v;
+		break;
+	}
+
+	return 0;
+}
+
+static int psbfb_check_var(struct fb_var_screeninfo *var, struct fb_info *info)
+{
+	struct psbfb_par *par = info->par;
+	struct drm_device *dev = par->dev;
+	struct drm_framebuffer *fb = par->crtc->fb;
+	struct drm_display_mode *drm_mode;
+	struct drm_output *output;
+	int depth;
+	int pitch;
+	int bpp = var->bits_per_pixel;
+
+	if (!fb)
+		return -ENOMEM;
+
+	if (!var->pixclock)
+		return -EINVAL;
+
+	/* don't support virtuals for now */
+	if (var->xres_virtual > var->xres)
+		return -EINVAL;
+
+	if (var->yres_virtual > var->yres)
+		return -EINVAL;
+
+	switch (bpp) {
+	case 8:
+		depth = 8;
+		break;
+	case 16:
+		depth = (var->green.length == 6) ? 16 : 15;
+		break;
+	case 24:		/* assume this is 32bpp / depth 24 */
+		bpp = 32;
+		/* fallthrough */
+	case 32:
+		depth = (var->transp.length > 0) ? 32 : 24;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	pitch = ((var->xres * ((bpp + 1) / 8)) + 0x3f) & ~0x3f;
+
+	/* Check that we can resize */
+	if ((pitch * var->yres) > (fb->bo->num_pages << PAGE_SHIFT)) {
+#if 1
+		/* Need to resize the fb object.
+		 * But the generic fbdev code doesn't really understand
+		 * that we can do this. So disable for now.
+		 */
+		DRM_INFO("Can't support requested size, too big!\n");
+		return -EINVAL;
+#else
+		int ret;
+		struct drm_buffer_object *fbo = NULL;
+		struct drm_bo_kmap_obj tmp_kmap;
+
+		/* a temporary BO to check if we could resize in setpar.
+		 * Therefore no need to set NO_EVICT.
+		 */
+		ret = drm_buffer_object_create(dev,
+					       pitch * var->yres,
+					       drm_bo_type_kernel,
+					       DRM_BO_FLAG_READ |
+					       DRM_BO_FLAG_WRITE |
+					       DRM_BO_FLAG_MEM_TT |
+					       DRM_BO_FLAG_MEM_VRAM,
+					       DRM_BO_HINT_DONT_FENCE,
+					       0, 0, &fbo);
+		if (ret || !fbo)
+			return -ENOMEM;
+
+		ret = drm_bo_kmap(fbo, 0, fbo->num_pages, &tmp_kmap);
+		if (ret) {
+			drm_bo_usage_deref_unlocked(&fbo);
+			return -EINVAL;
+		}
+
+		drm_bo_kunmap(&tmp_kmap);
+		/* destroy our current fbo! */
+		drm_bo_usage_deref_unlocked(&fbo);
+#endif
+	}
+
+	switch (depth) {
+	case 8:
+		var->red.offset = 0;
+		var->green.offset = 0;
+		var->blue.offset = 0;
+		var->red.length = 8;
+		var->green.length = 8;
+		var->blue.length = 8;
+		var->transp.length = 0;
+		var->transp.offset = 0;
+		break;
+	case 15:
+		var->red.offset = 10;
+		var->green.offset = 5;
+		var->blue.offset = 0;
+		var->red.length = 5;
+		var->green.length = 5;
+		var->blue.length = 5;
+		var->transp.length = 1;
+		var->transp.offset = 15;
+		break;
+	case 16:
+		var->red.offset = 11;
+		var->green.offset = 5;
+		var->blue.offset = 0;
+		var->red.length = 5;
+		var->green.length = 6;
+		var->blue.length = 5;
+		var->transp.length = 0;
+		var->transp.offset = 0;
+		break;
+	case 24:
+		var->red.offset = 16;
+		var->green.offset = 8;
+		var->blue.offset = 0;
+		var->red.length = 8;
+		var->green.length = 8;
+		var->blue.length = 8;
+		var->transp.length = 0;
+		var->transp.offset = 0;
+		break;
+	case 32:
+		var->red.offset = 16;
+		var->green.offset = 8;
+		var->blue.offset = 0;
+		var->red.length = 8;
+		var->green.length = 8;
+		var->blue.length = 8;
+		var->transp.length = 8;
+		var->transp.offset = 24;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+#if 0
+	/* Here we walk the output mode list and look for modes. If we haven't
+	 * got it, then bail. Not very nice, so this is disabled.
+	 * In the set_par code, we create our mode based on the incoming
+	 * parameters. Nicer, but may not be desired by some.
+	 */
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		if (output->crtc == par->crtc)
+			break;
+	}
+
+	list_for_each_entry(drm_mode, &output->modes, head) {
+		if (drm_mode->hdisplay == var->xres &&
+		    drm_mode->vdisplay == var->yres && drm_mode->clock != 0)
+			break;
+	}
+
+	if (!drm_mode)
+		return -EINVAL;
+#else
+	(void)dev;		/* silence warnings */
+	(void)output;
+	(void)drm_mode;
+#endif
+
+	return 0;
+}
+
+static int psbfb_move_fb_bo(struct fb_info *info, struct drm_buffer_object *bo,
+			    uint64_t mem_type_flags)
+{
+	struct psbfb_par *par;
+	loff_t holelen;
+	int ret;
+
+	/*
+	 * Kill all user-space mappings of this device. They will be
+	 * faulted back using nopfn when accessed.
+	 */
+
+	par = info->par;
+	holelen = ((loff_t) bo->mem.num_pages) << PAGE_SHIFT;
+	mutex_lock(&par->vi->vm_mutex);
+	if (par->vi->f_mapping) {
+		unmap_mapping_range(par->vi->f_mapping, 0, holelen, 1);
+	}
+
+	ret = drm_bo_do_validate(bo,
+				 mem_type_flags,
+				 DRM_BO_MASK_MEM |
+				 DRM_BO_FLAG_NO_EVICT,
+				 DRM_BO_HINT_DONT_FENCE, 0, 1, NULL);
+
+	mutex_unlock(&par->vi->vm_mutex);
+	return ret;
+}
+
+/* this will let fbcon do the mode init */
+static int psbfb_set_par(struct fb_info *info)
+{
+	struct psbfb_par *par = info->par;
+	struct drm_framebuffer *fb = par->crtc->fb;
+	struct drm_device *dev = par->dev;
+	struct drm_display_mode *drm_mode;
+	struct fb_var_screeninfo *var = &info->var;
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	struct drm_output *output;
+	int pitch;
+	int depth;
+	int bpp = var->bits_per_pixel;
+
+	if (!fb)
+		return -ENOMEM;
+
+	switch (bpp) {
+	case 8:
+		depth = 8;
+		break;
+	case 16:
+		depth = (var->green.length == 6) ? 16 : 15;
+		break;
+	case 24:		/* assume this is 32bpp / depth 24 */
+		bpp = 32;
+		/* fallthrough */
+	case 32:
+		depth = (var->transp.length > 0) ? 32 : 24;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	pitch = ((var->xres * ((bpp + 1) / 8)) + 0x3f) & ~0x3f;
+
+	if ((pitch * var->yres) > (fb->bo->num_pages << PAGE_SHIFT)) {
+#if 1
+		/* Need to resize the fb object.
+		 * But the generic fbdev code doesn't really understand
+		 * that we can do this. So disable for now.
+		 */
+		DRM_INFO("Can't support requested size, too big!\n");
+		return -EINVAL;
+#else
+		int ret;
+		struct drm_buffer_object *fbo = NULL, *tfbo;
+		struct drm_bo_kmap_obj tmp_kmap, tkmap;
+
+		ret = drm_buffer_object_create(dev,
+					       pitch * var->yres,
+					       drm_bo_type_kernel,
+					       DRM_BO_FLAG_READ |
+					       DRM_BO_FLAG_WRITE |
+					       DRM_BO_FLAG_MEM_TT |
+					       DRM_BO_FLAG_MEM_VRAM |
+					       DRM_BO_FLAG_NO_EVICT,
+					       DRM_BO_HINT_DONT_FENCE,
+					       0, 0, &fbo);
+		if (ret || !fbo) {
+			DRM_ERROR
+			    ("failed to allocate new resized framebuffer\n");
+			return -ENOMEM;
+		}
+
+		ret = drm_bo_kmap(fbo, 0, fbo->num_pages, &tmp_kmap);
+		if (ret) {
+			DRM_ERROR("failed to kmap framebuffer.\n");
+			drm_bo_usage_deref_unlocked(&fbo);
+			return -EINVAL;
+		}
+
+		DRM_DEBUG("allocated %dx%d fb: 0x%08lx, bo %p\n", fb->width,
+			  fb->height, fb->offset, fbo);
+
+		/* set new screen base */
+		info->screen_base = tmp_kmap.virtual;
+
+		tkmap = fb->kmap;
+		fb->kmap = tmp_kmap;
+		drm_bo_kunmap(&tkmap);
+
+		tfbo = fb->bo;
+		fb->bo = fbo;
+		drm_bo_usage_deref_unlocked(&tfbo);
+#endif
+	}
+
+	fb->offset = fb->bo->offset - dev_priv->pg->gatt_start;
+	fb->width = var->xres;
+	fb->height = var->yres;
+	fb->bits_per_pixel = bpp;
+	fb->pitch = pitch;
+	fb->depth = depth;
+
+	info->fix.line_length = fb->pitch;
+	info->fix.visual =
+	    (fb->depth == 8) ? FB_VISUAL_PSEUDOCOLOR : FB_VISUAL_DIRECTCOLOR;
+
+	/* some fbdev's apps don't want these to change */
+	info->fix.smem_start = dev->mode_config.fb_base + fb->offset;
+
+	/* we have to align the output base address because the fb->bo
+	   may be moved in the previous drm_bo_do_validate().
+	   Otherwise the output screens may go black when exit the X
+	   window and re-enter the console */
+	info->screen_base = fb->kmap.virtual;
+
+#if 0
+	/* relates to resize - disable */
+	info->fix.smem_len = info->fix.line_length * var->yres;
+	info->screen_size = info->fix.smem_len;	/* ??? */
+#endif
+
+	/* Should we walk the output's modelist or just create our own ???
+	 * For now, we create and destroy a mode based on the incoming
+	 * parameters. But there's commented out code below which scans
+	 * the output list too.
+	 */
+#if 0
+	list_for_each_entry(output, &dev->mode_config.output_list, head) {
+		if (output->crtc == par->crtc)
+			break;
+	}
+
+	list_for_each_entry(drm_mode, &output->modes, head) {
+		if (drm_mode->hdisplay == var->xres &&
+		    drm_mode->vdisplay == var->yres && drm_mode->clock != 0)
+			break;
+	}
+#else
+	(void)output;		/* silence warning */
+
+	drm_mode = drm_mode_create(dev);
+	drm_mode->hdisplay = var->xres;
+	drm_mode->hsync_start = drm_mode->hdisplay + var->right_margin;
+	drm_mode->hsync_end = drm_mode->hsync_start + var->hsync_len;
+	drm_mode->htotal = drm_mode->hsync_end + var->left_margin;
+	drm_mode->vdisplay = var->yres;
+	drm_mode->vsync_start = drm_mode->vdisplay + var->lower_margin;
+	drm_mode->vsync_end = drm_mode->vsync_start + var->vsync_len;
+	drm_mode->vtotal = drm_mode->vsync_end + var->upper_margin;
+	drm_mode->clock = PICOS2KHZ(var->pixclock);
+	drm_mode->vrefresh = drm_mode_vrefresh(drm_mode);
+	drm_mode_set_name(drm_mode);
+	drm_mode_set_crtcinfo(drm_mode, CRTC_INTERLACE_HALVE_V);
+#endif
+
+	if (!drm_crtc_set_mode(par->crtc, drm_mode, 0, 0))
+		return -EINVAL;
+
+	/* Have to destroy our created mode if we're not searching the mode
+	 * list for it.
+	 */
+#if 1
+	drm_mode_destroy(dev, drm_mode);
+#endif
+
+	return 0;
+}
+
+extern int psb_2d_submit(struct drm_psb_private *, uint32_t *, uint32_t);;
+
+static int psb_accel_2d_fillrect(struct drm_psb_private *dev_priv,
+				 uint32_t dst_offset, uint32_t dst_stride,
+				 uint32_t dst_format, uint16_t dst_x,
+				 uint16_t dst_y, uint16_t size_x,
+				 uint16_t size_y, uint32_t fill)
+{
+	uint32_t buffer[10];
+	uint32_t *buf;
+	int ret;
+
+	buf = buffer;
+
+	*buf++ = PSB_2D_FENCE_BH;
+
+	*buf++ =
+	    PSB_2D_DST_SURF_BH | dst_format | (dst_stride <<
+					       PSB_2D_DST_STRIDE_SHIFT);
+	*buf++ = dst_offset;
+
+	*buf++ =
+	    PSB_2D_BLIT_BH |
+	    PSB_2D_ROT_NONE |
+	    PSB_2D_COPYORDER_TL2BR |
+	    PSB_2D_DSTCK_DISABLE |
+	    PSB_2D_SRCCK_DISABLE | PSB_2D_USE_FILL | PSB_2D_ROP3_PATCOPY;
+
+	*buf++ = fill << PSB_2D_FILLCOLOUR_SHIFT;
+	*buf++ =
+	    (dst_x << PSB_2D_DST_XSTART_SHIFT) | (dst_y <<
+						  PSB_2D_DST_YSTART_SHIFT);
+	*buf++ =
+	    (size_x << PSB_2D_DST_XSIZE_SHIFT) | (size_y <<
+						  PSB_2D_DST_YSIZE_SHIFT);
+	*buf++ = PSB_2D_FLUSH_BH;
+
+	psb_2d_lock(dev_priv);
+	ret = psb_2d_submit(dev_priv, buffer, buf - buffer);
+	psb_2d_unlock(dev_priv);
+
+	return ret;
+}
+
+static void psbfb_fillrect_accel(struct fb_info *info,
+				 const struct fb_fillrect *r)
+{
+	struct psbfb_par *par = info->par;
+	struct drm_framebuffer *fb = par->crtc->fb;
+	struct drm_psb_private *dev_priv = par->dev->dev_private;
+	uint32_t offset;
+	uint32_t stride;
+	uint32_t format;
+
+	if (!fb)
+		return;
+
+	offset = fb->offset;
+	stride = fb->pitch;
+
+	switch (fb->depth) {
+	case 8:
+		format = PSB_2D_DST_332RGB;
+		break;
+	case 15:
+		format = PSB_2D_DST_555RGB;
+		break;
+	case 16:
+		format = PSB_2D_DST_565RGB;
+		break;
+	case 24:
+	case 32:
+		/* this is wrong but since we don't do blending its okay */
+		format = PSB_2D_DST_8888ARGB;
+		break;
+	default:
+		/* software fallback */
+		cfb_fillrect(info, r);
+		return;
+	}
+
+	psb_accel_2d_fillrect(dev_priv,
+			      offset, stride, format,
+			      r->dx, r->dy, r->width, r->height, r->color);
+}
+
+static void psbfb_fillrect(struct fb_info *info, const struct fb_fillrect *rect)
+{
+	if (info->state != FBINFO_STATE_RUNNING)
+		return;
+	if (info->flags & FBINFO_HWACCEL_DISABLED) {
+		cfb_fillrect(info, rect);
+		return;
+	}
+	if (in_interrupt() || in_atomic()) {
+		/*
+		 * Catch case when we're shutting down.
+		 */
+		cfb_fillrect(info, rect);
+		return;
+	}
+	psbfb_fillrect_accel(info, rect);
+}
+
+uint32_t psb_accel_2d_copy_direction(int xdir, int ydir)
+{
+	if (xdir < 0)
+		return ((ydir <
+			 0) ? PSB_2D_COPYORDER_BR2TL : PSB_2D_COPYORDER_TR2BL);
+	else
+		return ((ydir <
+			 0) ? PSB_2D_COPYORDER_BL2TR : PSB_2D_COPYORDER_TL2BR);
+}
+
+/*
+ * @srcOffset in bytes
+ * @srcStride in bytes
+ * @srcFormat psb 2D format defines
+ * @dstOffset in bytes
+ * @dstStride in bytes
+ * @dstFormat psb 2D format defines
+ * @srcX offset in pixels
+ * @srcY offset in pixels
+ * @dstX offset in pixels
+ * @dstY offset in pixels
+ * @sizeX of the copied area
+ * @sizeY of the copied area
+ */
+static int psb_accel_2d_copy(struct drm_psb_private *dev_priv,
+			     uint32_t src_offset, uint32_t src_stride,
+			     uint32_t src_format, uint32_t dst_offset,
+			     uint32_t dst_stride, uint32_t dst_format,
+			     uint16_t src_x, uint16_t src_y, uint16_t dst_x,
+			     uint16_t dst_y, uint16_t size_x, uint16_t size_y)
+{
+	uint32_t blit_cmd;
+	uint32_t buffer[10];
+	uint32_t *buf;
+	uint32_t direction;
+	int ret;
+
+	buf = buffer;
+
+	direction = psb_accel_2d_copy_direction(src_x - dst_x, src_y - dst_y);
+
+	if (direction == PSB_2D_COPYORDER_BR2TL ||
+	    direction == PSB_2D_COPYORDER_TR2BL) {
+		src_x += size_x - 1;
+		dst_x += size_x - 1;
+	}
+	if (direction == PSB_2D_COPYORDER_BR2TL ||
+	    direction == PSB_2D_COPYORDER_BL2TR) {
+		src_y += size_y - 1;
+		dst_y += size_y - 1;
+	}
+
+	blit_cmd =
+	    PSB_2D_BLIT_BH |
+	    PSB_2D_ROT_NONE |
+	    PSB_2D_DSTCK_DISABLE |
+	    PSB_2D_SRCCK_DISABLE |
+	    PSB_2D_USE_PAT | PSB_2D_ROP3_SRCCOPY | direction;
+
+	*buf++ = PSB_2D_FENCE_BH;
+	*buf++ =
+	    PSB_2D_DST_SURF_BH | dst_format | (dst_stride <<
+					       PSB_2D_DST_STRIDE_SHIFT);
+	*buf++ = dst_offset;
+	*buf++ =
+	    PSB_2D_SRC_SURF_BH | src_format | (src_stride <<
+					       PSB_2D_SRC_STRIDE_SHIFT);
+	*buf++ = src_offset;
+	*buf++ =
+	    PSB_2D_SRC_OFF_BH | (src_x << PSB_2D_SRCOFF_XSTART_SHIFT) | (src_y
+									 <<
+									 PSB_2D_SRCOFF_YSTART_SHIFT);
+	*buf++ = blit_cmd;
+	*buf++ =
+	    (dst_x << PSB_2D_DST_XSTART_SHIFT) | (dst_y <<
+						  PSB_2D_DST_YSTART_SHIFT);
+	*buf++ =
+	    (size_x << PSB_2D_DST_XSIZE_SHIFT) | (size_y <<
+						  PSB_2D_DST_YSIZE_SHIFT);
+	*buf++ = PSB_2D_FLUSH_BH;
+
+	psb_2d_lock(dev_priv);
+	ret = psb_2d_submit(dev_priv, buffer, buf - buffer);
+	psb_2d_unlock(dev_priv);
+	return ret;
+}
+
+static void psbfb_copyarea_accel(struct fb_info *info,
+				 const struct fb_copyarea *a)
+{
+	struct psbfb_par *par = info->par;
+	struct drm_framebuffer *fb = par->crtc->fb;
+	struct drm_psb_private *dev_priv = par->dev->dev_private;
+	uint32_t offset;
+	uint32_t stride;
+	uint32_t src_format;
+	uint32_t dst_format;
+
+	if (!fb)
+		return;
+
+	offset = fb->offset;
+	stride = fb->pitch;
+
+	if (a->width == 8 || a->height == 8) {
+		psb_2d_lock(dev_priv);
+		psb_idle_2d(par->dev);
+		psb_2d_unlock(dev_priv);
+		cfb_copyarea(info, a);
+		return;
+	}
+
+	switch (fb->depth) {
+	case 8:
+		src_format = PSB_2D_SRC_332RGB;
+		dst_format = PSB_2D_DST_332RGB;
+		break;
+	case 15:
+		src_format = PSB_2D_SRC_555RGB;
+		dst_format = PSB_2D_DST_555RGB;
+		break;
+	case 16:
+		src_format = PSB_2D_SRC_565RGB;
+		dst_format = PSB_2D_DST_565RGB;
+		break;
+	case 24:
+	case 32:
+		/* this is wrong but since we don't do blending its okay */
+		src_format = PSB_2D_SRC_8888ARGB;
+		dst_format = PSB_2D_DST_8888ARGB;
+		break;
+	default:
+		/* software fallback */
+		cfb_copyarea(info, a);
+		return;
+	}
+
+	psb_accel_2d_copy(dev_priv,
+			  offset, stride, src_format,
+			  offset, stride, dst_format,
+			  a->sx, a->sy, a->dx, a->dy, a->width, a->height);
+}
+
+static void psbfb_copyarea(struct fb_info *info,
+			   const struct fb_copyarea *region)
+{
+	if (info->state != FBINFO_STATE_RUNNING)
+		return;
+	if (info->flags & FBINFO_HWACCEL_DISABLED) {
+		cfb_copyarea(info, region);
+		return;
+	}
+	if (in_interrupt() || in_atomic()) {
+		/*
+		 * Catch case when we're shutting down.
+		 */
+		cfb_copyarea(info, region);
+		return;
+	}
+
+	psbfb_copyarea_accel(info, region);
+}
+
+void psbfb_imageblit(struct fb_info *info, const struct fb_image *image)
+{
+	if (info->state != FBINFO_STATE_RUNNING)
+		return;
+	if (info->flags & FBINFO_HWACCEL_DISABLED) {
+		cfb_imageblit(info, image);
+		return;
+	}
+	if (in_interrupt() || in_atomic()) {
+		cfb_imageblit(info, image);
+		return;
+	}
+
+	cfb_imageblit(info, image);
+}
+
+static int psbfb_blank(int blank_mode, struct fb_info *info)
+{
+	int dpms_mode;
+	struct psbfb_par *par = info->par;
+	struct drm_output *output;
+
+	par->dpms_state = blank_mode;
+
+	switch(blank_mode) {
+	case FB_BLANK_UNBLANK:
+		dpms_mode = DPMSModeOn;
+		break;
+	case FB_BLANK_NORMAL:
+		if (!par->crtc)
+			return 0;
+		(*par->crtc->funcs->dpms)(par->crtc, DPMSModeStandby);
+		return 0;
+	case FB_BLANK_HSYNC_SUSPEND:
+	default:
+		dpms_mode = DPMSModeStandby;
+		break;
+	case FB_BLANK_VSYNC_SUSPEND:
+		dpms_mode = DPMSModeSuspend;
+		break;
+	case FB_BLANK_POWERDOWN:
+		dpms_mode = DPMSModeOff;
+		break;
+	}
+
+	if (!par->crtc)
+		return 0;
+
+	list_for_each_entry(output, &par->dev->mode_config.output_list, head) {
+		if (output->crtc == par->crtc)
+			(*output->funcs->dpms)(output, dpms_mode);
+	}
+
+	(*par->crtc->funcs->dpms)(par->crtc, dpms_mode);
+	return 0;
+}
+
+
+static int psbfb_kms_off(struct drm_device *dev, int suspend)
+{
+	struct drm_framebuffer *fb = 0;
+	struct drm_buffer_object *bo = 0;
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	int ret = 0;
+
+	DRM_DEBUG("psbfb_kms_off_ioctl\n");
+
+	mutex_lock(&dev->mode_config.mutex);
+	list_for_each_entry(fb, &dev->mode_config.fb_list, head) {
+		struct fb_info *info = fb->fbdev;
+		struct psbfb_par *par = info->par;
+		int save_dpms_state;
+
+		if (suspend)
+			fb_set_suspend(info, 1);
+		else
+			info->state &= ~FBINFO_STATE_RUNNING;
+
+		info->screen_base = NULL;
+
+		bo = fb->bo;
+
+		if (!bo)
+			continue;
+
+		drm_bo_kunmap(&fb->kmap);
+
+		/*
+		 * We don't take the 2D lock here as we assume that the
+		 * 2D engine will eventually idle anyway.
+		 */
+
+		if (!suspend) {
+			uint32_t dummy2 = 0;
+			(void) psb_fence_emit_sequence(dev, PSB_ENGINE_2D, 0,
+							&dummy2, &dummy2);
+			psb_2d_lock(dev_priv);
+			(void)psb_idle_2d(dev);
+			psb_2d_unlock(dev_priv);
+		} else
+			psb_idle_2d(dev);
+
+		save_dpms_state = par->dpms_state;
+		psbfb_blank(FB_BLANK_NORMAL, info);
+		par->dpms_state = save_dpms_state;
+
+		ret = psbfb_move_fb_bo(info, bo, DRM_BO_FLAG_MEM_LOCAL);
+
+		if (ret)
+			goto out_err;
+	}
+      out_err:
+	mutex_unlock(&dev->mode_config.mutex);
+
+	return ret;
+}
+
+int psbfb_kms_off_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file_priv)
+{
+	int ret;
+
+	acquire_console_sem();
+	ret = psbfb_kms_off(dev, 0);
+	release_console_sem();
+
+	return ret;
+}
+
+static int psbfb_kms_on(struct drm_device *dev, int resume)
+{
+	struct drm_framebuffer *fb = 0;
+	struct drm_buffer_object *bo = 0;
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	int ret = 0;
+	int dummy;
+
+	DRM_DEBUG("psbfb_kms_on_ioctl\n");
+
+	if (!resume) {
+		uint32_t dummy2 = 0;
+		(void) psb_fence_emit_sequence(dev, PSB_ENGINE_2D, 0,
+						       &dummy2, &dummy2);
+		psb_2d_lock(dev_priv);
+		(void)psb_idle_2d(dev);
+		psb_2d_unlock(dev_priv);
+	} else
+		psb_idle_2d(dev);
+
+	mutex_lock(&dev->mode_config.mutex);
+	list_for_each_entry(fb, &dev->mode_config.fb_list, head) {
+		struct fb_info *info = fb->fbdev;
+		struct psbfb_par *par = info->par;
+
+		bo = fb->bo;
+		if (!bo)
+			continue;
+
+		ret = psbfb_move_fb_bo(info, bo,
+				       DRM_BO_FLAG_MEM_TT |
+				       DRM_BO_FLAG_MEM_VRAM |
+				       DRM_BO_FLAG_NO_EVICT);
+		if (ret)
+			goto out_err;
+
+		ret = drm_bo_kmap(bo, 0, bo->num_pages, &fb->kmap);
+		if (ret)
+			goto out_err;
+
+		info->screen_base = drm_bmo_virtual(&fb->kmap, &dummy);
+		fb->offset = bo->offset - dev_priv->pg->gatt_start;
+
+		if (ret)
+			goto out_err;
+
+		if (resume)
+			fb_set_suspend(info, 0);
+		else
+			info->state |= FBINFO_STATE_RUNNING;
+
+		/*
+		 * Re-run modesetting here, since the VDS scanout offset may
+		 * have changed.
+		 */
+
+		if (par->crtc->enabled) {
+			psbfb_set_par(info);
+			psbfb_blank(par->dpms_state, info);
+		}
+	}
+      out_err:
+	mutex_unlock(&dev->mode_config.mutex);
+
+	return ret;
+}
+
+int psbfb_kms_on_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv)
+{
+	int ret;
+
+	acquire_console_sem();
+	ret = psbfb_kms_on(dev, 0);
+	release_console_sem();
+#ifdef SII_1392_WA
+	if((SII_1392 != 1) || (drm_psb_no_fb==0))
+		drm_disable_unused_functions(dev);
+#else
+	drm_disable_unused_functions(dev);
+#endif
+	return ret;
+}
+
+void psbfb_suspend(struct drm_device *dev)
+{
+	acquire_console_sem();
+	psbfb_kms_off(dev, 1);
+	release_console_sem();
+}
+
+void psbfb_resume(struct drm_device *dev)
+{
+	acquire_console_sem();
+	psbfb_kms_on(dev, 1);
+	release_console_sem();
+#ifdef SII_1392_WA
+	if((SII_1392 != 1) || (drm_psb_no_fb==0))
+		drm_disable_unused_functions(dev);
+#else
+	drm_disable_unused_functions(dev);
+#endif
+}
+
+/*
+ * FIXME: Before kernel inclusion, migrate nopfn to fault.
+ * Also, these should be the default vm ops for buffer object type fbs.
+ */
+
+extern unsigned long drm_bo_vm_fault(struct vm_area_struct *vma,
+				     struct vm_fault *vmf);
+
+/*
+ * This wrapper is a bit ugly and is here because we need access to a mutex
+ * that we can lock both around nopfn and around unmap_mapping_range + move.
+ * Normally, this would've been done using the bo mutex, but unfortunately
+ * we cannot lock it around drm_bo_do_validate(), since that would imply
+ * recursive locking.
+ */
+
+static int psbfb_fault(struct vm_area_struct *vma,
+				 struct vm_fault *vmf)
+{
+	struct psbfb_vm_info *vi = (struct psbfb_vm_info *)vma->vm_private_data;
+	struct vm_area_struct tmp_vma;
+	int ret;
+
+	mutex_lock(&vi->vm_mutex);
+	tmp_vma = *vma;
+	tmp_vma.vm_private_data = vi->bo;
+	ret = drm_bo_vm_fault(&tmp_vma, vmf);
+	mutex_unlock(&vi->vm_mutex);
+	return ret;
+}
+
+static void psbfb_vm_open(struct vm_area_struct *vma)
+{
+	struct psbfb_vm_info *vi = (struct psbfb_vm_info *)vma->vm_private_data;
+
+	atomic_inc(&vi->refcount);
+}
+
+static void psbfb_vm_close(struct vm_area_struct *vma)
+{
+	psbfb_vm_info_deref((struct psbfb_vm_info **)&vma->vm_private_data);
+}
+
+static struct vm_operations_struct psbfb_vm_ops = {
+	.fault = psbfb_fault,
+	.open = psbfb_vm_open,
+	.close = psbfb_vm_close,
+};
+
+static int psbfb_mmap(struct fb_info *info, struct vm_area_struct *vma)
+{
+	struct psbfb_par *par = info->par;
+	struct drm_framebuffer *fb = par->crtc->fb;
+	struct drm_buffer_object *bo = fb->bo;
+	unsigned long size = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+	unsigned long offset = vma->vm_pgoff;
+
+	if (vma->vm_pgoff != 0)
+		return -EINVAL;
+	if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT))
+		return -EINVAL;
+	if (offset + size > bo->num_pages)
+		return -EINVAL;
+
+	mutex_lock(&par->vi->vm_mutex);
+	if (!par->vi->f_mapping)
+		par->vi->f_mapping = vma->vm_file->f_mapping;
+	mutex_unlock(&par->vi->vm_mutex);
+
+	vma->vm_private_data = psbfb_vm_info_ref(par->vi);
+
+	vma->vm_ops = &psbfb_vm_ops;
+	vma->vm_flags |= VM_PFNMAP;
+
+	return 0;
+}
+
+int psbfb_sync(struct fb_info *info)
+{
+	struct psbfb_par *par = info->par;
+	struct drm_psb_private *dev_priv = par->dev->dev_private;
+
+	psb_2d_lock(dev_priv);
+	psb_idle_2d(par->dev);
+	psb_2d_unlock(dev_priv);
+
+	return 0;
+}
+
+static struct fb_ops psbfb_ops = {
+	.owner = THIS_MODULE,
+	.fb_check_var = psbfb_check_var,
+	.fb_set_par = psbfb_set_par,
+	.fb_setcolreg = psbfb_setcolreg,
+	.fb_fillrect = psbfb_fillrect,
+	.fb_copyarea = psbfb_copyarea,
+	.fb_imageblit = psbfb_imageblit,
+	.fb_mmap = psbfb_mmap,
+	.fb_sync = psbfb_sync,
+	.fb_blank = psbfb_blank,
+};
+
+int psbfb_probe(struct drm_device *dev, struct drm_crtc *crtc)
+{
+	struct fb_info *info;
+	struct psbfb_par *par;
+	struct device *device = &dev->pdev->dev;
+	struct drm_framebuffer *fb;
+	struct drm_display_mode *mode = crtc->desired_mode;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_buffer_object *fbo = NULL;
+	int ret;
+	int is_iomem;
+
+	if (drm_psb_no_fb) {
+		/* need to do this as the DRM will disable the output */
+		crtc->enabled = 1;
+		return 0;
+	}
+
+	info = framebuffer_alloc(sizeof(struct psbfb_par), device);
+	if (!info) {
+		return -ENOMEM;
+	}
+
+	fb = drm_framebuffer_create(dev);
+	if (!fb) {
+		framebuffer_release(info);
+		DRM_ERROR("failed to allocate fb.\n");
+		return -ENOMEM;
+	}
+	crtc->fb = fb;
+
+	fb->width = mode->hdisplay;
+	fb->height = mode->vdisplay;
+
+	fb->bits_per_pixel = 32;
+	fb->depth = 24;
+	fb->pitch =
+	    ((fb->width * ((fb->bits_per_pixel + 1) / 8)) + 0x3f) & ~0x3f;
+
+	ret = drm_buffer_object_create(dev,
+				       fb->pitch * fb->height,
+				       drm_bo_type_kernel,
+				       DRM_BO_FLAG_READ |
+				       DRM_BO_FLAG_WRITE |
+				       DRM_BO_FLAG_MEM_TT |
+				       DRM_BO_FLAG_MEM_VRAM |
+				       DRM_BO_FLAG_NO_EVICT,
+				       DRM_BO_HINT_DONT_FENCE, 0, 0, &fbo);
+	if (ret || !fbo) {
+		DRM_ERROR("failed to allocate framebuffer\n");
+		goto out_err0;
+	}
+
+	fb->offset = fbo->offset - dev_priv->pg->gatt_start;
+	fb->bo = fbo;
+	DRM_DEBUG("allocated %dx%d fb: 0x%08lx, bo %p\n", fb->width,
+		  fb->height, fb->offset, fbo);
+
+	fb->fbdev = info;
+
+	par = info->par;
+
+	par->dev = dev;
+	par->crtc = crtc;
+	par->vi = psbfb_vm_info_create();
+	if (!par->vi)
+		goto out_err1;
+
+	mutex_lock(&dev->struct_mutex);
+	par->vi->bo = fbo;
+	atomic_inc(&fbo->usage);
+	mutex_unlock(&dev->struct_mutex);
+
+	par->vi->f_mapping = NULL;
+	info->fbops = &psbfb_ops;
+
+	strcpy(info->fix.id, "psbfb");
+	info->fix.type = FB_TYPE_PACKED_PIXELS;
+	info->fix.visual = FB_VISUAL_DIRECTCOLOR;
+	info->fix.type_aux = 0;
+	info->fix.xpanstep = 1;
+	info->fix.ypanstep = 1;
+	info->fix.ywrapstep = 0;
+	info->fix.accel = FB_ACCEL_NONE;	/* ??? */
+	info->fix.type_aux = 0;
+	info->fix.mmio_start = 0;
+	info->fix.mmio_len = 0;
+	info->fix.line_length = fb->pitch;
+	info->fix.smem_start = dev->mode_config.fb_base + fb->offset;
+	info->fix.smem_len = info->fix.line_length * fb->height;
+
+	info->flags = FBINFO_DEFAULT |
+	    FBINFO_PARTIAL_PAN_OK /*| FBINFO_MISC_ALWAYS_SETPAR */ ;
+
+	ret = drm_bo_kmap(fb->bo, 0, fb->bo->num_pages, &fb->kmap);
+	if (ret) {
+		DRM_ERROR("error mapping fb: %d\n", ret);
+		goto out_err2;
+	}
+
+	info->screen_base = drm_bmo_virtual(&fb->kmap, &is_iomem);
+	memset(info->screen_base, 0x00, fb->pitch*fb->height);
+	info->screen_size = info->fix.smem_len;	/* FIXME */
+	info->pseudo_palette = fb->pseudo_palette;
+	info->var.xres_virtual = fb->width;
+	info->var.yres_virtual = fb->height;
+	info->var.bits_per_pixel = fb->bits_per_pixel;
+	info->var.xoffset = 0;
+	info->var.yoffset = 0;
+	info->var.activate = FB_ACTIVATE_NOW;
+	info->var.height = -1;
+	info->var.width = -1;
+	info->var.vmode = FB_VMODE_NONINTERLACED;
+
+	info->var.xres = mode->hdisplay;
+	info->var.right_margin = mode->hsync_start - mode->hdisplay;
+	info->var.hsync_len = mode->hsync_end - mode->hsync_start;
+	info->var.left_margin = mode->htotal - mode->hsync_end;
+	info->var.yres = mode->vdisplay;
+	info->var.lower_margin = mode->vsync_start - mode->vdisplay;
+	info->var.vsync_len = mode->vsync_end - mode->vsync_start;
+	info->var.upper_margin = mode->vtotal - mode->vsync_end;
+	info->var.pixclock = 10000000 / mode->htotal * 1000 /
+	    mode->vtotal * 100;
+	/* avoid overflow */
+	info->var.pixclock = info->var.pixclock * 1000 / mode->vrefresh;
+
+	info->pixmap.size = 64 * 1024;
+	info->pixmap.buf_align = 8;
+	info->pixmap.access_align = 32;
+	info->pixmap.flags = FB_PIXMAP_SYSTEM;
+	info->pixmap.scan_align = 1;
+
+	DRM_DEBUG("fb depth is %d\n", fb->depth);
+	DRM_DEBUG("   pitch is %d\n", fb->pitch);
+	switch (fb->depth) {
+	case 8:
+		info->var.red.offset = 0;
+		info->var.green.offset = 0;
+		info->var.blue.offset = 0;
+		info->var.red.length = 8;	/* 8bit DAC */
+		info->var.green.length = 8;
+		info->var.blue.length = 8;
+		info->var.transp.offset = 0;
+		info->var.transp.length = 0;
+		break;
+	case 15:
+		info->var.red.offset = 10;
+		info->var.green.offset = 5;
+		info->var.blue.offset = 0;
+		info->var.red.length = info->var.green.length =
+		    info->var.blue.length = 5;
+		info->var.transp.offset = 15;
+		info->var.transp.length = 1;
+		break;
+	case 16:
+		info->var.red.offset = 11;
+		info->var.green.offset = 5;
+		info->var.blue.offset = 0;
+		info->var.red.length = 5;
+		info->var.green.length = 6;
+		info->var.blue.length = 5;
+		info->var.transp.offset = 0;
+		break;
+	case 24:
+		info->var.red.offset = 16;
+		info->var.green.offset = 8;
+		info->var.blue.offset = 0;
+		info->var.red.length = info->var.green.length =
+		    info->var.blue.length = 8;
+		info->var.transp.offset = 0;
+		info->var.transp.length = 0;
+		break;
+	case 32:
+		info->var.red.offset = 16;
+		info->var.green.offset = 8;
+		info->var.blue.offset = 0;
+		info->var.red.length = info->var.green.length =
+		    info->var.blue.length = 8;
+		info->var.transp.offset = 24;
+		info->var.transp.length = 8;
+		break;
+	default:
+		break;
+	}
+
+	if (register_framebuffer(info) < 0)
+		goto out_err3;
+
+	if (psbfb_check_var(&info->var, info) < 0)
+		goto out_err4;
+
+	psbfb_set_par(info);
+
+	DRM_INFO("fb%d: %s frame buffer device\n", info->node, info->fix.id);
+
+	return 0;
+      out_err4:
+	unregister_framebuffer(info);
+      out_err3:
+	drm_bo_kunmap(&fb->kmap);
+      out_err2:
+	psbfb_vm_info_deref(&par->vi);
+      out_err1:
+	drm_bo_usage_deref_unlocked(&fb->bo);
+      out_err0:
+	drm_framebuffer_destroy(fb);
+	framebuffer_release(info);
+	crtc->fb = NULL;
+	return -EINVAL;
+}
+
+EXPORT_SYMBOL(psbfb_probe);
+
+int psbfb_remove(struct drm_device *dev, struct drm_crtc *crtc)
+{
+	struct drm_framebuffer *fb;
+	struct fb_info *info;
+	struct psbfb_par *par;
+
+	if (drm_psb_no_fb)
+		return 0;
+
+	fb = crtc->fb;
+	info = fb->fbdev;
+
+	if (info) {
+		unregister_framebuffer(info);
+		drm_bo_kunmap(&fb->kmap);
+		par = info->par;
+		if (par)
+			psbfb_vm_info_deref(&par->vi);
+		drm_bo_usage_deref_unlocked(&fb->bo);
+		drm_framebuffer_destroy(fb);
+		framebuffer_release(info);
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL(psbfb_remove);
+
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_fence.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_fence.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,285 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+#include "psb_drv.h"
+
+static void psb_poll_ta(struct drm_device *dev, uint32_t waiting_types)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_fence_driver *driver = dev->driver->fence_driver;
+	uint32_t cur_flag = 1;
+	uint32_t flags = 0;
+	uint32_t sequence = 0;
+	uint32_t remaining = 0xFFFFFFFF;
+	uint32_t diff;
+
+	struct psb_scheduler *scheduler;
+	struct psb_scheduler_seq *seq;
+	struct drm_fence_class_manager *fc =
+	    &dev->fm.fence_class[PSB_ENGINE_TA];
+
+	if (unlikely(!dev_priv))
+		return;
+
+	scheduler = &dev_priv->scheduler;
+	seq = scheduler->seq;
+
+	while (likely(waiting_types & remaining)) {
+		if (!(waiting_types & cur_flag))
+			goto skip;
+		if (seq->reported)
+			goto skip;
+		if (flags == 0)
+			sequence = seq->sequence;
+		else if (sequence != seq->sequence) {
+			drm_fence_handler(dev, PSB_ENGINE_TA,
+					  sequence, flags, 0);
+			sequence = seq->sequence;
+			flags = 0;
+		}
+		flags |= cur_flag;
+
+		/*
+		 * Sequence may not have ended up on the ring yet.
+		 * In that case, report it but don't mark it as
+		 * reported. A subsequent poll will report it again.
+		 */
+
+		diff = (fc->latest_queued_sequence - sequence) &
+		    driver->sequence_mask;
+		if (diff < driver->wrap_diff)
+			seq->reported = 1;
+
+	      skip:
+		cur_flag <<= 1;
+		remaining <<= 1;
+		seq++;
+	}
+
+	if (flags) {
+		drm_fence_handler(dev, PSB_ENGINE_TA, sequence, flags, 0);
+	}
+}
+
+static void psb_poll_other(struct drm_device *dev, uint32_t fence_class,
+			   uint32_t waiting_types)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence_class];
+	uint32_t sequence;
+
+	if (unlikely(!dev_priv))
+		return;
+
+	if (waiting_types) {
+		if (fence_class == PSB_ENGINE_VIDEO)
+			sequence = dev_priv->msvdx_current_sequence;
+		else
+			sequence = dev_priv->comm[fence_class << 4];
+
+		drm_fence_handler(dev, fence_class, sequence,
+				  DRM_FENCE_TYPE_EXE, 0);
+
+		switch (fence_class) {
+		case PSB_ENGINE_2D:
+			if (dev_priv->fence0_irq_on && !fc->waiting_types) {
+				psb_2D_irq_off(dev_priv);
+				dev_priv->fence0_irq_on = 0;
+			} else if (!dev_priv->fence0_irq_on
+				   && fc->waiting_types) {
+				psb_2D_irq_on(dev_priv);
+				dev_priv->fence0_irq_on = 1;
+			}
+			break;
+#if 0
+			/*
+			 * FIXME: MSVDX irq switching
+			 */
+
+		case PSB_ENGINE_VIDEO:
+			if (dev_priv->fence2_irq_on && !fc->waiting_types) {
+				psb_msvdx_irq_off(dev_priv);
+				dev_priv->fence2_irq_on = 0;
+			} else if (!dev_priv->fence2_irq_on
+				   && fc->pending_exe_flush) {
+				psb_msvdx_irq_on(dev_priv);
+				dev_priv->fence2_irq_on = 1;
+			}
+			break;
+#endif
+		default:
+			return;
+		}
+	}
+}
+
+static void psb_fence_poll(struct drm_device *dev,
+			   uint32_t fence_class, uint32_t waiting_types)
+{
+	switch (fence_class) {
+	case PSB_ENGINE_TA:
+		psb_poll_ta(dev, waiting_types);
+		break;
+	default:
+		psb_poll_other(dev, fence_class, waiting_types);
+		break;
+	}
+}
+
+void psb_fence_error(struct drm_device *dev,
+		     uint32_t fence_class,
+		     uint32_t sequence, uint32_t type, int error)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	unsigned long irq_flags;
+
+	BUG_ON(fence_class >= PSB_NUM_ENGINES);
+	write_lock_irqsave(&fm->lock, irq_flags);
+	drm_fence_handler(dev, fence_class, sequence, type, error);
+	write_unlock_irqrestore(&fm->lock, irq_flags);
+}
+
+int psb_fence_emit_sequence(struct drm_device *dev, uint32_t fence_class,
+			    uint32_t flags, uint32_t * sequence,
+			    uint32_t * native_type)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	uint32_t seq = 0;
+	int ret;
+
+	if (!dev_priv)
+		return -EINVAL;
+
+	if (fence_class >= PSB_NUM_ENGINES)
+		return -EINVAL;
+
+	switch (fence_class) {
+	case PSB_ENGINE_2D:
+		spin_lock(&dev_priv->sequence_lock);
+		seq = ++dev_priv->sequence[fence_class];
+		spin_unlock(&dev_priv->sequence_lock);
+		ret = psb_blit_sequence(dev_priv, seq);
+		if (ret)
+			return ret;
+		break;
+	case PSB_ENGINE_VIDEO:
+		spin_lock(&dev_priv->sequence_lock);
+		seq = ++dev_priv->sequence[fence_class];
+		spin_unlock(&dev_priv->sequence_lock);
+		break;
+	default:
+		spin_lock(&dev_priv->sequence_lock);
+		seq = dev_priv->sequence[fence_class];
+		spin_unlock(&dev_priv->sequence_lock);
+	}
+
+	*sequence = seq;
+	*native_type = DRM_FENCE_TYPE_EXE;
+
+	return 0;
+}
+
+uint32_t psb_fence_advance_sequence(struct drm_device * dev,
+				    uint32_t fence_class)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	uint32_t sequence;
+
+	spin_lock(&dev_priv->sequence_lock);
+	sequence = ++dev_priv->sequence[fence_class];
+	spin_unlock(&dev_priv->sequence_lock);
+
+	return sequence;
+}
+
+void psb_fence_handler(struct drm_device *dev, uint32_t fence_class)
+{
+	struct drm_fence_manager *fm = &dev->fm;
+	struct drm_fence_class_manager *fc = &fm->fence_class[fence_class];
+
+#ifdef FIX_TG_16
+	if (fence_class == 0) {
+		struct drm_psb_private *dev_priv =
+		    (struct drm_psb_private *)dev->dev_private;
+
+		if ((atomic_read(&dev_priv->ta_wait_2d_irq) == 1) &&
+		    (PSB_RSGX32(PSB_CR_2D_SOCIF) == _PSB_C2_SOCIF_EMPTY) &&
+		    ((PSB_RSGX32(PSB_CR_2D_BLIT_STATUS) &
+		      _PSB_C2B_STATUS_BUSY) == 0))
+			psb_resume_ta_2d_idle(dev_priv);
+	}
+#endif
+	write_lock(&fm->lock);
+	psb_fence_poll(dev, fence_class, fc->waiting_types);
+	write_unlock(&fm->lock);
+}
+
+static int psb_fence_wait(struct drm_fence_object *fence,
+			  int lazy, int interruptible, uint32_t mask)
+{
+	struct drm_device *dev = fence->dev;
+	struct drm_fence_class_manager *fc =
+	    &dev->fm.fence_class[fence->fence_class];
+	int ret = 0;
+	unsigned long timeout = DRM_HZ *
+	    ((fence->fence_class == PSB_ENGINE_TA) ? 30 : 3);
+
+	drm_fence_object_flush(fence, mask);
+	if (interruptible)
+		ret = wait_event_interruptible_timeout
+		    (fc->fence_queue, drm_fence_object_signaled(fence, mask),
+		     timeout);
+	else
+		ret = wait_event_timeout
+		    (fc->fence_queue, drm_fence_object_signaled(fence, mask),
+		     timeout);
+
+	if (unlikely(ret == -ERESTARTSYS))
+		return -EAGAIN;
+
+	if (unlikely(ret == 0))
+		return -EBUSY;
+
+	return 0;
+}
+
+struct drm_fence_driver psb_fence_driver = {
+	.num_classes = PSB_NUM_ENGINES,
+	.wrap_diff = (1 << 30),
+	.flush_diff = (1 << 29),
+	.sequence_mask = 0xFFFFFFFFU,
+	.has_irq = NULL,
+	.emit = psb_fence_emit_sequence,
+	.flush = NULL,
+	.poll = psb_fence_poll,
+	.needed_flush = NULL,
+	.wait = psb_fence_wait
+};
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_gtt.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_gtt.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,233 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics.com>
+ */
+#include "drmP.h"
+#include "psb_drv.h"
+
+static inline uint32_t psb_gtt_mask_pte(uint32_t pfn, int type)
+{
+	uint32_t mask = PSB_PTE_VALID;
+
+	if (type & PSB_MMU_CACHED_MEMORY)
+		mask |= PSB_PTE_CACHED;
+	if (type & PSB_MMU_RO_MEMORY)
+		mask |= PSB_PTE_RO;
+	if (type & PSB_MMU_WO_MEMORY)
+		mask |= PSB_PTE_WO;
+
+	return (pfn << PAGE_SHIFT) | mask;
+}
+
+struct psb_gtt *psb_gtt_alloc(struct drm_device *dev)
+{
+	struct psb_gtt *tmp = drm_calloc(1, sizeof(*tmp), DRM_MEM_DRIVER);
+
+	if (!tmp)
+		return NULL;
+
+	init_rwsem(&tmp->sem);
+	tmp->dev = dev;
+
+	return tmp;
+}
+
+void psb_gtt_takedown(struct psb_gtt *pg, int free)
+{
+	struct drm_psb_private *dev_priv = pg->dev->dev_private;
+
+	if (!pg)
+		return;
+
+	if (pg->gtt_map) {
+		iounmap(pg->gtt_map);
+		pg->gtt_map = NULL;
+	}
+	if (pg->initialized) {
+		pci_write_config_word(pg->dev->pdev, PSB_GMCH_CTRL,
+				      pg->gmch_ctrl);
+		PSB_WVDC32(pg->pge_ctl, PSB_PGETBL_CTL);
+		(void)PSB_RVDC32(PSB_PGETBL_CTL);
+	}
+	if (free)
+		drm_free(pg, sizeof(*pg), DRM_MEM_DRIVER);
+}
+
+int psb_gtt_init(struct psb_gtt *pg, int resume)
+{
+	struct drm_device *dev = pg->dev;
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	unsigned gtt_pages;
+	unsigned long stolen_size;
+	unsigned i, num_pages;
+	unsigned pfn_base;
+
+	int ret = 0;
+	uint32_t pte;
+
+	pci_read_config_word(dev->pdev, PSB_GMCH_CTRL, &pg->gmch_ctrl);
+	pci_write_config_word(dev->pdev, PSB_GMCH_CTRL,
+			      pg->gmch_ctrl | _PSB_GMCH_ENABLED);
+
+	pg->pge_ctl = PSB_RVDC32(PSB_PGETBL_CTL);
+	PSB_WVDC32(pg->pge_ctl | _PSB_PGETBL_ENABLED, PSB_PGETBL_CTL);
+	(void)PSB_RVDC32(PSB_PGETBL_CTL);
+
+	pg->initialized = 1;
+
+	pg->gtt_phys_start = pg->pge_ctl & PAGE_MASK;
+	pg->gatt_start = pci_resource_start(dev->pdev, PSB_GATT_RESOURCE);
+	pg->gtt_start = pci_resource_start(dev->pdev, PSB_GTT_RESOURCE);
+	gtt_pages = pci_resource_len(dev->pdev, PSB_GTT_RESOURCE) >> PAGE_SHIFT;
+	pg->gatt_pages = pci_resource_len(dev->pdev, PSB_GATT_RESOURCE)
+	    >> PAGE_SHIFT;
+	pci_read_config_dword(dev->pdev, PSB_BSM, &pg->stolen_base);
+	stolen_size = pg->gtt_phys_start - pg->stolen_base - PAGE_SIZE;
+
+	PSB_DEBUG_INIT("GTT phys start: 0x%08x.\n", pg->gtt_phys_start);
+	PSB_DEBUG_INIT("GTT start: 0x%08x.\n", pg->gtt_start);
+	PSB_DEBUG_INIT("GATT start: 0x%08x.\n", pg->gatt_start);
+	PSB_DEBUG_INIT("GTT pages: %u\n", gtt_pages);
+	PSB_DEBUG_INIT("Stolen size: %lu kiB\n", stolen_size / 1024);
+
+	if (resume && (gtt_pages != pg->gtt_pages) &&
+	    (stolen_size != pg->stolen_size)) {
+		DRM_ERROR("GTT resume error.\n");
+		ret = -EINVAL;
+		goto out_err;
+	}
+
+	pg->gtt_pages = gtt_pages;
+	pg->stolen_size = stolen_size;
+	pg->gtt_map =
+	    ioremap_nocache(pg->gtt_phys_start, gtt_pages << PAGE_SHIFT);
+	if (!pg->gtt_map) {
+		DRM_ERROR("Failure to map gtt.\n");
+		ret = -ENOMEM;
+		goto out_err;
+	}
+
+	/*
+	 * insert stolen pages.
+	 */
+
+	pfn_base = pg->stolen_base >> PAGE_SHIFT;
+	num_pages = stolen_size >> PAGE_SHIFT;
+	PSB_DEBUG_INIT("Set up %d stolen pages starting at 0x%08x\n",
+		       num_pages, pfn_base);
+	for (i = 0; i < num_pages; ++i) {
+		pte = psb_gtt_mask_pte(pfn_base + i, 0);
+		iowrite32(pte, pg->gtt_map + i);
+	}
+
+	/*
+	 * Init rest of gtt.
+	 */
+
+	pfn_base = page_to_pfn(dev_priv->scratch_page);
+	pte = psb_gtt_mask_pte(pfn_base, 0);
+	PSB_DEBUG_INIT("Initializing the rest of a total "
+		       "of %d gtt pages.\n", pg->gatt_pages);
+
+	for (; i < pg->gatt_pages; ++i)
+		iowrite32(pte, pg->gtt_map + i);
+	(void)ioread32(pg->gtt_map + i - 1);
+
+	return 0;
+
+      out_err:
+	psb_gtt_takedown(pg, 0);
+	return ret;
+}
+
+int psb_gtt_insert_pages(struct psb_gtt *pg, struct page **pages,
+			 unsigned offset_pages, unsigned num_pages,
+			 unsigned desired_tile_stride, unsigned hw_tile_stride,
+			 int type)
+{
+	unsigned rows = 1;
+	unsigned add;
+	unsigned row_add;
+	unsigned i;
+	unsigned j;
+	uint32_t *cur_page = NULL;
+	uint32_t pte;
+
+	if (hw_tile_stride)
+		rows = num_pages / desired_tile_stride;
+	else
+		desired_tile_stride = num_pages;
+
+	add = desired_tile_stride;
+	row_add = hw_tile_stride;
+
+	down_read(&pg->sem);
+	for (i = 0; i < rows; ++i) {
+		cur_page = pg->gtt_map + offset_pages;
+		for (j = 0; j < desired_tile_stride; ++j) {
+			pte = psb_gtt_mask_pte(page_to_pfn(*pages++), type);
+			iowrite32(pte, cur_page++);
+		}
+		offset_pages += add;
+	}
+	(void)ioread32(cur_page - 1);
+	up_read(&pg->sem);
+
+	return 0;
+}
+
+int psb_gtt_remove_pages(struct psb_gtt *pg, unsigned offset_pages,
+			 unsigned num_pages, unsigned desired_tile_stride,
+			 unsigned hw_tile_stride)
+{
+	struct drm_psb_private *dev_priv = pg->dev->dev_private;
+	unsigned rows = 1;
+	unsigned add;
+	unsigned row_add;
+	unsigned i;
+	unsigned j;
+	uint32_t *cur_page = NULL;
+	unsigned pfn_base = page_to_pfn(dev_priv->scratch_page);
+	uint32_t pte = psb_gtt_mask_pte(pfn_base, 0);
+
+	if (hw_tile_stride)
+		rows = num_pages / desired_tile_stride;
+	else
+		desired_tile_stride = num_pages;
+
+	add = desired_tile_stride;
+	row_add = hw_tile_stride;
+
+	down_read(&pg->sem);
+	for (i = 0; i < rows; ++i) {
+		cur_page = pg->gtt_map + offset_pages;
+		for (j = 0; j < desired_tile_stride; ++j) {
+			iowrite32(pte, cur_page++);
+		}
+		offset_pages += add;
+	}
+	(void)ioread32(cur_page - 1);
+	up_read(&pg->sem);
+
+	return 0;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_i2c.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_i2c.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,179 @@
+/*
+ * Copyright Â© 2006-2007 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Eric Anholt <eric@anholt.net>
+ */
+/*
+ * Copyright (c) 2006 Dave Airlie <airlied@linux.ie>
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+
+#include <linux/i2c.h>
+#include <linux/i2c-id.h>
+#include <linux/i2c-algo-bit.h>
+#include "drmP.h"
+#include "drm.h"
+#include "intel_drv.h"
+#include "psb_drv.h"
+
+/*
+ * Intel GPIO access functions
+ */
+
+#define I2C_RISEFALL_TIME 20
+
+static int get_clock(void *data)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_psb_private *dev_priv = chan->drm_dev->dev_private;
+	uint32_t val;
+
+	val = PSB_RVDC32(chan->reg);
+	return ((val & GPIO_CLOCK_VAL_IN) != 0);
+}
+
+static int get_data(void *data)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_psb_private *dev_priv = chan->drm_dev->dev_private;
+	uint32_t val;
+
+	val = PSB_RVDC32(chan->reg);
+	return ((val & GPIO_DATA_VAL_IN) != 0);
+}
+
+static void set_clock(void *data, int state_high)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_psb_private *dev_priv = chan->drm_dev->dev_private;
+	uint32_t reserved = 0, clock_bits;
+
+	/* On most chips, these bits must be preserved in software. */
+	reserved = PSB_RVDC32(chan->reg) & (GPIO_DATA_PULLUP_DISABLE |
+					    GPIO_CLOCK_PULLUP_DISABLE);
+
+	if (state_high)
+		clock_bits = GPIO_CLOCK_DIR_IN | GPIO_CLOCK_DIR_MASK;
+	else
+		clock_bits = GPIO_CLOCK_DIR_OUT | GPIO_CLOCK_DIR_MASK |
+		    GPIO_CLOCK_VAL_MASK;
+	PSB_WVDC32(reserved | clock_bits, chan->reg);
+	udelay(I2C_RISEFALL_TIME);	/* wait for the line to change state */
+}
+
+static void set_data(void *data, int state_high)
+{
+	struct intel_i2c_chan *chan = data;
+	struct drm_psb_private *dev_priv = chan->drm_dev->dev_private;
+	uint32_t reserved = 0, data_bits;
+
+	/* On most chips, these bits must be preserved in software. */
+	reserved = PSB_RVDC32(chan->reg) & (GPIO_DATA_PULLUP_DISABLE |
+					    GPIO_CLOCK_PULLUP_DISABLE);
+
+	if (state_high)
+		data_bits = GPIO_DATA_DIR_IN | GPIO_DATA_DIR_MASK;
+	else
+		data_bits = GPIO_DATA_DIR_OUT | GPIO_DATA_DIR_MASK |
+		    GPIO_DATA_VAL_MASK;
+
+	PSB_WVDC32(data_bits, chan->reg);
+	udelay(I2C_RISEFALL_TIME);	/* wait for the line to change state */
+}
+
+/**
+ * intel_i2c_create - instantiate an Intel i2c bus using the specified GPIO reg
+ * @dev: DRM device
+ * @output: driver specific output device
+ * @reg: GPIO reg to use
+ * @name: name for this bus
+ *
+ * Creates and registers a new i2c bus with the Linux i2c layer, for use
+ * in output probing and control (e.g. DDC or SDVO control functions).
+ *
+ * Possible values for @reg include:
+ *   %GPIOA
+ *   %GPIOB
+ *   %GPIOC
+ *   %GPIOD
+ *   %GPIOE
+ *   %GPIOF
+ *   %GPIOG
+ *   %GPIOH
+ * see PRM for details on how these different busses are used.
+ */
+struct intel_i2c_chan *intel_i2c_create(struct drm_device *dev,
+					const uint32_t reg, const char *name)
+{
+	struct intel_i2c_chan *chan;
+
+	chan = kzalloc(sizeof(struct intel_i2c_chan), GFP_KERNEL);
+	if (!chan)
+		goto out_free;
+
+	chan->drm_dev = dev;
+	chan->reg = reg;
+	snprintf(chan->adapter.name, I2C_NAME_SIZE, "intel drm %s", name);
+	chan->adapter.owner = THIS_MODULE;
+	chan->adapter.id = I2C_HW_B_INTELFB;
+	chan->adapter.algo_data = &chan->algo;
+	chan->adapter.dev.parent = &dev->pdev->dev;
+	chan->algo.setsda = set_data;
+	chan->algo.setscl = set_clock;
+	chan->algo.getsda = get_data;
+	chan->algo.getscl = get_clock;
+	chan->algo.udelay = 20;
+	chan->algo.timeout = usecs_to_jiffies(2200);
+	chan->algo.data = chan;
+
+	i2c_set_adapdata(&chan->adapter, chan);
+
+	if (i2c_bit_add_bus(&chan->adapter))
+		goto out_free;
+
+	/* JJJ:  raise SCL and SDA? */
+	set_data(chan, 1);
+	set_clock(chan, 1);
+	udelay(20);
+
+	return chan;
+
+      out_free:
+	kfree(chan);
+	return NULL;
+}
+
+/**
+ * intel_i2c_destroy - unregister and free i2c bus resources
+ * @output: channel to free
+ *
+ * Unregister the adapter from the i2c layer, then free the structure.
+ */
+void intel_i2c_destroy(struct intel_i2c_chan *chan)
+{
+	if (!chan)
+		return;
+
+	i2c_del_adapter(&chan->adapter);
+	kfree(chan);
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_irq.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_irq.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,382 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ */
+
+#include "drmP.h"
+#include "psb_drv.h"
+#include "psb_reg.h"
+#include "psb_msvdx.h"
+
+/*
+ * Video display controller interrupt.
+ */
+
+static void psb_vdc_interrupt(struct drm_device *dev, uint32_t vdc_stat)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	uint32_t pipe_stats;
+	int wake = 0;
+
+	if (!drm_psb_disable_vsync && (vdc_stat & _PSB_VSYNC_PIPEA_FLAG)) {
+		pipe_stats = PSB_RVDC32(PSB_PIPEASTAT);
+		atomic_inc(&dev->vbl_received);
+		wake = 1;
+		PSB_WVDC32(pipe_stats | _PSB_VBLANK_INTERRUPT_ENABLE |
+			   _PSB_VBLANK_CLEAR, PSB_PIPEASTAT);
+	}
+
+	if (!drm_psb_disable_vsync && (vdc_stat & _PSB_VSYNC_PIPEB_FLAG)) {
+		pipe_stats = PSB_RVDC32(PSB_PIPEBSTAT);
+		atomic_inc(&dev->vbl_received2);
+		wake = 1;
+		PSB_WVDC32(pipe_stats | _PSB_VBLANK_INTERRUPT_ENABLE |
+			   _PSB_VBLANK_CLEAR, PSB_PIPEBSTAT);
+	}
+
+	PSB_WVDC32(vdc_stat, PSB_INT_IDENTITY_R);
+	(void)PSB_RVDC32(PSB_INT_IDENTITY_R);
+	DRM_READMEMORYBARRIER();
+
+	if (wake) {
+		DRM_WAKEUP(&dev->vbl_queue);
+		drm_vbl_send_signals(dev);
+	}
+}
+
+/*
+ * SGX interrupt source 1.
+ */
+
+static void psb_sgx_interrupt(struct drm_device *dev, uint32_t sgx_stat,
+			      uint32_t sgx_stat2)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	if (sgx_stat & _PSB_CE_TWOD_COMPLETE) {
+		DRM_WAKEUP(&dev_priv->event_2d_queue);
+		psb_fence_handler(dev, 0);
+	}
+
+	if (unlikely(sgx_stat2 & _PSB_CE2_BIF_REQUESTER_FAULT))
+		psb_print_pagefault(dev_priv);
+
+	psb_scheduler_handler(dev_priv, sgx_stat);
+}
+
+/*
+ * MSVDX interrupt.
+ */
+static void psb_msvdx_interrupt(struct drm_device *dev, uint32_t msvdx_stat)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	if (msvdx_stat & MSVDX_INTERRUPT_STATUS_CR_MMU_FAULT_IRQ_MASK) {
+		/*Ideally we should we should never get to this */
+		PSB_DEBUG_GENERAL
+		    ("******MSVDX: msvdx_stat: 0x%x fence2_irq_on=%d ***** (MMU FAULT)\n",
+		     msvdx_stat, dev_priv->fence2_irq_on);
+
+		/* Pause MMU */
+		PSB_WMSVDX32(MSVDX_MMU_CONTROL0_CR_MMU_PAUSE_MASK,
+			     MSVDX_MMU_CONTROL0);
+		DRM_WRITEMEMORYBARRIER();
+
+		/* Clear this interupt bit only */
+		PSB_WMSVDX32(MSVDX_INTERRUPT_STATUS_CR_MMU_FAULT_IRQ_MASK,
+			     MSVDX_INTERRUPT_CLEAR);
+		PSB_RMSVDX32(MSVDX_INTERRUPT_CLEAR);
+		DRM_READMEMORYBARRIER();
+
+		dev_priv->msvdx_needs_reset = 1;
+	} else if (msvdx_stat & MSVDX_INTERRUPT_STATUS_CR_MTX_IRQ_MASK) {
+		PSB_DEBUG_GENERAL
+		    ("******MSVDX: msvdx_stat: 0x%x fence2_irq_on=%d ***** (MTX)\n",
+		     msvdx_stat, dev_priv->fence2_irq_on);
+
+		/* Clear all interupt bits */
+		PSB_WMSVDX32(0xffff, MSVDX_INTERRUPT_CLEAR);
+		PSB_RMSVDX32(MSVDX_INTERRUPT_CLEAR);
+		DRM_READMEMORYBARRIER();
+
+		psb_msvdx_mtx_interrupt(dev);
+	}
+}
+
+irqreturn_t psb_irq_handler(DRM_IRQ_ARGS)
+{
+	struct drm_device *dev = (struct drm_device *)arg;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	uint32_t vdc_stat;
+	uint32_t sgx_stat;
+	uint32_t sgx_stat2;
+	uint32_t msvdx_stat;
+	int handled = 0;
+
+	spin_lock(&dev_priv->irqmask_lock);
+
+	vdc_stat = PSB_RVDC32(PSB_INT_IDENTITY_R);
+	sgx_stat = PSB_RSGX32(PSB_CR_EVENT_STATUS);
+	sgx_stat2 = PSB_RSGX32(PSB_CR_EVENT_STATUS2);
+	msvdx_stat = PSB_RMSVDX32(MSVDX_INTERRUPT_STATUS);
+
+	sgx_stat2 &= dev_priv->sgx2_irq_mask;
+	sgx_stat &= dev_priv->sgx_irq_mask;
+	PSB_WSGX32(sgx_stat2, PSB_CR_EVENT_HOST_CLEAR2);
+	PSB_WSGX32(sgx_stat, PSB_CR_EVENT_HOST_CLEAR);
+	(void)PSB_RSGX32(PSB_CR_EVENT_HOST_CLEAR);
+
+	vdc_stat &= dev_priv->vdc_irq_mask;
+	spin_unlock(&dev_priv->irqmask_lock);
+
+	if (msvdx_stat) {
+		psb_msvdx_interrupt(dev, msvdx_stat);
+		handled = 1;
+	}
+
+	if (vdc_stat) {
+		/* MSVDX IRQ status is part of vdc_irq_mask */
+		psb_vdc_interrupt(dev, vdc_stat);
+		handled = 1;
+	}
+
+	if (sgx_stat || sgx_stat2) {
+		psb_sgx_interrupt(dev, sgx_stat, sgx_stat2);
+		handled = 1;
+	}
+
+	if (!handled) {
+		return IRQ_NONE;
+	}
+
+	return IRQ_HANDLED;
+}
+
+void psb_msvdx_irq_preinstall(struct drm_psb_private *dev_priv)
+{
+	unsigned long mtx_int = 0;
+	dev_priv->vdc_irq_mask |= _PSB_IRQ_MSVDX_FLAG;
+
+	/*Clear MTX interrupt */
+	REGIO_WRITE_FIELD_LITE(mtx_int, MSVDX_INTERRUPT_STATUS, CR_MTX_IRQ, 1);
+	PSB_WMSVDX32(mtx_int, MSVDX_INTERRUPT_CLEAR);
+}
+
+void psb_irq_preinstall(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	spin_lock(&dev_priv->irqmask_lock);
+	PSB_WVDC32(0xFFFFFFFF, PSB_HWSTAM);
+	PSB_WVDC32(0x00000000, PSB_INT_MASK_R);
+	PSB_WVDC32(0x00000000, PSB_INT_ENABLE_R);
+	PSB_WSGX32(0x00000000, PSB_CR_EVENT_HOST_ENABLE);
+	(void)PSB_RSGX32(PSB_CR_EVENT_HOST_ENABLE);
+
+	dev_priv->sgx_irq_mask = _PSB_CE_PIXELBE_END_RENDER |
+	    _PSB_CE_DPM_3D_MEM_FREE |
+	    _PSB_CE_TA_FINISHED |
+	    _PSB_CE_DPM_REACHED_MEM_THRESH |
+	    _PSB_CE_DPM_OUT_OF_MEMORY_GBL |
+	    _PSB_CE_DPM_OUT_OF_MEMORY_MT |
+	    _PSB_CE_TA_TERMINATE | _PSB_CE_SW_EVENT;
+
+	dev_priv->sgx2_irq_mask = _PSB_CE2_BIF_REQUESTER_FAULT;
+
+	dev_priv->vdc_irq_mask = _PSB_IRQ_SGX_FLAG | _PSB_IRQ_MSVDX_FLAG;
+
+	if (!drm_psb_disable_vsync)
+		dev_priv->vdc_irq_mask |= _PSB_VSYNC_PIPEA_FLAG |
+		    _PSB_VSYNC_PIPEB_FLAG;
+
+	/*Clear MTX interrupt */
+	{
+		unsigned long mtx_int = 0;
+		REGIO_WRITE_FIELD_LITE(mtx_int, MSVDX_INTERRUPT_STATUS,
+				       CR_MTX_IRQ, 1);
+		PSB_WMSVDX32(mtx_int, MSVDX_INTERRUPT_CLEAR);
+	}
+	spin_unlock(&dev_priv->irqmask_lock);
+}
+
+void psb_msvdx_irq_postinstall(struct drm_psb_private *dev_priv)
+{
+	/* Enable Mtx Interupt to host */
+	unsigned long enables = 0;
+	PSB_DEBUG_GENERAL("Setting up MSVDX IRQs.....\n");
+	REGIO_WRITE_FIELD_LITE(enables, MSVDX_INTERRUPT_STATUS, CR_MTX_IRQ, 1);
+	PSB_WMSVDX32(enables, MSVDX_HOST_INTERRUPT_ENABLE);
+}
+
+int psb_irq_postinstall(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&dev_priv->irqmask_lock, irqflags);
+	PSB_WVDC32(dev_priv->vdc_irq_mask, PSB_INT_ENABLE_R);
+	PSB_WSGX32(dev_priv->sgx2_irq_mask, PSB_CR_EVENT_HOST_ENABLE2);
+	PSB_WSGX32(dev_priv->sgx_irq_mask, PSB_CR_EVENT_HOST_ENABLE);
+	(void)PSB_RSGX32(PSB_CR_EVENT_HOST_ENABLE);
+	/****MSVDX IRQ Setup...*****/
+	/* Enable Mtx Interupt to host */
+	{
+		unsigned long enables = 0;
+		PSB_DEBUG_GENERAL("Setting up MSVDX IRQs.....\n");
+		REGIO_WRITE_FIELD_LITE(enables, MSVDX_INTERRUPT_STATUS,
+				       CR_MTX_IRQ, 1);
+		PSB_WMSVDX32(enables, MSVDX_HOST_INTERRUPT_ENABLE);
+	}
+	dev_priv->irq_enabled = 1;
+	spin_unlock_irqrestore(&dev_priv->irqmask_lock, irqflags);
+	return 0;
+}
+
+void psb_irq_uninstall(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&dev_priv->irqmask_lock, irqflags);
+
+	dev_priv->sgx_irq_mask = 0x00000000;
+	dev_priv->sgx2_irq_mask = 0x00000000;
+	dev_priv->vdc_irq_mask = 0x00000000;
+
+	PSB_WVDC32(0xFFFFFFFF, PSB_HWSTAM);
+	PSB_WVDC32(0xFFFFFFFF, PSB_INT_MASK_R);
+	PSB_WVDC32(dev_priv->vdc_irq_mask, PSB_INT_ENABLE_R);
+	PSB_WSGX32(dev_priv->sgx_irq_mask, PSB_CR_EVENT_HOST_ENABLE);
+	PSB_WSGX32(dev_priv->sgx2_irq_mask, PSB_CR_EVENT_HOST_ENABLE2);
+	wmb();
+	PSB_WVDC32(PSB_RVDC32(PSB_INT_IDENTITY_R), PSB_INT_IDENTITY_R);
+	PSB_WSGX32(PSB_RSGX32(PSB_CR_EVENT_STATUS), PSB_CR_EVENT_HOST_CLEAR);
+	PSB_WSGX32(PSB_RSGX32(PSB_CR_EVENT_STATUS2), PSB_CR_EVENT_HOST_CLEAR2);
+
+	/****MSVDX IRQ Setup...*****/
+	/* Clear interrupt enabled flag */
+	PSB_WMSVDX32(0, MSVDX_HOST_INTERRUPT_ENABLE);
+
+	dev_priv->irq_enabled = 0;
+	spin_unlock_irqrestore(&dev_priv->irqmask_lock, irqflags);
+
+}
+
+void psb_2D_irq_off(struct drm_psb_private *dev_priv)
+{
+	unsigned long irqflags;
+	uint32_t old_mask;
+	uint32_t cleared_mask;
+
+	spin_lock_irqsave(&dev_priv->irqmask_lock, irqflags);
+	--dev_priv->irqen_count_2d;
+	if (dev_priv->irq_enabled && dev_priv->irqen_count_2d == 0) {
+
+		old_mask = dev_priv->sgx_irq_mask;
+		dev_priv->sgx_irq_mask &= ~_PSB_CE_TWOD_COMPLETE;
+		PSB_WSGX32(dev_priv->sgx_irq_mask, PSB_CR_EVENT_HOST_ENABLE);
+		(void)PSB_RSGX32(PSB_CR_EVENT_HOST_ENABLE);
+
+		cleared_mask = (old_mask ^ dev_priv->sgx_irq_mask) & old_mask;
+		PSB_WSGX32(cleared_mask, PSB_CR_EVENT_HOST_CLEAR);
+		(void)PSB_RSGX32(PSB_CR_EVENT_HOST_CLEAR);
+	}
+	spin_unlock_irqrestore(&dev_priv->irqmask_lock, irqflags);
+}
+
+void psb_2D_irq_on(struct drm_psb_private *dev_priv)
+{
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&dev_priv->irqmask_lock, irqflags);
+	if (dev_priv->irq_enabled && dev_priv->irqen_count_2d == 0) {
+		dev_priv->sgx_irq_mask |= _PSB_CE_TWOD_COMPLETE;
+		PSB_WSGX32(dev_priv->sgx_irq_mask, PSB_CR_EVENT_HOST_ENABLE);
+		(void)PSB_RSGX32(PSB_CR_EVENT_HOST_ENABLE);
+	}
+	++dev_priv->irqen_count_2d;
+	spin_unlock_irqrestore(&dev_priv->irqmask_lock, irqflags);
+}
+
+static int psb_vblank_do_wait(struct drm_device *dev, unsigned int *sequence,
+			      atomic_t * counter)
+{
+	unsigned int cur_vblank;
+	int ret = 0;
+
+	DRM_WAIT_ON(ret, dev->vbl_queue, 3 * DRM_HZ,
+		    (((cur_vblank = atomic_read(counter))
+		      - *sequence) <= (1 << 23)));
+
+	*sequence = cur_vblank;
+
+	return ret;
+}
+
+int psb_vblank_wait(struct drm_device *dev, unsigned int *sequence)
+{
+	int ret;
+
+	ret = psb_vblank_do_wait(dev, sequence, &dev->vbl_received);
+	return ret;
+}
+
+int psb_vblank_wait2(struct drm_device *dev, unsigned int *sequence)
+{
+	int ret;
+
+	ret = psb_vblank_do_wait(dev, sequence, &dev->vbl_received2);
+	return ret;
+}
+
+void psb_msvdx_irq_off(struct drm_psb_private *dev_priv)
+{
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&dev_priv->irqmask_lock, irqflags);
+	if (dev_priv->irq_enabled) {
+		dev_priv->vdc_irq_mask &= ~_PSB_IRQ_MSVDX_FLAG;
+		PSB_WSGX32(dev_priv->vdc_irq_mask, PSB_INT_ENABLE_R);
+		(void)PSB_RSGX32(PSB_INT_ENABLE_R);
+	}
+	spin_unlock_irqrestore(&dev_priv->irqmask_lock, irqflags);
+}
+
+void psb_msvdx_irq_on(struct drm_psb_private *dev_priv)
+{
+	unsigned long irqflags;
+
+	spin_lock_irqsave(&dev_priv->irqmask_lock, irqflags);
+	if (dev_priv->irq_enabled) {
+		dev_priv->vdc_irq_mask |= _PSB_IRQ_MSVDX_FLAG;
+		PSB_WSGX32(dev_priv->vdc_irq_mask, PSB_INT_ENABLE_R);
+		(void)PSB_RSGX32(PSB_INT_ENABLE_R);
+	}
+	spin_unlock_irqrestore(&dev_priv->irqmask_lock, irqflags);
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_mmu.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_mmu.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,1037 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+#include "drmP.h"
+#include "psb_drv.h"
+#include "psb_reg.h"
+
+/*
+ * Code for the SGX MMU:
+ */
+
+/*
+ * clflush on one processor only:
+ * clflush should apparently flush the cache line on all processors in an
+ * SMP system.
+ */
+
+/*
+ * kmap atomic:
+ * The usage of the slots must be completely encapsulated within a spinlock, and
+ * no other functions that may be using the locks for other purposed may be
+ * called from within the locked region.
+ * Since the slots are per processor, this will guarantee that we are the only
+ * user.
+ */
+
+/*
+ * TODO: Inserting ptes from an interrupt handler:
+ * This may be desirable for some SGX functionality where the GPU can fault in
+ * needed pages. For that, we need to make an atomic insert_pages function, that
+ * may fail.
+ * If it fails, the caller need to insert the page using a workqueue function,
+ * but on average it should be fast.
+ */
+
+struct psb_mmu_driver {
+	/* protects driver- and pd structures. Always take in read mode
+	 * before taking the page table spinlock.
+	 */
+	struct rw_semaphore sem;
+
+	/* protects page tables, directory tables and pt tables.
+	 * and pt structures.
+	 */
+	spinlock_t lock;
+
+	atomic_t needs_tlbflush;
+	atomic_t *msvdx_mmu_invaldc;
+	uint8_t __iomem *register_map;
+	struct psb_mmu_pd *default_pd;
+	uint32_t bif_ctrl;
+	int has_clflush;
+	int clflush_add;
+	unsigned long clflush_mask;
+};
+
+struct psb_mmu_pd;
+
+struct psb_mmu_pt {
+	struct psb_mmu_pd *pd;
+	uint32_t index;
+	uint32_t count;
+	struct page *p;
+	uint32_t *v;
+};
+
+struct psb_mmu_pd {
+	struct psb_mmu_driver *driver;
+	int hw_context;
+	struct psb_mmu_pt **tables;
+	struct page *p;
+	struct page *dummy_pt;
+	struct page *dummy_page;
+	uint32_t pd_mask;
+	uint32_t invalid_pde;
+	uint32_t invalid_pte;
+};
+
+static inline uint32_t psb_mmu_pt_index(uint32_t offset)
+{
+	return (offset >> PSB_PTE_SHIFT) & 0x3FF;
+}
+static inline uint32_t psb_mmu_pd_index(uint32_t offset)
+{
+	return (offset >> PSB_PDE_SHIFT);
+}
+
+#if defined(CONFIG_X86)
+static inline void psb_clflush(void *addr)
+{
+	__asm__ __volatile__("clflush (%0)\n"::"r"(addr):"memory");
+}
+
+static inline void psb_mmu_clflush(struct psb_mmu_driver *driver, void *addr)
+{
+	if (!driver->has_clflush)
+		return;
+
+	mb();
+	psb_clflush(addr);
+	mb();
+}
+#else
+
+static inline void psb_mmu_clflush(struct psb_mmu_driver *driver, void *addr)
+{;
+}
+
+#endif
+
+static inline void psb_iowrite32(const struct psb_mmu_driver *d,
+				 uint32_t val, uint32_t offset)
+{
+	iowrite32(val, d->register_map + offset);
+}
+
+static inline uint32_t psb_ioread32(const struct psb_mmu_driver *d,
+				    uint32_t offset)
+{
+	return ioread32(d->register_map + offset);
+}
+
+static void psb_mmu_flush_pd_locked(struct psb_mmu_driver *driver, int force)
+{
+	if (atomic_read(&driver->needs_tlbflush) || force) {
+		uint32_t val = psb_ioread32(driver, PSB_CR_BIF_CTRL);
+		psb_iowrite32(driver, val | _PSB_CB_CTRL_INVALDC,
+			      PSB_CR_BIF_CTRL);
+		wmb();
+		psb_iowrite32(driver, val & ~_PSB_CB_CTRL_INVALDC,
+			      PSB_CR_BIF_CTRL);
+		(void)psb_ioread32(driver, PSB_CR_BIF_CTRL);
+		if (driver->msvdx_mmu_invaldc)
+			atomic_set(driver->msvdx_mmu_invaldc, 1);
+	}
+	atomic_set(&driver->needs_tlbflush, 0);
+}
+
+static void psb_mmu_flush_pd(struct psb_mmu_driver *driver, int force)
+{
+	down_write(&driver->sem);
+	psb_mmu_flush_pd_locked(driver, force);
+	up_write(&driver->sem);
+}
+
+void psb_mmu_flush(struct psb_mmu_driver *driver)
+{
+	uint32_t val;
+
+	down_write(&driver->sem);
+	val = psb_ioread32(driver, PSB_CR_BIF_CTRL);
+	if (atomic_read(&driver->needs_tlbflush))
+		psb_iowrite32(driver, val | _PSB_CB_CTRL_INVALDC,
+			      PSB_CR_BIF_CTRL);
+	else
+		psb_iowrite32(driver, val | _PSB_CB_CTRL_FLUSH,
+			      PSB_CR_BIF_CTRL);
+	wmb();
+	psb_iowrite32(driver,
+		      val & ~(_PSB_CB_CTRL_FLUSH | _PSB_CB_CTRL_INVALDC),
+		      PSB_CR_BIF_CTRL);
+	(void)psb_ioread32(driver, PSB_CR_BIF_CTRL);
+	atomic_set(&driver->needs_tlbflush, 0);
+	if (driver->msvdx_mmu_invaldc)
+		atomic_set(driver->msvdx_mmu_invaldc, 1);
+	up_write(&driver->sem);
+}
+
+void psb_mmu_set_pd_context(struct psb_mmu_pd *pd, int hw_context)
+{
+	uint32_t offset = (hw_context == 0) ? PSB_CR_BIF_DIR_LIST_BASE0 :
+	    PSB_CR_BIF_DIR_LIST_BASE1 + hw_context * 4;
+
+	drm_ttm_cache_flush();
+	down_write(&pd->driver->sem);
+	psb_iowrite32(pd->driver, (page_to_pfn(pd->p) << PAGE_SHIFT), offset);
+	wmb();
+	psb_mmu_flush_pd_locked(pd->driver, 1);
+	pd->hw_context = hw_context;
+	up_write(&pd->driver->sem);
+
+}
+
+static inline unsigned long psb_pd_addr_end(unsigned long addr,
+					    unsigned long end)
+{
+
+	addr = (addr + PSB_PDE_MASK + 1) & ~PSB_PDE_MASK;
+	return (addr < end) ? addr : end;
+}
+
+static inline uint32_t psb_mmu_mask_pte(uint32_t pfn, int type)
+{
+	uint32_t mask = PSB_PTE_VALID;
+
+	if (type & PSB_MMU_CACHED_MEMORY)
+		mask |= PSB_PTE_CACHED;
+	if (type & PSB_MMU_RO_MEMORY)
+		mask |= PSB_PTE_RO;
+	if (type & PSB_MMU_WO_MEMORY)
+		mask |= PSB_PTE_WO;
+
+	return (pfn << PAGE_SHIFT) | mask;
+}
+
+struct psb_mmu_pd *psb_mmu_alloc_pd(struct psb_mmu_driver *driver,
+				    int trap_pagefaults, int invalid_type)
+{
+	struct psb_mmu_pd *pd = kmalloc(sizeof(*pd), GFP_KERNEL);
+	uint32_t *v;
+	int i;
+
+	if (!pd)
+		return NULL;
+
+	pd->p = alloc_page(GFP_DMA32);
+	if (!pd->p)
+		goto out_err1;
+	pd->dummy_pt = alloc_page(GFP_DMA32);
+	if (!pd->dummy_pt)
+		goto out_err2;
+	pd->dummy_page = alloc_page(GFP_DMA32);
+	if (!pd->dummy_page)
+		goto out_err3;
+
+	if (!trap_pagefaults) {
+		pd->invalid_pde = psb_mmu_mask_pte(page_to_pfn(pd->dummy_pt),
+						   invalid_type |
+						   PSB_MMU_CACHED_MEMORY);
+		pd->invalid_pte = psb_mmu_mask_pte(page_to_pfn(pd->dummy_page),
+						   invalid_type |
+						   PSB_MMU_CACHED_MEMORY);
+	} else {
+		pd->invalid_pde = 0;
+		pd->invalid_pte = 0;
+	}
+
+	v = kmap(pd->dummy_pt);
+	for (i = 0; i < (PAGE_SIZE / sizeof(uint32_t)); ++i) {
+		v[i] = pd->invalid_pte;
+	}
+	kunmap(pd->dummy_pt);
+
+	v = kmap(pd->p);
+	for (i = 0; i < (PAGE_SIZE / sizeof(uint32_t)); ++i) {
+		v[i] = pd->invalid_pde;
+	}
+	kunmap(pd->p);
+
+	clear_page(kmap(pd->dummy_page));
+	kunmap(pd->dummy_page);
+
+	pd->tables = vmalloc_user(sizeof(struct psb_mmu_pt *) * 1024);
+	if (!pd->tables)
+		goto out_err4;
+
+	pd->hw_context = -1;
+	pd->pd_mask = PSB_PTE_VALID;
+	pd->driver = driver;
+
+	return pd;
+
+      out_err4:
+	__free_page(pd->dummy_page);
+      out_err3:
+	__free_page(pd->dummy_pt);
+      out_err2:
+	__free_page(pd->p);
+      out_err1:
+	kfree(pd);
+	return NULL;
+}
+
+void psb_mmu_free_pt(struct psb_mmu_pt *pt)
+{
+	__free_page(pt->p);
+	kfree(pt);
+}
+
+void psb_mmu_free_pagedir(struct psb_mmu_pd *pd)
+{
+	struct psb_mmu_driver *driver = pd->driver;
+	struct psb_mmu_pt *pt;
+	int i;
+
+	down_write(&driver->sem);
+	if (pd->hw_context != -1) {
+		psb_iowrite32(driver, 0,
+			      PSB_CR_BIF_DIR_LIST_BASE0 + pd->hw_context * 4);
+		psb_mmu_flush_pd_locked(driver, 1);
+	}
+
+	/* Should take the spinlock here, but we don't need to do that
+	   since we have the semaphore in write mode. */
+
+	for (i = 0; i < 1024; ++i) {
+		pt = pd->tables[i];
+		if (pt)
+			psb_mmu_free_pt(pt);
+	}
+
+	vfree(pd->tables);
+	__free_page(pd->dummy_page);
+	__free_page(pd->dummy_pt);
+	__free_page(pd->p);
+	kfree(pd);
+	up_write(&driver->sem);
+}
+
+static struct psb_mmu_pt *psb_mmu_alloc_pt(struct psb_mmu_pd *pd)
+{
+	struct psb_mmu_pt *pt = kmalloc(sizeof(*pt), GFP_KERNEL);
+	void *v;
+	uint32_t clflush_add = pd->driver->clflush_add >> PAGE_SHIFT;
+	uint32_t clflush_count = PAGE_SIZE / clflush_add;
+	spinlock_t *lock = &pd->driver->lock;
+	uint8_t *clf;
+	uint32_t *ptes;
+	int i;
+
+	if (!pt)
+		return NULL;
+
+	pt->p = alloc_page(GFP_DMA32);
+	if (!pt->p) {
+		kfree(pt);
+		return NULL;
+	}
+
+	spin_lock(lock);
+
+	v = kmap_atomic(pt->p, KM_USER0);
+	clf = (uint8_t *) v;
+	ptes = (uint32_t *) v;
+	for (i = 0; i < (PAGE_SIZE / sizeof(uint32_t)); ++i) {
+		*ptes++ = pd->invalid_pte;
+	}
+
+#if defined(CONFIG_X86)
+	if (pd->driver->has_clflush && pd->hw_context != -1) {
+		mb();
+		for (i = 0; i < clflush_count; ++i) {
+			psb_clflush(clf);
+			clf += clflush_add;
+		}
+		mb();
+	}
+#endif
+	kunmap_atomic(v, KM_USER0);
+	spin_unlock(lock);
+
+	pt->count = 0;
+	pt->pd = pd;
+	pt->index = 0;
+
+	return pt;
+}
+
+struct psb_mmu_pt *psb_mmu_pt_alloc_map_lock(struct psb_mmu_pd *pd,
+					     unsigned long addr)
+{
+	uint32_t index = psb_mmu_pd_index(addr);
+	struct psb_mmu_pt *pt;
+	volatile uint32_t *v;
+	spinlock_t *lock = &pd->driver->lock;
+
+	spin_lock(lock);
+	pt = pd->tables[index];
+	while (!pt) {
+		spin_unlock(lock);
+		pt = psb_mmu_alloc_pt(pd);
+		if (!pt)
+			return NULL;
+		spin_lock(lock);
+
+		if (pd->tables[index]) {
+			spin_unlock(lock);
+			psb_mmu_free_pt(pt);
+			spin_lock(lock);
+			pt = pd->tables[index];
+			continue;
+		}
+
+		v = kmap_atomic(pd->p, KM_USER0);
+		pd->tables[index] = pt;
+		v[index] = (page_to_pfn(pt->p) << 12) | pd->pd_mask;
+		pt->index = index;
+		kunmap_atomic((void *)v, KM_USER0);
+
+		if (pd->hw_context != -1) {
+			psb_mmu_clflush(pd->driver, (void *)&v[index]);
+			atomic_set(&pd->driver->needs_tlbflush, 1);
+		}
+	}
+	pt->v = kmap_atomic(pt->p, KM_USER0);
+	return pt;
+}
+
+static struct psb_mmu_pt *psb_mmu_pt_map_lock(struct psb_mmu_pd *pd,
+					      unsigned long addr)
+{
+	uint32_t index = psb_mmu_pd_index(addr);
+	struct psb_mmu_pt *pt;
+	spinlock_t *lock = &pd->driver->lock;
+
+	spin_lock(lock);
+	pt = pd->tables[index];
+	if (!pt) {
+		spin_unlock(lock);
+		return NULL;
+	}
+	pt->v = kmap_atomic(pt->p, KM_USER0);
+	return pt;
+}
+
+static void psb_mmu_pt_unmap_unlock(struct psb_mmu_pt *pt)
+{
+	struct psb_mmu_pd *pd = pt->pd;
+	volatile uint32_t *v;
+
+	kunmap_atomic(pt->v, KM_USER0);
+	if (pt->count == 0) {
+		v = kmap_atomic(pd->p, KM_USER0);
+		v[pt->index] = pd->invalid_pde;
+		pd->tables[pt->index] = NULL;
+
+		if (pd->hw_context != -1) {
+			psb_mmu_clflush(pd->driver, (void *)&v[pt->index]);
+			atomic_set(&pd->driver->needs_tlbflush, 1);
+		}
+		kunmap_atomic(pt->v, KM_USER0);
+		spin_unlock(&pd->driver->lock);
+		psb_mmu_free_pt(pt);
+		return;
+	}
+	spin_unlock(&pd->driver->lock);
+}
+
+static inline void psb_mmu_set_pte(struct psb_mmu_pt *pt, unsigned long addr,
+				   uint32_t pte)
+{
+	pt->v[psb_mmu_pt_index(addr)] = pte;
+}
+
+static inline void psb_mmu_invalidate_pte(struct psb_mmu_pt *pt,
+					  unsigned long addr)
+{
+	pt->v[psb_mmu_pt_index(addr)] = pt->pd->invalid_pte;
+}
+
+#if 0
+static uint32_t psb_mmu_check_pte_locked(struct psb_mmu_pd *pd,
+					 uint32_t mmu_offset)
+{
+	uint32_t *v;
+	uint32_t pfn;
+
+	v = kmap_atomic(pd->p, KM_USER0);
+	if (!v) {
+		printk(KERN_INFO "Could not kmap pde page.\n");
+		return 0;
+	}
+	pfn = v[psb_mmu_pd_index(mmu_offset)];
+	//      printk(KERN_INFO "pde is 0x%08x\n",pfn);
+	kunmap_atomic(v, KM_USER0);
+	if (((pfn & 0x0F) != PSB_PTE_VALID)) {
+		printk(KERN_INFO "Strange pde at 0x%08x: 0x%08x.\n",
+		       mmu_offset, pfn);
+	}
+	v = ioremap(pfn & 0xFFFFF000, 4096);
+	if (!v) {
+		printk(KERN_INFO "Could not kmap pte page.\n");
+		return 0;
+	}
+	pfn = v[psb_mmu_pt_index(mmu_offset)];
+	// printk(KERN_INFO "pte is 0x%08x\n",pfn);
+	iounmap(v);
+	if (((pfn & 0x0F) != PSB_PTE_VALID)) {
+		printk(KERN_INFO "Strange pte at 0x%08x: 0x%08x.\n",
+		       mmu_offset, pfn);
+	}
+	return pfn >> PAGE_SHIFT;
+}
+
+static void psb_mmu_check_mirrored_gtt(struct psb_mmu_pd *pd,
+				       uint32_t mmu_offset, uint32_t gtt_pages)
+{
+	uint32_t start;
+	uint32_t next;
+
+	printk(KERN_INFO "Checking mirrored gtt 0x%08x %d\n",
+	       mmu_offset, gtt_pages);
+	down_read(&pd->driver->sem);
+	start = psb_mmu_check_pte_locked(pd, mmu_offset);
+	mmu_offset += PAGE_SIZE;
+	gtt_pages -= 1;
+	while (gtt_pages--) {
+		next = psb_mmu_check_pte_locked(pd, mmu_offset);
+		if (next != start + 1) {
+			printk(KERN_INFO "Ptes out of order: 0x%08x, 0x%08x.\n",
+			       start, next);
+		}
+		start = next;
+		mmu_offset += PAGE_SIZE;
+	}
+	up_read(&pd->driver->sem);
+}
+
+#endif
+
+void psb_mmu_mirror_gtt(struct psb_mmu_pd *pd,
+			uint32_t mmu_offset, uint32_t gtt_start,
+			uint32_t gtt_pages)
+{
+	uint32_t *v;
+	uint32_t start = psb_mmu_pd_index(mmu_offset);
+	struct psb_mmu_driver *driver = pd->driver;
+
+	down_read(&driver->sem);
+	spin_lock(&driver->lock);
+
+	v = kmap_atomic(pd->p, KM_USER0);
+	v += start;
+
+	while (gtt_pages--) {
+		*v++ = gtt_start | pd->pd_mask;
+		gtt_start += PAGE_SIZE;
+	}
+
+	drm_ttm_cache_flush();
+	kunmap_atomic(v, KM_USER0);
+	spin_unlock(&driver->lock);
+
+	if (pd->hw_context != -1)
+		atomic_set(&pd->driver->needs_tlbflush, 1);
+
+	up_read(&pd->driver->sem);
+	psb_mmu_flush_pd(pd->driver, 0);
+}
+
+struct psb_mmu_pd *psb_mmu_get_default_pd(struct psb_mmu_driver *driver)
+{
+	struct psb_mmu_pd *pd;
+
+	down_read(&driver->sem);
+	pd = driver->default_pd;
+	up_read(&driver->sem);
+
+	return pd;
+}
+
+/* Returns the physical address of the PD shared by sgx/msvdx */
+uint32_t psb_get_default_pd_addr(struct psb_mmu_driver * driver)
+{
+	struct psb_mmu_pd *pd;
+
+	pd = psb_mmu_get_default_pd(driver);
+	return ((page_to_pfn(pd->p) << PAGE_SHIFT));
+}
+
+void psb_mmu_driver_takedown(struct psb_mmu_driver *driver)
+{
+	psb_iowrite32(driver, driver->bif_ctrl, PSB_CR_BIF_CTRL);
+	psb_mmu_free_pagedir(driver->default_pd);
+	kfree(driver);
+}
+
+struct psb_mmu_driver *psb_mmu_driver_init(uint8_t __iomem * registers,
+					   int trap_pagefaults,
+					   int invalid_type,
+					   atomic_t *msvdx_mmu_invaldc)
+{
+	struct psb_mmu_driver *driver;
+
+	driver = (struct psb_mmu_driver *)kmalloc(sizeof(*driver), GFP_KERNEL);
+
+	if (!driver)
+		return NULL;
+
+	driver->default_pd = psb_mmu_alloc_pd(driver, trap_pagefaults,
+					      invalid_type);
+	if (!driver->default_pd)
+		goto out_err1;
+
+	spin_lock_init(&driver->lock);
+	init_rwsem(&driver->sem);
+	down_write(&driver->sem);
+	driver->register_map = registers;
+	atomic_set(&driver->needs_tlbflush, 1);
+	driver->msvdx_mmu_invaldc = msvdx_mmu_invaldc;
+
+	driver->bif_ctrl = psb_ioread32(driver, PSB_CR_BIF_CTRL);
+	psb_iowrite32(driver, driver->bif_ctrl | _PSB_CB_CTRL_CLEAR_FAULT,
+		      PSB_CR_BIF_CTRL);
+	psb_iowrite32(driver, driver->bif_ctrl & ~_PSB_CB_CTRL_CLEAR_FAULT,
+		      PSB_CR_BIF_CTRL);
+
+	driver->has_clflush = 0;
+
+#if defined(CONFIG_X86)
+	if (boot_cpu_has(X86_FEATURE_CLFLSH)) {
+		uint32_t tfms, misc, cap0, cap4, clflush_size;
+
+		/*
+		 * clflush size is determined at kernel setup for x86_64 but not for
+		 * i386. We have to do it here.
+		 */
+
+		cpuid(0x00000001, &tfms, &misc, &cap0, &cap4);
+		clflush_size = ((misc >> 8) & 0xff) * 8;
+		driver->has_clflush = 1;
+		driver->clflush_add =
+		    PAGE_SIZE * clflush_size / sizeof(uint32_t);
+		driver->clflush_mask = driver->clflush_add - 1;
+		driver->clflush_mask = ~driver->clflush_mask;
+	}
+#endif
+
+	up_write(&driver->sem);
+	return driver;
+
+      out_err1:
+	kfree(driver);
+	return NULL;
+}
+
+#if defined(CONFIG_X86)
+static void psb_mmu_flush_ptes(struct psb_mmu_pd *pd, unsigned long address,
+			       uint32_t num_pages, uint32_t desired_tile_stride,
+			       uint32_t hw_tile_stride)
+{
+	struct psb_mmu_pt *pt;
+	uint32_t rows = 1;
+	uint32_t i;
+	unsigned long addr;
+	unsigned long end;
+	unsigned long next;
+	unsigned long add;
+	unsigned long row_add;
+	unsigned long clflush_add = pd->driver->clflush_add;
+	unsigned long clflush_mask = pd->driver->clflush_mask;
+
+	if (!pd->driver->has_clflush) {
+		drm_ttm_cache_flush();
+		return;
+	}
+
+	if (hw_tile_stride)
+		rows = num_pages / desired_tile_stride;
+	else
+		desired_tile_stride = num_pages;
+
+	add = desired_tile_stride << PAGE_SHIFT;
+	row_add = hw_tile_stride << PAGE_SHIFT;
+	mb();
+	for (i = 0; i < rows; ++i) {
+
+		addr = address;
+		end = addr + add;
+
+		do {
+			next = psb_pd_addr_end(addr, end);
+			pt = psb_mmu_pt_map_lock(pd, addr);
+			if (!pt)
+				continue;
+			do {
+				psb_clflush(&pt->v[psb_mmu_pt_index(addr)]);
+			} while (addr += clflush_add,
+				 (addr & clflush_mask) < next);
+
+			psb_mmu_pt_unmap_unlock(pt);
+		} while (addr = next, next != end);
+		address += row_add;
+	}
+	mb();
+}
+#else
+static void psb_mmu_flush_ptes(struct psb_mmu_pd *pd, unsigned long address,
+			       uint32_t num_pages, uint32_t desired_tile_stride,
+			       uint32_t hw_tile_stride)
+{
+	drm_ttm_cache_flush();
+}
+#endif
+
+void psb_mmu_remove_pfn_sequence(struct psb_mmu_pd *pd,
+				 unsigned long address, uint32_t num_pages)
+{
+	struct psb_mmu_pt *pt;
+	unsigned long addr;
+	unsigned long end;
+	unsigned long next;
+	unsigned long f_address = address;
+
+	down_read(&pd->driver->sem);
+
+	addr = address;
+	end = addr + (num_pages << PAGE_SHIFT);
+
+	do {
+		next = psb_pd_addr_end(addr, end);
+		pt = psb_mmu_pt_alloc_map_lock(pd, addr);
+		if (!pt)
+			goto out;
+		do {
+			psb_mmu_invalidate_pte(pt, addr);
+			--pt->count;
+		} while (addr += PAGE_SIZE, addr < next);
+		psb_mmu_pt_unmap_unlock(pt);
+
+	} while (addr = next, next != end);
+
+      out:
+	if (pd->hw_context != -1)
+		psb_mmu_flush_ptes(pd, f_address, num_pages, 1, 1);
+
+	up_read(&pd->driver->sem);
+
+	if (pd->hw_context != -1)
+		psb_mmu_flush(pd->driver);
+
+	return;
+}
+
+void psb_mmu_remove_pages(struct psb_mmu_pd *pd, unsigned long address,
+			  uint32_t num_pages, uint32_t desired_tile_stride,
+			  uint32_t hw_tile_stride)
+{
+	struct psb_mmu_pt *pt;
+	uint32_t rows = 1;
+	uint32_t i;
+	unsigned long addr;
+	unsigned long end;
+	unsigned long next;
+	unsigned long add;
+	unsigned long row_add;
+	unsigned long f_address = address;
+
+	if (hw_tile_stride)
+		rows = num_pages / desired_tile_stride;
+	else
+		desired_tile_stride = num_pages;
+
+	add = desired_tile_stride << PAGE_SHIFT;
+	row_add = hw_tile_stride << PAGE_SHIFT;
+
+	down_read(&pd->driver->sem);
+
+	/* Make sure we only need to flush this processor's cache */
+
+	for (i = 0; i < rows; ++i) {
+
+		addr = address;
+		end = addr + add;
+
+		do {
+			next = psb_pd_addr_end(addr, end);
+			pt = psb_mmu_pt_map_lock(pd, addr);
+			if (!pt)
+				continue;
+			do {
+				psb_mmu_invalidate_pte(pt, addr);
+				--pt->count;
+
+			} while (addr += PAGE_SIZE, addr < next);
+			psb_mmu_pt_unmap_unlock(pt);
+
+		} while (addr = next, next != end);
+		address += row_add;
+	}
+	if (pd->hw_context != -1)
+		psb_mmu_flush_ptes(pd, f_address, num_pages,
+				   desired_tile_stride, hw_tile_stride);
+
+	up_read(&pd->driver->sem);
+
+	if (pd->hw_context != -1)
+		psb_mmu_flush(pd->driver);
+}
+
+int psb_mmu_insert_pfn_sequence(struct psb_mmu_pd *pd, uint32_t start_pfn,
+				unsigned long address, uint32_t num_pages,
+				int type)
+{
+	struct psb_mmu_pt *pt;
+	uint32_t pte;
+	unsigned long addr;
+	unsigned long end;
+	unsigned long next;
+	unsigned long f_address = address;
+	int ret = -ENOMEM;
+
+	down_read(&pd->driver->sem);
+
+	addr = address;
+	end = addr + (num_pages << PAGE_SHIFT);
+
+	do {
+		next = psb_pd_addr_end(addr, end);
+		pt = psb_mmu_pt_alloc_map_lock(pd, addr);
+		if (!pt) {
+			ret = -ENOMEM;
+			goto out;
+		}
+		do {
+			pte = psb_mmu_mask_pte(start_pfn++, type);
+			psb_mmu_set_pte(pt, addr, pte);
+			pt->count++;
+		} while (addr += PAGE_SIZE, addr < next);
+		psb_mmu_pt_unmap_unlock(pt);
+
+	} while (addr = next, next != end);
+	ret = 0;
+
+      out:
+	if (pd->hw_context != -1)
+		psb_mmu_flush_ptes(pd, f_address, num_pages, 1, 1);
+
+	up_read(&pd->driver->sem);
+
+	if (pd->hw_context != -1)
+		psb_mmu_flush(pd->driver);
+
+	return 0;
+}
+
+int psb_mmu_insert_pages(struct psb_mmu_pd *pd, struct page **pages,
+			 unsigned long address, uint32_t num_pages,
+			 uint32_t desired_tile_stride, uint32_t hw_tile_stride,
+			 int type)
+{
+	struct psb_mmu_pt *pt;
+	uint32_t rows = 1;
+	uint32_t i;
+	uint32_t pte;
+	unsigned long addr;
+	unsigned long end;
+	unsigned long next;
+	unsigned long add;
+	unsigned long row_add;
+	unsigned long f_address = address;
+	int ret = -ENOMEM;
+
+	if (hw_tile_stride) {
+		if (num_pages % desired_tile_stride != 0)
+			return -EINVAL;
+		rows = num_pages / desired_tile_stride;
+	} else {
+		desired_tile_stride = num_pages;
+	}
+
+	add = desired_tile_stride << PAGE_SHIFT;
+	row_add = hw_tile_stride << PAGE_SHIFT;
+
+	down_read(&pd->driver->sem);
+
+	for (i = 0; i < rows; ++i) {
+
+		addr = address;
+		end = addr + add;
+
+		do {
+			next = psb_pd_addr_end(addr, end);
+			pt = psb_mmu_pt_alloc_map_lock(pd, addr);
+			if (!pt)
+				goto out;
+			do {
+				pte = psb_mmu_mask_pte(page_to_pfn(*pages++),
+						       type);
+				psb_mmu_set_pte(pt, addr, pte);
+				pt->count++;
+			} while (addr += PAGE_SIZE, addr < next);
+			psb_mmu_pt_unmap_unlock(pt);
+
+		} while (addr = next, next != end);
+
+		address += row_add;
+	}
+	ret = 0;
+      out:
+	if (pd->hw_context != -1)
+		psb_mmu_flush_ptes(pd, f_address, num_pages,
+				   desired_tile_stride, hw_tile_stride);
+
+	up_read(&pd->driver->sem);
+
+	if (pd->hw_context != -1)
+		psb_mmu_flush(pd->driver);
+
+	return 0;
+}
+
+void psb_mmu_enable_requestor(struct psb_mmu_driver *driver, uint32_t mask)
+{
+	mask &= _PSB_MMU_ER_MASK;
+	psb_iowrite32(driver, psb_ioread32(driver, PSB_CR_BIF_CTRL) & ~mask,
+		      PSB_CR_BIF_CTRL);
+	(void)psb_ioread32(driver, PSB_CR_BIF_CTRL);
+}
+
+void psb_mmu_disable_requestor(struct psb_mmu_driver *driver, uint32_t mask)
+{
+	mask &= _PSB_MMU_ER_MASK;
+	psb_iowrite32(driver, psb_ioread32(driver, PSB_CR_BIF_CTRL) | mask,
+		      PSB_CR_BIF_CTRL);
+	(void)psb_ioread32(driver, PSB_CR_BIF_CTRL);
+}
+
+int psb_mmu_virtual_to_pfn(struct psb_mmu_pd *pd, uint32_t virtual,
+			   unsigned long *pfn)
+{
+	int ret;
+	struct psb_mmu_pt *pt;
+	uint32_t tmp;
+	spinlock_t *lock = &pd->driver->lock;
+
+	down_read(&pd->driver->sem);
+	pt = psb_mmu_pt_map_lock(pd, virtual);
+	if (!pt) {
+		uint32_t *v;
+
+		spin_lock(lock);
+		v = kmap_atomic(pd->p, KM_USER0);
+		tmp = v[psb_mmu_pd_index(virtual)];
+		kunmap_atomic(v, KM_USER0);
+		spin_unlock(lock);
+
+		if (tmp != pd->invalid_pde || !(tmp & PSB_PTE_VALID) ||
+		    !(pd->invalid_pte & PSB_PTE_VALID)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = 0;
+		*pfn = pd->invalid_pte >> PAGE_SHIFT;
+		goto out;
+	}
+	tmp = pt->v[psb_mmu_pt_index(virtual)];
+	if (!(tmp & PSB_PTE_VALID)) {
+		ret = -EINVAL;
+	} else {
+		ret = 0;
+		*pfn = tmp >> PAGE_SHIFT;
+	}
+	psb_mmu_pt_unmap_unlock(pt);
+      out:
+	up_read(&pd->driver->sem);
+	return ret;
+}
+
+void psb_mmu_test(struct psb_mmu_driver *driver, uint32_t offset)
+{
+	struct page *p;
+	unsigned long pfn;
+	int ret = 0;
+	struct psb_mmu_pd *pd;
+	uint32_t *v;
+	uint32_t *vmmu;
+
+	pd = driver->default_pd;
+	if (!pd) {
+		printk(KERN_WARNING "Could not get default pd\n");
+	}
+
+	p = alloc_page(GFP_DMA32);
+
+	if (!p) {
+		printk(KERN_WARNING "Failed allocating page\n");
+		return;
+	}
+
+	v = kmap(p);
+	memset(v, 0x67, PAGE_SIZE);
+
+	pfn = (offset >> PAGE_SHIFT);
+
+	ret = psb_mmu_insert_pages(pd, &p, pfn << PAGE_SHIFT, 1, 0, 0,
+				   PSB_MMU_CACHED_MEMORY);
+	if (ret) {
+		printk(KERN_WARNING "Failed inserting mmu page\n");
+		goto out_err1;
+	}
+
+	/* Ioremap the page through the GART aperture */
+
+	vmmu = ioremap(pfn << PAGE_SHIFT, PAGE_SIZE);
+	if (!vmmu) {
+		printk(KERN_WARNING "Failed ioremapping page\n");
+		goto out_err2;
+	}
+
+	/* Read from the page with mmu disabled. */
+	printk(KERN_INFO "Page first dword is 0x%08x\n", ioread32(vmmu));
+
+	/* Enable the mmu for host accesses and read again. */
+	psb_mmu_enable_requestor(driver, _PSB_MMU_ER_HOST);
+
+	printk(KERN_INFO "MMU Page first dword is (0x67676767) 0x%08x\n",
+	       ioread32(vmmu));
+	*v = 0x15243705;
+	printk(KERN_INFO "MMU Page new dword is (0x15243705) 0x%08x\n",
+	       ioread32(vmmu));
+	iowrite32(0x16243355, vmmu);
+	(void)ioread32(vmmu);
+	printk(KERN_INFO "Page new dword is (0x16243355) 0x%08x\n", *v);
+
+	printk(KERN_INFO "Int stat is 0x%08x\n",
+	       psb_ioread32(driver, PSB_CR_BIF_INT_STAT));
+	printk(KERN_INFO "Fault is 0x%08x\n",
+	       psb_ioread32(driver, PSB_CR_BIF_FAULT));
+
+	/* Disable MMU for host accesses and clear page fault register */
+	psb_mmu_disable_requestor(driver, _PSB_MMU_ER_HOST);
+	iounmap(vmmu);
+      out_err2:
+	psb_mmu_remove_pages(pd, pfn << PAGE_SHIFT, 1, 0, 0);
+      out_err1:
+	kunmap(p);
+	__free_page(p);
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_msvdx.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_msvdx.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,676 @@
+/**
+ * file psb_msvdx.c
+ * MSVDX I/O operations and IRQ handling
+ *
+ */
+
+/**************************************************************************
+ *
+ * Copyright (c) 2007 Intel Corporation, Hillsboro, OR, USA
+ * Copyright (c) Imagination Technologies Limited, UK
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+#include "drmP.h"
+#include "drm_os_linux.h"
+#include "psb_drv.h"
+#include "psb_drm.h"
+#include "psb_msvdx.h"
+
+#include <asm/io.h>
+#include <linux/delay.h>
+
+#ifndef list_first_entry
+#define list_first_entry(ptr, type, member) \
+	list_entry((ptr)->next, type, member)
+#endif
+
+static int psb_msvdx_send (struct drm_device *dev, void *cmd,
+			   unsigned long cmd_size);
+
+int
+psb_msvdx_dequeue_send (struct drm_device *dev)
+{
+  struct drm_psb_private *dev_priv = dev->dev_private;
+  struct psb_msvdx_cmd_queue *msvdx_cmd = NULL;
+  int ret = 0;
+
+  if (list_empty (&dev_priv->msvdx_queue))
+    {
+      PSB_DEBUG_GENERAL ("MSVDXQUE: msvdx list empty.\n");
+      dev_priv->msvdx_busy = 0;
+      return -EINVAL;
+    }
+  msvdx_cmd =
+    list_first_entry (&dev_priv->msvdx_queue, struct psb_msvdx_cmd_queue,
+		      head);
+  PSB_DEBUG_GENERAL ("MSVDXQUE: Queue has id %08x\n", msvdx_cmd->sequence);
+  ret = psb_msvdx_send (dev, msvdx_cmd->cmd, msvdx_cmd->cmd_size);
+  if (ret)
+    {
+      PSB_DEBUG_GENERAL ("MSVDXQUE: psb_msvdx_send failed\n");
+      ret = -EINVAL;
+    }
+  list_del (&msvdx_cmd->head);
+  kfree (msvdx_cmd->cmd);
+  drm_free (msvdx_cmd, sizeof (struct psb_msvdx_cmd_queue), DRM_MEM_DRIVER);
+  return ret;
+}
+
+int
+psb_msvdx_map_command (struct drm_device *dev,
+		       struct drm_buffer_object *cmd_buffer,
+		       unsigned long cmd_offset, unsigned long cmd_size,
+		       void **msvdx_cmd, uint32_t sequence, int copy_cmd)
+{
+  struct drm_psb_private *dev_priv = dev->dev_private;
+  int ret = 0;
+  unsigned long cmd_page_offset = cmd_offset & ~PAGE_MASK;
+  unsigned long cmd_size_remaining;
+  struct drm_bo_kmap_obj cmd_kmap;
+  void *cmd, *tmp, *cmd_start;
+  int is_iomem;
+
+  /* command buffers may not exceed page boundary */
+  if (cmd_size + cmd_page_offset > PAGE_SIZE)
+    return -EINVAL;
+
+  ret = drm_bo_kmap (cmd_buffer, cmd_offset >> PAGE_SHIFT, 2, &cmd_kmap);
+
+  if (ret)
+    {
+      PSB_DEBUG_GENERAL ("MSVDXQUE:ret:%d\n", ret);
+      return ret;
+    }
+
+  cmd_start =
+    (void *) drm_bmo_virtual (&cmd_kmap, &is_iomem) + cmd_page_offset;
+  cmd = cmd_start;
+  cmd_size_remaining = cmd_size;
+
+  while (cmd_size_remaining > 0)
+    {
+      uint32_t mmu_ptd;
+      uint32_t cur_cmd_size = MEMIO_READ_FIELD (cmd, FWRK_GENMSG_SIZE);
+      uint32_t cur_cmd_id = MEMIO_READ_FIELD (cmd, FWRK_GENMSG_ID);
+      PSB_DEBUG_GENERAL
+	("cmd start at %08x cur_cmd_size = %d cur_cmd_id = %02x fence = %08x\n",
+	 (uint32_t) cmd, cur_cmd_size, cur_cmd_id, sequence);
+      if ((cur_cmd_size % sizeof (uint32_t))
+	  || (cur_cmd_size > cmd_size_remaining))
+	{
+	  ret = -EINVAL;
+	  PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+	  goto out;
+	}
+
+      switch (cur_cmd_id)
+	{
+	case VA_MSGID_RENDER:
+	  /* Fence ID */
+	  MEMIO_WRITE_FIELD (cmd, FW_VA_RENDER_FENCE_VALUE, sequence);
+
+	  mmu_ptd = psb_get_default_pd_addr (dev_priv->mmu);
+          if (atomic_cmpxchg(&dev_priv->msvdx_mmu_invaldc, 1, 0) == 1)
+            {
+                mmu_ptd |= 1;
+                PSB_DEBUG_GENERAL ("MSVDX: Setting MMU invalidate flag\n");
+	    }
+	  /* PTD */
+	  MEMIO_WRITE_FIELD (cmd, FW_VA_RENDER_MMUPTD, mmu_ptd);
+	  break;
+
+	default:
+	  /* Msg not supported */
+	  ret = -EINVAL;
+	  PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+	  goto out;
+	}
+
+      cmd += cur_cmd_size;
+      cmd_size_remaining -= cur_cmd_size;
+    }
+
+  if (copy_cmd)
+    {
+      PSB_DEBUG_GENERAL
+	("MSVDXQUE: psb_msvdx_map_command copying command...\n");
+      tmp = drm_calloc (1, cmd_size, DRM_MEM_DRIVER);
+      if (tmp == NULL)
+	{
+	  ret = -ENOMEM;
+	  PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+	  goto out;
+	}
+      memcpy (tmp, cmd_start, cmd_size);
+      *msvdx_cmd = tmp;
+    }
+  else
+    {
+      PSB_DEBUG_GENERAL
+	("MSVDXQUE: psb_msvdx_map_command did NOT copy command...\n");
+      ret = psb_msvdx_send (dev, cmd_start, cmd_size);
+      if (ret)
+	{
+	  PSB_DEBUG_GENERAL ("MSVDXQUE: psb_msvdx_send failed\n");
+	  ret = -EINVAL;
+	}
+    }
+
+out:
+  drm_bo_kunmap (&cmd_kmap);
+
+  return ret;
+}
+
+int
+psb_submit_video_cmdbuf (struct drm_device *dev,
+			 struct drm_buffer_object *cmd_buffer,
+			 unsigned long cmd_offset, unsigned long cmd_size,
+			 struct drm_fence_object *fence)
+{
+  struct drm_psb_private *dev_priv = dev->dev_private;
+  uint32_t sequence = fence->sequence;
+  unsigned long irq_flags;
+  int ret = 0;
+
+  mutex_lock (&dev_priv->msvdx_mutex);
+  psb_schedule_watchdog (dev_priv);
+
+  spin_lock_irqsave (&dev_priv->msvdx_lock, irq_flags);
+  if (dev_priv->msvdx_needs_reset)
+    {
+      spin_unlock_irqrestore (&dev_priv->msvdx_lock, irq_flags);
+      PSB_DEBUG_GENERAL ("MSVDX: Needs reset\n");
+      if (psb_msvdx_reset (dev_priv))
+	{
+	  mutex_unlock (&dev_priv->msvdx_mutex);
+	  ret = -EBUSY;
+	  PSB_DEBUG_GENERAL ("MSVDX: Reset failed\n");
+	  return ret;
+	}
+      PSB_DEBUG_GENERAL ("MSVDX: Reset ok\n");
+      dev_priv->msvdx_needs_reset = 0;
+      dev_priv->msvdx_busy = 0;
+      dev_priv->msvdx_start_idle = 0;
+
+      psb_msvdx_init (dev);
+      psb_msvdx_irq_preinstall (dev_priv);
+      psb_msvdx_irq_postinstall (dev_priv);
+      PSB_DEBUG_GENERAL ("MSVDX: Init ok\n");
+      spin_lock_irqsave (&dev_priv->msvdx_lock, irq_flags);
+    }
+
+  if (!dev_priv->msvdx_busy)
+    {
+      dev_priv->msvdx_busy = 1;
+      spin_unlock_irqrestore (&dev_priv->msvdx_lock, irq_flags);
+      PSB_DEBUG_GENERAL
+	("MSVDXQUE: nothing in the queue sending sequence:%08x..\n",
+	 sequence);
+      ret =
+	psb_msvdx_map_command (dev, cmd_buffer, cmd_offset, cmd_size,
+			       NULL, sequence, 0);
+      if (ret)
+	{
+	  mutex_unlock (&dev_priv->msvdx_mutex);
+	  PSB_DEBUG_GENERAL ("MSVDXQUE: Failed to extract cmd...\n");
+	  return ret;
+	}
+    }
+  else
+    {
+      struct psb_msvdx_cmd_queue *msvdx_cmd;
+      void *cmd = NULL;
+
+      spin_unlock_irqrestore (&dev_priv->msvdx_lock, irq_flags);
+      /*queue the command to be sent when the h/w is ready */
+      PSB_DEBUG_GENERAL ("MSVDXQUE: queueing sequence:%08x..\n", sequence);
+      msvdx_cmd =
+	drm_calloc (1, sizeof (struct psb_msvdx_cmd_queue), DRM_MEM_DRIVER);
+      if (msvdx_cmd == NULL)
+	{
+	  mutex_unlock (&dev_priv->msvdx_mutex);
+	  PSB_DEBUG_GENERAL ("MSVDXQUE: Out of memory...\n");
+	  return -ENOMEM;
+	}
+
+      ret =
+	psb_msvdx_map_command (dev, cmd_buffer, cmd_offset, cmd_size,
+			       &cmd, sequence, 1);
+      if (ret)
+	{
+	  mutex_unlock (&dev_priv->msvdx_mutex);
+	  PSB_DEBUG_GENERAL ("MSVDXQUE: Failed to extract cmd...\n");
+	  drm_free (msvdx_cmd, sizeof (struct psb_msvdx_cmd_queue),
+		    DRM_MEM_DRIVER);
+	  return ret;
+	}
+      msvdx_cmd->cmd = cmd;
+      msvdx_cmd->cmd_size = cmd_size;
+      msvdx_cmd->sequence = sequence;
+      spin_lock_irqsave (&dev_priv->msvdx_lock, irq_flags);
+      list_add_tail (&msvdx_cmd->head, &dev_priv->msvdx_queue);
+      if (!dev_priv->msvdx_busy)
+	{
+	  dev_priv->msvdx_busy = 1;
+	  PSB_DEBUG_GENERAL ("MSVDXQUE: Need immediate dequeue\n");
+	  psb_msvdx_dequeue_send (dev);
+	}
+      spin_unlock_irqrestore (&dev_priv->msvdx_lock, irq_flags);
+    }
+  mutex_unlock (&dev_priv->msvdx_mutex);
+  return ret;
+}
+
+int
+psb_msvdx_send (struct drm_device *dev, void *cmd, unsigned long cmd_size)
+{
+  int ret = 0;
+  struct drm_psb_private *dev_priv = dev->dev_private;
+
+  while (cmd_size > 0)
+    {
+      uint32_t cur_cmd_size = MEMIO_READ_FIELD (cmd, FWRK_GENMSG_SIZE);
+      if (cur_cmd_size > cmd_size)
+	{
+	  ret = -EINVAL;
+	  PSB_DEBUG_GENERAL
+	    ("MSVDX: cmd_size = %d cur_cmd_size = %d\n",
+	     (int) cmd_size, cur_cmd_size);
+	  goto out;
+	}
+      /* Send the message to h/w */
+      ret = psb_mtx_send (dev_priv, cmd);
+      if (ret)
+	{
+	  PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+	  goto out;
+	}
+      cmd += cur_cmd_size;
+      cmd_size -= cur_cmd_size;
+    }
+
+out:
+  PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+  return ret;
+}
+
+/***********************************************************************************
+ * Function Name      : psb_mtx_send
+ * Inputs             :
+ * Outputs            :
+ * Returns            :
+ * Description        :
+ ************************************************************************************/
+int
+psb_mtx_send (struct drm_psb_private *dev_priv, const void *pvMsg)
+{
+
+  static uint32_t padMessage[FWRK_PADMSG_SIZE];
+
+  const uint32_t *pui32Msg = (uint32_t *) pvMsg;
+  uint32_t msgNumWords, wordsFree, readIndex, writeIndex;
+  int ret = 0;
+
+  PSB_DEBUG_GENERAL ("MSVDX: psb_mtx_send\n");
+
+  /* we need clocks enabled before we touch VEC local ram */
+  PSB_WMSVDX32 (clk_enable_all, MSVDX_MAN_CLK_ENABLE);
+
+  msgNumWords = (MEMIO_READ_FIELD (pvMsg, FWRK_GENMSG_SIZE) + 3) / 4;
+
+  if (msgNumWords > NUM_WORDS_MTX_BUF)
+    {
+      ret = -EINVAL;
+      PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+      goto out;
+    }
+
+  readIndex = PSB_RMSVDX32 (MSVDX_COMMS_TO_MTX_RD_INDEX);
+  writeIndex = PSB_RMSVDX32 (MSVDX_COMMS_TO_MTX_WRT_INDEX);
+
+  if (writeIndex + msgNumWords > NUM_WORDS_MTX_BUF)
+    {				/* message would wrap, need to send a pad message */
+      BUG_ON (MEMIO_READ_FIELD (pvMsg, FWRK_GENMSG_ID) == FWRK_MSGID_PADDING);	/* Shouldn't happen for a PAD message itself */
+      /* if the read pointer is at zero then we must wait for it to change otherwise the write
+       * pointer will equal the read pointer,which should only happen when the buffer is empty
+       *
+       * This will only happens if we try to overfill the queue, queue management should make
+       * sure this never happens in the first place.
+       */
+      BUG_ON (0 == readIndex);
+      if (0 == readIndex)
+	{
+	  ret = -EINVAL;
+	  PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+	  goto out;
+	}
+      /* Send a pad message */
+      MEMIO_WRITE_FIELD (padMessage, FWRK_GENMSG_SIZE,
+			 (NUM_WORDS_MTX_BUF - writeIndex) << 2);
+      MEMIO_WRITE_FIELD (padMessage, FWRK_GENMSG_ID, FWRK_MSGID_PADDING);
+      psb_mtx_send (dev_priv, padMessage);
+      writeIndex = PSB_RMSVDX32 (MSVDX_COMMS_TO_MTX_WRT_INDEX);
+    }
+
+  wordsFree =
+    (writeIndex >=
+     readIndex) ? NUM_WORDS_MTX_BUF - (writeIndex -
+				       readIndex) : readIndex - writeIndex;
+
+  BUG_ON (msgNumWords > wordsFree);
+  if (msgNumWords > wordsFree)
+    {
+      ret = -EINVAL;
+      PSB_DEBUG_GENERAL ("MSVDX: ret:%d\n", ret);
+      goto out;
+    }
+
+  while (msgNumWords > 0)
+    {
+      PSB_WMSVDX32 (*pui32Msg++, MSVDX_COMMS_TO_MTX_BUF + (writeIndex << 2));
+      msgNumWords--;
+      writeIndex++;
+      if (NUM_WORDS_MTX_BUF == writeIndex)
+	{
+	  writeIndex = 0;
+	}
+    }
+  PSB_WMSVDX32 (writeIndex, MSVDX_COMMS_TO_MTX_WRT_INDEX);
+
+  /* Make sure clocks are enabled before we kick */
+  PSB_WMSVDX32 (clk_enable_all, MSVDX_MAN_CLK_ENABLE);
+
+  /* signal an interrupt to let the mtx know there is a new message */
+  PSB_WMSVDX32 (1, MSVDX_MTX_KICKI);
+
+out:
+  return ret;
+}
+
+/*
+ * MSVDX MTX interrupt
+ */
+void
+psb_msvdx_mtx_interrupt (struct drm_device *dev)
+{
+  static uint32_t msgBuffer[128];
+  uint32_t readIndex, writeIndex;
+  uint32_t msgNumWords, msgWordOffset;
+  struct drm_psb_private *dev_priv =
+    (struct drm_psb_private *) dev->dev_private;
+
+  /* Are clocks enabled  - If not enable before attempting to read from VLR */
+  if (PSB_RMSVDX32 (MSVDX_MAN_CLK_ENABLE) != (clk_enable_all))
+    {
+      PSB_DEBUG_GENERAL
+	("MSVDX: Warning - Clocks disabled when Interupt set\n");
+      PSB_WMSVDX32 (clk_enable_all, MSVDX_MAN_CLK_ENABLE);
+    }
+
+  for (;;)
+    {
+      readIndex = PSB_RMSVDX32 (MSVDX_COMMS_TO_HOST_RD_INDEX);
+      writeIndex = PSB_RMSVDX32 (MSVDX_COMMS_TO_HOST_WRT_INDEX);
+
+      if (readIndex != writeIndex)
+	{
+	  msgWordOffset = 0;
+
+	  msgBuffer[msgWordOffset] =
+	    PSB_RMSVDX32 (MSVDX_COMMS_TO_HOST_BUF + (readIndex << 2));
+
+	  msgNumWords = (MEMIO_READ_FIELD (msgBuffer, FWRK_GENMSG_SIZE) + 3) / 4;	/* round to nearest word */
+
+	  /*ASSERT(msgNumWords <= sizeof(msgBuffer) / sizeof(uint32_t)); */
+
+	  if (++readIndex >= NUM_WORDS_HOST_BUF)
+	    readIndex = 0;
+
+	  for (msgWordOffset++; msgWordOffset < msgNumWords; msgWordOffset++)
+	    {
+	      msgBuffer[msgWordOffset] =
+		PSB_RMSVDX32 (MSVDX_COMMS_TO_HOST_BUF + (readIndex << 2));
+
+	      if (++readIndex >= NUM_WORDS_HOST_BUF)
+		{
+		  readIndex = 0;
+		}
+	    }
+
+	  /* Update the Read index */
+	  PSB_WMSVDX32 (readIndex, MSVDX_COMMS_TO_HOST_RD_INDEX);
+
+	  if (!dev_priv->msvdx_needs_reset)
+	    switch (MEMIO_READ_FIELD (msgBuffer, FWRK_GENMSG_ID))
+	      {
+	      case VA_MSGID_CMD_HW_PANIC:
+	      case VA_MSGID_CMD_FAILED:
+		{
+		  uint32_t ui32Fence = MEMIO_READ_FIELD (msgBuffer,
+							 FW_VA_CMD_FAILED_FENCE_VALUE);
+		  uint32_t ui32FaultStatus = MEMIO_READ_FIELD (msgBuffer,
+							       FW_VA_CMD_FAILED_IRQSTATUS);
+
+		 if(MEMIO_READ_FIELD (msgBuffer, FWRK_GENMSG_ID) == VA_MSGID_CMD_HW_PANIC )
+		  PSB_DEBUG_GENERAL
+		    ("MSVDX: VA_MSGID_CMD_HW_PANIC: Msvdx fault detected - Fence: %08x, Status: %08x - resetting and ignoring error\n",
+		     ui32Fence, ui32FaultStatus);
+		 else
+		  PSB_DEBUG_GENERAL
+		    ("MSVDX: VA_MSGID_CMD_FAILED: Msvdx fault detected - Fence: %08x, Status: %08x - resetting and ignoring error\n",
+		     ui32Fence, ui32FaultStatus);
+
+		  dev_priv->msvdx_needs_reset = 1;
+
+		 if(MEMIO_READ_FIELD (msgBuffer, FWRK_GENMSG_ID) == VA_MSGID_CMD_HW_PANIC)
+                   {
+                     if (dev_priv->
+                         msvdx_current_sequence
+                         - dev_priv->sequence[PSB_ENGINE_VIDEO] > 0x0FFFFFFF)
+                       dev_priv->msvdx_current_sequence++;
+                     PSB_DEBUG_GENERAL
+                       ("MSVDX: Fence ID missing, assuming %08x\n",
+                        dev_priv->msvdx_current_sequence);
+                   }
+		 else
+	           dev_priv->msvdx_current_sequence = ui32Fence;
+
+		  psb_fence_error (dev,
+				   PSB_ENGINE_VIDEO,
+				   dev_priv->
+				   msvdx_current_sequence,
+				   DRM_FENCE_TYPE_EXE, DRM_CMD_FAILED);
+
+		  /* Flush the command queue */
+		  psb_msvdx_flush_cmd_queue (dev);
+
+		  goto isrExit;
+		  break;
+		}
+	      case VA_MSGID_CMD_COMPLETED:
+		{
+		  uint32_t ui32Fence = MEMIO_READ_FIELD (msgBuffer,
+							 FW_VA_CMD_COMPLETED_FENCE_VALUE);
+		  uint32_t ui32Flags =
+		    MEMIO_READ_FIELD (msgBuffer, FW_VA_CMD_COMPLETED_FLAGS);
+
+		  PSB_DEBUG_GENERAL
+		    ("msvdx VA_MSGID_CMD_COMPLETED: FenceID: %08x, flags: 0x%x\n",
+		     ui32Fence, ui32Flags);
+		  dev_priv->msvdx_current_sequence = ui32Fence;
+
+		  psb_fence_handler (dev, PSB_ENGINE_VIDEO);
+
+
+		  if (ui32Flags & FW_VA_RENDER_HOST_INT)
+		    {
+		      /*Now send the next command from the msvdx cmd queue */
+		      psb_msvdx_dequeue_send (dev);
+		      goto isrExit;
+		    }
+		  break;
+		}
+	      case VA_MSGID_ACK:
+		PSB_DEBUG_GENERAL ("msvdx VA_MSGID_ACK\n");
+		break;
+
+	      case VA_MSGID_TEST1:
+		PSB_DEBUG_GENERAL ("msvdx VA_MSGID_TEST1\n");
+		break;
+
+	      case VA_MSGID_TEST2:
+		PSB_DEBUG_GENERAL ("msvdx VA_MSGID_TEST2\n");
+		break;
+		/* Don't need to do anything with these messages */
+
+	      case VA_MSGID_DEBLOCK_REQUIRED:
+		{
+		  uint32_t ui32ContextId = MEMIO_READ_FIELD (msgBuffer,
+							     FW_VA_DEBLOCK_REQUIRED_CONTEXT);
+
+		  /* The BE we now be locked. */
+
+		  /* Unblock rendec by reading the mtx2mtx end of slice */
+		  (void) PSB_RMSVDX32 (MSVDX_RENDEC_READ_DATA);
+
+		  PSB_DEBUG_GENERAL
+		    ("msvdx VA_MSGID_DEBLOCK_REQUIRED Context=%08x\n",
+		     ui32ContextId);
+		  goto isrExit;
+		  break;
+		}
+
+	      default:
+		{
+		  PSB_DEBUG_GENERAL
+		    ("ERROR: msvdx Unknown message from MTX \n");
+		}
+		break;
+
+	      }
+	}
+      else
+	{
+	  /* Get out of here if nothing */
+	  break;
+	}
+    }
+isrExit:
+
+#if 1
+  if (!dev_priv->msvdx_busy)
+  {
+    /* check that clocks are enabled before reading VLR */
+    if( PSB_RMSVDX32( MSVDX_MAN_CLK_ENABLE ) != (clk_enable_all) )
+        PSB_WMSVDX32 (clk_enable_all, MSVDX_MAN_CLK_ENABLE);
+
+   /* If the firmware says the hardware is idle and the CCB is empty then we can power down */
+   uint32_t ui32FWStatus = PSB_RMSVDX32( MSVDX_COMMS_FW_STATUS );
+   uint32_t ui32CCBRoff = PSB_RMSVDX32 ( MSVDX_COMMS_TO_MTX_RD_INDEX );
+   uint32_t ui32CCBWoff = PSB_RMSVDX32 ( MSVDX_COMMS_TO_MTX_WRT_INDEX );
+
+   if( (ui32FWStatus & MSVDX_FW_STATUS_HW_IDLE) && (ui32CCBRoff == ui32CCBWoff))
+   {
+	PSB_DEBUG_GENERAL("MSVDX_CLOCK: Setting clock to minimal...\n");
+        PSB_WMSVDX32 (clk_enable_minimal, MSVDX_MAN_CLK_ENABLE);
+   }
+   }
+#endif
+  DRM_MEMORYBARRIER ();
+}
+
+void
+psb_msvdx_lockup (struct drm_psb_private *dev_priv,
+		  int *msvdx_lockup, int *msvdx_idle)
+{
+	unsigned long irq_flags;
+//	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+
+  spin_lock_irqsave (&dev_priv->msvdx_lock, irq_flags);
+  *msvdx_lockup = 0;
+  *msvdx_idle = 1;
+
+  if (!dev_priv->has_msvdx)
+  {
+      spin_unlock_irqrestore (&dev_priv->msvdx_lock, irq_flags);
+      return;
+  }
+#if 0
+  PSB_DEBUG_GENERAL ("MSVDXTimer: current_sequence:%d "
+		     "last_sequence:%d and last_submitted_sequence :%d\n",
+		     dev_priv->msvdx_current_sequence,
+		     dev_priv->msvdx_last_sequence,
+		     dev_priv->sequence[PSB_ENGINE_VIDEO]);
+#endif
+  if (dev_priv->msvdx_current_sequence -
+      dev_priv->sequence[PSB_ENGINE_VIDEO] > 0x0FFFFFFF)
+    {
+
+      if (dev_priv->msvdx_current_sequence == dev_priv->msvdx_last_sequence)
+	{
+	  PSB_DEBUG_GENERAL
+	    ("MSVDXTimer: msvdx locked-up for sequence:%d\n",
+	     dev_priv->msvdx_current_sequence);
+	  *msvdx_lockup = 1;
+	}
+      else
+	{
+	  PSB_DEBUG_GENERAL ("MSVDXTimer: msvdx responded fine so far...\n");
+	  dev_priv->msvdx_last_sequence = dev_priv->msvdx_current_sequence;
+	  *msvdx_idle = 0;
+	}
+	if (dev_priv->msvdx_start_idle)
+		dev_priv->msvdx_start_idle = 0;
+    }
+    else
+    {
+	if (dev_priv->msvdx_needs_reset == 0)
+	{
+	    if (dev_priv->msvdx_start_idle && (dev_priv->msvdx_finished_sequence == dev_priv->msvdx_current_sequence))
+	    {
+		//if (dev_priv->msvdx_idle_start_jiffies + MSVDX_MAX_IDELTIME >= jiffies)
+		if (time_after_eq(jiffies, dev_priv->msvdx_idle_start_jiffies + MSVDX_MAX_IDELTIME))
+		{
+		    printk("set the msvdx clock to 0 in the %s\n", __FUNCTION__);
+      		    PSB_WMSVDX32 (0, MSVDX_MAN_CLK_ENABLE);
+		    dev_priv->msvdx_needs_reset = 1;
+		}
+		else
+		{
+		    *msvdx_idle = 0;
+		}
+	    }
+	    else
+	    {
+		dev_priv->msvdx_start_idle = 1;
+		dev_priv->msvdx_idle_start_jiffies = jiffies;
+		dev_priv->msvdx_finished_sequence = dev_priv->msvdx_current_sequence;
+		*msvdx_idle = 0;
+	    }
+	}
+    }
+    spin_unlock_irqrestore (&dev_priv->msvdx_lock, irq_flags);
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_msvdx.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_msvdx.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,564 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2007 Intel Corporation, Hillsboro, OR, USA
+ * Copyright (c) Imagination Technologies Limited, UK
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+#ifndef _PSB_MSVDX_H_
+#define _PSB_MSVDX_H_
+
+#define assert(expr) \
+        if(unlikely(!(expr))) {                                   \
+        printk(KERN_ERR "Assertion failed! %s,%s,%s,line=%d\n", \
+        #expr,__FILE__,__FUNCTION__,__LINE__);          \
+        }
+
+#define PSB_ASSERT(x) assert (x)
+#define IMG_ASSERT(x) assert (x)
+
+#include "psb_drv.h"
+int
+psb_wait_for_register (struct drm_psb_private *dev_priv,
+                       uint32_t ui32Offset,
+                       uint32_t ui32Value, uint32_t ui32Enable);
+
+void psb_msvdx_mtx_interrupt (struct drm_device *dev);
+int psb_msvdx_init (struct drm_device *dev);
+int psb_msvdx_uninit (struct drm_device *dev);
+int psb_msvdx_reset (struct drm_psb_private *dev_priv);
+uint32_t psb_get_default_pd_addr (struct psb_mmu_driver *driver);
+int psb_mtx_send (struct drm_psb_private *dev_priv, const void *pvMsg);
+void psb_msvdx_irq_preinstall (struct drm_psb_private *dev_priv);
+void psb_msvdx_irq_postinstall (struct drm_psb_private *dev_priv);
+void psb_msvdx_flush_cmd_queue (struct drm_device *dev);
+extern void psb_msvdx_lockup (struct drm_psb_private *dev_priv,
+			      int *msvdx_lockup, int *msvdx_idle);
+#define MSVDX_DEVICE_NODE_FLAGS_MMU_NONOPT_INV	2	/*  Non-Optimal Invalidation is not default */
+#define FW_VA_RENDER_HOST_INT		0x00004000
+#define MSVDX_DEVICE_NODE_FLAGS_MMU_HW_INVALIDATION	0x00000020
+
+#define MSVDX_DEVICE_NODE_FLAG_BRN23154_BLOCK_ON_FE	0x00000200
+
+#define MSVDX_DEVICE_NODE_FLAGS_DEFAULT_D0 (MSVDX_DEVICE_NODE_FLAGS_MMU_NONOPT_INV | MSVDX_DEVICE_NODE_FLAGS_MMU_HW_INVALIDATION \
+						| MSVDX_DEVICE_NODE_FLAG_BRN23154_BLOCK_ON_FE)
+#define MSVDX_DEVICE_NODE_FLAGS_DEFAULT_D1 (MSVDX_DEVICE_NODE_FLAGS_MMU_HW_INVALIDATION \
+                                                | MSVDX_DEVICE_NODE_FLAG_BRN23154_BLOCK_ON_FE)
+
+
+#define POULSBO_D0	0x5
+#define POULSBO_D1	0x6
+#define PSB_REVID_OFFSET 0x8
+
+#define MSVDX_FW_STATUS_HW_IDLE	0x00000001 /* There is no work currently underway on the hardware*/
+
+#define clk_enable_all		MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_CORE_MAN_CLK_ENABLE_MASK			|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDEB_PROCESS_MAN_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDEB_ACCESS_MAN_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDMC_MAN_CLK_ENABLE_MASK			|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ENTDEC_MAN_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ITRANS_MAN_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_MTX_MAN_CLK_ENABLE_MASK
+
+#define clk_enable_minimal	MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_CORE_MAN_CLK_ENABLE_MASK | \
+				MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_MTX_MAN_CLK_ENABLE_MASK
+
+#define clk_enable_auto		MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDEB_PROCESS_AUTO_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDEB_ACCESS_AUTO_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDMC_AUTO_CLK_ENABLE_MASK			|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ENTDEC_AUTO_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ITRANS_AUTO_CLK_ENABLE_MASK	|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_CORE_MAN_CLK_ENABLE_MASK			|		\
+							MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_MTX_MAN_CLK_ENABLE_MASK
+
+#define msvdx_sw_reset_all	MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_SOFT_RESET_MASK					|		\
+							MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_FE_SOFT_RESET_MASK					|		\
+							MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_BE_SOFT_RESET_MASK					|		\
+							MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_VEC_MEMIF_SOFT_RESET_MASK			|		\
+							MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_VEC_RENDEC_DEC_SOFT_RESET_MASK
+
+
+#define PCI_PORT5_REG80_FFUSE                           0xD0058000
+#define MTX_CODE_BASE		(0x80900000)
+#define MTX_DATA_BASE		(0x82880000)
+#define PC_START_ADDRESS	(0x80900000)
+
+#define MTX_CORE_CODE_MEM			(0x10 )
+#define MTX_CORE_DATA_MEM			(0x18 )
+
+#define MTX_INTERNAL_REG( R_SPECIFIER , U_SPECIFIER )		( ((R_SPECIFIER)<<4) | (U_SPECIFIER) )
+#define MTX_PC			MTX_INTERNAL_REG( 0 , 5 )
+
+#define RENDEC_A_SIZE	( 2 * 1024* 1024 )
+#define RENDEC_B_SIZE	( RENDEC_A_SIZE / 4 )
+
+#define MEMIO_READ_FIELD(vpMem, field)																				    \
+	((uint32_t)(((*((field##_TYPE *)(((uint32_t)vpMem) + field##_OFFSET))) & field##_MASK) >> field##_SHIFT))
+
+#define MEMIO_WRITE_FIELD(vpMem, field, ui32Value)														\
+	(*((field##_TYPE *)(((uint32_t)vpMem) + field##_OFFSET))) =										\
+	((*((field##_TYPE *)(((uint32_t)vpMem) + field##_OFFSET))) & (field##_TYPE)~field##_MASK) |		\
+		(field##_TYPE)(( (uint32_t) (ui32Value) << field##_SHIFT) & field##_MASK);
+
+#define MEMIO_WRITE_FIELD_LITE(vpMem, field, ui32Value)													\
+	 (*((field##_TYPE *)(((uint32_t)vpMem) + field##_OFFSET))) =										\
+	((*((field##_TYPE *)(((uint32_t)vpMem) + field##_OFFSET))) |		                                \
+		(field##_TYPE) (( (uint32_t) (ui32Value) << field##_SHIFT)) );
+
+#define REGIO_READ_FIELD(ui32RegValue, reg, field)							\
+	((ui32RegValue & reg##_##field##_MASK) >> reg##_##field##_SHIFT)
+
+#define REGIO_WRITE_FIELD(ui32RegValue, reg, field, ui32Value)					\
+	(ui32RegValue) =									\
+	((ui32RegValue) & ~(reg##_##field##_MASK)) |						\
+		(((ui32Value) << (reg##_##field##_SHIFT)) & (reg##_##field##_MASK));
+
+#define REGIO_WRITE_FIELD_LITE(ui32RegValue, reg, field, ui32Value)				\
+	(ui32RegValue) =									\
+	( (ui32RegValue) | ( (ui32Value) << (reg##_##field##_SHIFT) ) );
+
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_CORE_MAN_CLK_ENABLE_MASK		(0x00000001)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDEB_PROCESS_MAN_CLK_ENABLE_MASK		(0x00000002)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDEB_ACCESS_MAN_CLK_ENABLE_MASK		(0x00000004)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDMC_MAN_CLK_ENABLE_MASK		(0x00000008)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ENTDEC_MAN_CLK_ENABLE_MASK		(0x00000010)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ITRANS_MAN_CLK_ENABLE_MASK		(0x00000020)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_MTX_MAN_CLK_ENABLE_MASK		(0x00000040)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDEB_ACCESS_AUTO_CLK_ENABLE_MASK		(0x00040000)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VDMC_AUTO_CLK_ENABLE_MASK		(0x00080000)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ENTDEC_AUTO_CLK_ENABLE_MASK		(0x00100000)
+#define MSVDX_CORE_CR_MSVDX_MAN_CLK_ENABLE_CR_VEC_ITRANS_AUTO_CLK_ENABLE_MASK		(0x00200000)
+#define MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_SOFT_RESET_MASK		(0x00000100)
+#define MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_FE_SOFT_RESET_MASK		(0x00010000)
+#define MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_BE_SOFT_RESET_MASK		(0x00100000)
+#define MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_VEC_MEMIF_SOFT_RESET_MASK		(0x01000000)
+#define MSVDX_CORE_CR_MSVDX_CONTROL_CR_MSVDX_VEC_RENDEC_DEC_SOFT_RESET_MASK		(0x10000000)
+
+/* MTX registers */
+#define MSVDX_MTX_ENABLE		(0x0000)
+#define MSVDX_MTX_KICKI			(0x0088)
+#define MSVDX_MTX_REGISTER_READ_WRITE_REQUEST	(0x00FC)
+#define MSVDX_MTX_REGISTER_READ_WRITE_DATA	(0x00F8)
+#define MSVDX_MTX_RAM_ACCESS_DATA_TRANSFER	(0x0104)
+#define MSVDX_MTX_RAM_ACCESS_CONTROL	(0x0108)
+#define MSVDX_MTX_RAM_ACCESS_STATUS	(0x010C)
+#define MSVDX_MTX_SOFT_RESET		(0x0200)
+
+/* MSVDX registers */
+#define MSVDX_CONTROL			(0x0600)
+#define MSVDX_INTERRUPT_CLEAR		(0x060C)
+#define MSVDX_INTERRUPT_STATUS		(0x0608)
+#define MSVDX_HOST_INTERRUPT_ENABLE	(0x0610)
+#define MSVDX_MMU_CONTROL0		(0x0680)
+#define MSVDX_MTX_RAM_BANK		(0x06F0)
+#define MSVDX_MAN_CLK_ENABLE		(0x0620)
+
+/* RENDEC registers */
+#define MSVDX_RENDEC_CONTROL0		(0x0868)
+#define MSVDX_RENDEC_CONTROL1		(0x086C)
+#define MSVDX_RENDEC_BUFFER_SIZE	(0x0870)
+#define MSVDX_RENDEC_BASE_ADDR0		(0x0874)
+#define MSVDX_RENDEC_BASE_ADDR1		(0x0878)
+#define MSVDX_RENDEC_READ_DATA		(0x0898)
+#define MSVDX_RENDEC_CONTEXT0		(0x0950)
+#define MSVDX_RENDEC_CONTEXT1		(0x0954)
+#define MSVDX_RENDEC_CONTEXT2		(0x0958)
+#define MSVDX_RENDEC_CONTEXT3		(0x095C)
+#define MSVDX_RENDEC_CONTEXT4		(0x0960)
+#define MSVDX_RENDEC_CONTEXT5		(0x0964)
+
+/*
+ * This defines the MSVDX communication buffer
+ */
+#define MSVDX_COMMS_SIGNATURE_VALUE	(0xA5A5A5A5)	/*!< Signature value */
+#define NUM_WORDS_HOST_BUF		(100)	/*!< Host buffer size (in 32-bit words) */
+#define NUM_WORDS_MTX_BUF		(100)	/*!< MTX buffer size (in 32-bit words) */
+
+#define MSVDX_COMMS_AREA_ADDR (0x02cc0)
+
+#define MSVDX_COMMS_FW_STATUS			(MSVDX_COMMS_AREA_ADDR - 0x10)
+#define	MSVDX_COMMS_SCRATCH				(MSVDX_COMMS_AREA_ADDR - 0x08)
+#define	MSVDX_COMMS_MSG_COUNTER			(MSVDX_COMMS_AREA_ADDR - 0x04)
+#define	MSVDX_COMMS_SIGNATURE			(MSVDX_COMMS_AREA_ADDR + 0x00)
+#define	MSVDX_COMMS_TO_HOST_BUF_SIZE	(MSVDX_COMMS_AREA_ADDR + 0x04)
+#define MSVDX_COMMS_TO_HOST_RD_INDEX	(MSVDX_COMMS_AREA_ADDR + 0x08)
+#define MSVDX_COMMS_TO_HOST_WRT_INDEX	(MSVDX_COMMS_AREA_ADDR + 0x0C)
+#define MSVDX_COMMS_TO_MTX_BUF_SIZE		(MSVDX_COMMS_AREA_ADDR + 0x10)
+#define MSVDX_COMMS_TO_MTX_RD_INDEX		(MSVDX_COMMS_AREA_ADDR + 0x14)
+#define MSVDX_COMMS_OFFSET_FLAGS		(MSVDX_COMMS_AREA_ADDR + 0x18)
+#define MSVDX_COMMS_TO_MTX_WRT_INDEX	(MSVDX_COMMS_AREA_ADDR + 0x1C)
+#define MSVDX_COMMS_TO_HOST_BUF			(MSVDX_COMMS_AREA_ADDR + 0x20)
+#define MSVDX_COMMS_TO_MTX_BUF			(MSVDX_COMMS_TO_HOST_BUF + (NUM_WORDS_HOST_BUF << 2))
+
+#define MSVDX_COMMS_AREA_END			(MSVDX_COMMS_TO_MTX_BUF + (NUM_WORDS_HOST_BUF << 2))
+
+#if (MSVDX_COMMS_AREA_END != 0x03000)
+#error
+#endif
+
+#define MSVDX_MTX_REGISTER_READ_WRITE_REQUEST_MTX_DREADY_MASK		(0x80000000)
+#define MSVDX_MTX_REGISTER_READ_WRITE_REQUEST_MTX_DREADY_SHIFT		(31)
+
+#define MSVDX_MTX_REGISTER_READ_WRITE_REQUEST_MTX_RNW_MASK		(0x00010000)
+#define MSVDX_MTX_REGISTER_READ_WRITE_REQUEST_MTX_RNW_SHIFT		(16)
+
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCMID_MASK		(0x0FF00000)
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCMID_SHIFT		(20)
+
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCM_ADDR_MASK		(0x000FFFFC)
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCM_ADDR_SHIFT		(2)
+
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCMAI_MASK		(0x00000002)
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCMAI_SHIFT		(1)
+
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCMR_MASK		(0x00000001)
+#define MSVDX_MTX_RAM_ACCESS_CONTROL_MTX_MCMR_SHIFT		(0)
+
+#define MSVDX_MTX_SOFT_RESET_MTX_RESET_MASK		(0x00000001)
+#define MSVDX_MTX_SOFT_RESET_MTX_RESET_SHIFT		(0)
+
+#define MSVDX_MTX_ENABLE_MTX_ENABLE_MASK		(0x00000001)
+#define MSVDX_MTX_ENABLE_MTX_ENABLE_SHIFT		(0)
+
+#define MSVDX_CONTROL_CR_MSVDX_SOFT_RESET_MASK		(0x00000100)
+#define MSVDX_CONTROL_CR_MSVDX_SOFT_RESET_SHIFT		(8)
+
+#define MSVDX_INTERRUPT_STATUS_CR_MMU_FAULT_IRQ_MASK		(0x00000F00)
+#define MSVDX_INTERRUPT_STATUS_CR_MMU_FAULT_IRQ_SHIFT		(8)
+
+#define MSVDX_INTERRUPT_STATUS_CR_MTX_IRQ_MASK		(0x00004000)
+#define MSVDX_INTERRUPT_STATUS_CR_MTX_IRQ_SHIFT		(14)
+
+#define MSVDX_MMU_CONTROL0_CR_MMU_PAUSE_MASK			(0x00000002)
+#define MSVDX_MMU_CONTROL0_CR_MMU_PAUSE_SHIFT			(1)
+
+#define MSVDX_MTX_RAM_BANK_CR_MTX_RAM_BANK_SIZE_MASK		(0x000F0000)
+#define MSVDX_MTX_RAM_BANK_CR_MTX_RAM_BANK_SIZE_SHIFT		(16)
+
+#define MSVDX_RENDEC_BUFFER_SIZE_RENDEC_BUFFER_SIZE0_MASK		(0x0000FFFF)
+#define MSVDX_RENDEC_BUFFER_SIZE_RENDEC_BUFFER_SIZE0_SHIFT		(0)
+
+#define MSVDX_RENDEC_BUFFER_SIZE_RENDEC_BUFFER_SIZE1_MASK		(0xFFFF0000)
+#define MSVDX_RENDEC_BUFFER_SIZE_RENDEC_BUFFER_SIZE1_SHIFT		(16)
+
+#define MSVDX_RENDEC_CONTROL1_RENDEC_DECODE_START_SIZE_MASK		(0x000000FF)
+#define MSVDX_RENDEC_CONTROL1_RENDEC_DECODE_START_SIZE_SHIFT		(0)
+
+#define MSVDX_RENDEC_CONTROL1_RENDEC_BURST_SIZE_W_MASK		(0x000C0000)
+#define MSVDX_RENDEC_CONTROL1_RENDEC_BURST_SIZE_W_SHIFT		(18)
+
+#define MSVDX_RENDEC_CONTROL1_RENDEC_BURST_SIZE_R_MASK		(0x00030000)
+#define MSVDX_RENDEC_CONTROL1_RENDEC_BURST_SIZE_R_SHIFT		(16)
+
+#define MSVDX_RENDEC_CONTROL1_RENDEC_EXTERNAL_MEMORY_MASK		(0x01000000)
+#define MSVDX_RENDEC_CONTROL1_RENDEC_EXTERNAL_MEMORY_SHIFT		(24)
+
+#define MSVDX_RENDEC_CONTROL0_RENDEC_INITIALISE_MASK		(0x00000001)
+#define MSVDX_RENDEC_CONTROL0_RENDEC_INITIALISE_SHIFT		(0)
+
+#define	FWRK_MSGID_START_PSR_HOSTMTX_MSG	(0x80)	/*!< Start of parser specific Host->MTX messages. */
+#define	FWRK_MSGID_START_PSR_MTXHOST_MSG	(0xC0)	/*!< Start of parser specific MTX->Host messages. */
+#define FWRK_MSGID_PADDING					( 0 )
+
+#define FWRK_GENMSG_SIZE_TYPE		uint8_t
+#define FWRK_GENMSG_SIZE_MASK		(0xFF)
+#define FWRK_GENMSG_SIZE_SHIFT		(0)
+#define FWRK_GENMSG_SIZE_OFFSET		(0x0000)
+#define FWRK_GENMSG_ID_TYPE		uint8_t
+#define FWRK_GENMSG_ID_MASK		(0xFF)
+#define FWRK_GENMSG_ID_SHIFT		(0)
+#define FWRK_GENMSG_ID_OFFSET		(0x0001)
+#define FWRK_PADMSG_SIZE		(2)
+
+/*!
+******************************************************************************
+ This type defines the framework specified message ids
+******************************************************************************/
+enum
+{
+  /*! Sent by the DXVA driver on the host to the mtx firmware.
+   */
+  VA_MSGID_INIT = FWRK_MSGID_START_PSR_HOSTMTX_MSG,
+  VA_MSGID_RENDER,
+  VA_MSGID_DEBLOCK,
+  VA_MSGID_OOLD,
+
+  /* Test Messages */
+  VA_MSGID_TEST1,
+  VA_MSGID_TEST2,
+
+  /*! Sent by the mtx firmware to itself.
+   */
+  VA_MSGID_RENDER_MC_INTERRUPT,
+
+  /*! Sent by the DXVA firmware on the MTX to the host.
+   */
+  VA_MSGID_CMD_COMPLETED = FWRK_MSGID_START_PSR_MTXHOST_MSG,
+  VA_MSGID_CMD_COMPLETED_BATCH,
+  VA_MSGID_DEBLOCK_REQUIRED,
+  VA_MSGID_TEST_RESPONCE,
+  VA_MSGID_ACK,
+
+  VA_MSGID_CMD_FAILED,
+  VA_MSGID_CMD_UNSUPPORTED,
+  VA_MSGID_CMD_HW_PANIC,
+};
+
+/* MSVDX Firmware interface */
+
+#define FW_VA_RENDER_SIZE		(32)
+
+// FW_VA_RENDER     MSG_SIZE
+#define FW_VA_RENDER_MSG_SIZE_ALIGNMENT		(1)
+#define FW_VA_RENDER_MSG_SIZE_TYPE		uint8_t
+#define FW_VA_RENDER_MSG_SIZE_MASK		(0xFF)
+#define FW_VA_RENDER_MSG_SIZE_LSBMASK		(0xFF)
+#define FW_VA_RENDER_MSG_SIZE_OFFSET		(0x0000)
+#define FW_VA_RENDER_MSG_SIZE_SHIFT		(0)
+
+// FW_VA_RENDER     ID
+#define FW_VA_RENDER_ID_ALIGNMENT		(1)
+#define FW_VA_RENDER_ID_TYPE		uint8_t
+#define FW_VA_RENDER_ID_MASK		(0xFF)
+#define FW_VA_RENDER_ID_LSBMASK		(0xFF)
+#define FW_VA_RENDER_ID_OFFSET		(0x0001)
+#define FW_VA_RENDER_ID_SHIFT		(0)
+
+// FW_VA_RENDER     BUFFER_SIZE
+#define FW_VA_RENDER_BUFFER_SIZE_ALIGNMENT		(2)
+#define FW_VA_RENDER_BUFFER_SIZE_TYPE		uint16_t
+#define FW_VA_RENDER_BUFFER_SIZE_MASK		(0x0FFF)
+#define FW_VA_RENDER_BUFFER_SIZE_LSBMASK		(0x0FFF)
+#define FW_VA_RENDER_BUFFER_SIZE_OFFSET		(0x0002)
+#define FW_VA_RENDER_BUFFER_SIZE_SHIFT		(0)
+
+// FW_VA_RENDER     MMUPTD
+#define FW_VA_RENDER_MMUPTD_ALIGNMENT		(4)
+#define FW_VA_RENDER_MMUPTD_TYPE		uint32_t
+#define FW_VA_RENDER_MMUPTD_MASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_MMUPTD_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_MMUPTD_OFFSET		(0x0004)
+#define FW_VA_RENDER_MMUPTD_SHIFT		(0)
+
+// FW_VA_RENDER     LLDMA_ADDRESS
+#define FW_VA_RENDER_LLDMA_ADDRESS_ALIGNMENT		(4)
+#define FW_VA_RENDER_LLDMA_ADDRESS_TYPE		uint32_t
+#define FW_VA_RENDER_LLDMA_ADDRESS_MASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_LLDMA_ADDRESS_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_LLDMA_ADDRESS_OFFSET		(0x0008)
+#define FW_VA_RENDER_LLDMA_ADDRESS_SHIFT		(0)
+
+// FW_VA_RENDER     CONTEXT
+#define FW_VA_RENDER_CONTEXT_ALIGNMENT		(4)
+#define FW_VA_RENDER_CONTEXT_TYPE		uint32_t
+#define FW_VA_RENDER_CONTEXT_MASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_CONTEXT_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_CONTEXT_OFFSET		(0x000C)
+#define FW_VA_RENDER_CONTEXT_SHIFT		(0)
+
+// FW_VA_RENDER     FENCE_VALUE
+#define FW_VA_RENDER_FENCE_VALUE_ALIGNMENT		(4)
+#define FW_VA_RENDER_FENCE_VALUE_TYPE		uint32_t
+#define FW_VA_RENDER_FENCE_VALUE_MASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_FENCE_VALUE_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_FENCE_VALUE_OFFSET		(0x0010)
+#define FW_VA_RENDER_FENCE_VALUE_SHIFT		(0)
+
+// FW_VA_RENDER     OPERATING_MODE
+#define FW_VA_RENDER_OPERATING_MODE_ALIGNMENT		(4)
+#define FW_VA_RENDER_OPERATING_MODE_TYPE		uint32_t
+#define FW_VA_RENDER_OPERATING_MODE_MASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_OPERATING_MODE_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_OPERATING_MODE_OFFSET		(0x0014)
+#define FW_VA_RENDER_OPERATING_MODE_SHIFT		(0)
+
+// FW_VA_RENDER     FIRST_MB_IN_SLICE
+#define FW_VA_RENDER_FIRST_MB_IN_SLICE_ALIGNMENT		(2)
+#define FW_VA_RENDER_FIRST_MB_IN_SLICE_TYPE		uint16_t
+#define FW_VA_RENDER_FIRST_MB_IN_SLICE_MASK		(0xFFFF)
+#define FW_VA_RENDER_FIRST_MB_IN_SLICE_LSBMASK		(0xFFFF)
+#define FW_VA_RENDER_FIRST_MB_IN_SLICE_OFFSET		(0x0018)
+#define FW_VA_RENDER_FIRST_MB_IN_SLICE_SHIFT		(0)
+
+// FW_VA_RENDER     LAST_MB_IN_FRAME
+#define FW_VA_RENDER_LAST_MB_IN_FRAME_ALIGNMENT		(2)
+#define FW_VA_RENDER_LAST_MB_IN_FRAME_TYPE		uint16_t
+#define FW_VA_RENDER_LAST_MB_IN_FRAME_MASK		(0xFFFF)
+#define FW_VA_RENDER_LAST_MB_IN_FRAME_LSBMASK		(0xFFFF)
+#define FW_VA_RENDER_LAST_MB_IN_FRAME_OFFSET		(0x001A)
+#define FW_VA_RENDER_LAST_MB_IN_FRAME_SHIFT		(0)
+
+// FW_VA_RENDER     FLAGS
+#define FW_VA_RENDER_FLAGS_ALIGNMENT		(4)
+#define FW_VA_RENDER_FLAGS_TYPE		uint32_t
+#define FW_VA_RENDER_FLAGS_MASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_FLAGS_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_RENDER_FLAGS_OFFSET		(0x001C)
+#define FW_VA_RENDER_FLAGS_SHIFT		(0)
+
+#define FW_VA_CMD_COMPLETED_SIZE		(12)
+
+// FW_VA_CMD_COMPLETED     MSG_SIZE
+#define FW_VA_CMD_COMPLETED_MSG_SIZE_ALIGNMENT		(1)
+#define FW_VA_CMD_COMPLETED_MSG_SIZE_TYPE		uint8_t
+#define FW_VA_CMD_COMPLETED_MSG_SIZE_MASK		(0xFF)
+#define FW_VA_CMD_COMPLETED_MSG_SIZE_LSBMASK		(0xFF)
+#define FW_VA_CMD_COMPLETED_MSG_SIZE_OFFSET		(0x0000)
+#define FW_VA_CMD_COMPLETED_MSG_SIZE_SHIFT		(0)
+
+// FW_VA_CMD_COMPLETED     ID
+#define FW_VA_CMD_COMPLETED_ID_ALIGNMENT		(1)
+#define FW_VA_CMD_COMPLETED_ID_TYPE		uint8_t
+#define FW_VA_CMD_COMPLETED_ID_MASK		(0xFF)
+#define FW_VA_CMD_COMPLETED_ID_LSBMASK		(0xFF)
+#define FW_VA_CMD_COMPLETED_ID_OFFSET		(0x0001)
+#define FW_VA_CMD_COMPLETED_ID_SHIFT		(0)
+
+// FW_VA_CMD_COMPLETED     FENCE_VALUE
+#define FW_VA_CMD_COMPLETED_FENCE_VALUE_ALIGNMENT		(4)
+#define FW_VA_CMD_COMPLETED_FENCE_VALUE_TYPE		uint32_t
+#define FW_VA_CMD_COMPLETED_FENCE_VALUE_MASK		(0xFFFFFFFF)
+#define FW_VA_CMD_COMPLETED_FENCE_VALUE_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_CMD_COMPLETED_FENCE_VALUE_OFFSET		(0x0004)
+#define FW_VA_CMD_COMPLETED_FENCE_VALUE_SHIFT		(0)
+
+// FW_VA_CMD_COMPLETED     FLAGS
+#define FW_VA_CMD_COMPLETED_FLAGS_ALIGNMENT		(4)
+#define FW_VA_CMD_COMPLETED_FLAGS_TYPE		uint32_t
+#define FW_VA_CMD_COMPLETED_FLAGS_MASK		(0xFFFFFFFF)
+#define FW_VA_CMD_COMPLETED_FLAGS_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_CMD_COMPLETED_FLAGS_OFFSET		(0x0008)
+#define FW_VA_CMD_COMPLETED_FLAGS_SHIFT		(0)
+
+#define FW_VA_CMD_FAILED_SIZE		(12)
+
+// FW_VA_CMD_FAILED     MSG_SIZE
+#define FW_VA_CMD_FAILED_MSG_SIZE_ALIGNMENT		(1)
+#define FW_VA_CMD_FAILED_MSG_SIZE_TYPE		uint8_t
+#define FW_VA_CMD_FAILED_MSG_SIZE_MASK		(0xFF)
+#define FW_VA_CMD_FAILED_MSG_SIZE_LSBMASK		(0xFF)
+#define FW_VA_CMD_FAILED_MSG_SIZE_OFFSET		(0x0000)
+#define FW_VA_CMD_FAILED_MSG_SIZE_SHIFT		(0)
+
+// FW_VA_CMD_FAILED     ID
+#define FW_VA_CMD_FAILED_ID_ALIGNMENT		(1)
+#define FW_VA_CMD_FAILED_ID_TYPE		uint8_t
+#define FW_VA_CMD_FAILED_ID_MASK		(0xFF)
+#define FW_VA_CMD_FAILED_ID_LSBMASK		(0xFF)
+#define FW_VA_CMD_FAILED_ID_OFFSET		(0x0001)
+#define FW_VA_CMD_FAILED_ID_SHIFT		(0)
+
+// FW_VA_CMD_FAILED     FLAGS
+#define FW_VA_CMD_FAILED_FLAGS_ALIGNMENT		(2)
+#define FW_VA_CMD_FAILED_FLAGS_TYPE		uint16_t
+#define FW_VA_CMD_FAILED_FLAGS_MASK		(0xFFFF)
+#define FW_VA_CMD_FAILED_FLAGS_LSBMASK		(0xFFFF)
+#define FW_VA_CMD_FAILED_FLAGS_OFFSET		(0x0002)
+#define FW_VA_CMD_FAILED_FLAGS_SHIFT		(0)
+
+// FW_VA_CMD_FAILED     FENCE_VALUE
+#define FW_VA_CMD_FAILED_FENCE_VALUE_ALIGNMENT		(4)
+#define FW_VA_CMD_FAILED_FENCE_VALUE_TYPE		uint32_t
+#define FW_VA_CMD_FAILED_FENCE_VALUE_MASK		(0xFFFFFFFF)
+#define FW_VA_CMD_FAILED_FENCE_VALUE_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_CMD_FAILED_FENCE_VALUE_OFFSET		(0x0004)
+#define FW_VA_CMD_FAILED_FENCE_VALUE_SHIFT		(0)
+
+// FW_VA_CMD_FAILED     IRQSTATUS
+#define FW_VA_CMD_FAILED_IRQSTATUS_ALIGNMENT		(4)
+#define FW_VA_CMD_FAILED_IRQSTATUS_TYPE		uint32_t
+#define FW_VA_CMD_FAILED_IRQSTATUS_MASK		(0xFFFFFFFF)
+#define FW_VA_CMD_FAILED_IRQSTATUS_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_CMD_FAILED_IRQSTATUS_OFFSET		(0x0008)
+#define FW_VA_CMD_FAILED_IRQSTATUS_SHIFT		(0)
+
+#define FW_VA_DEBLOCK_REQUIRED_SIZE		(8)
+
+// FW_VA_DEBLOCK_REQUIRED     MSG_SIZE
+#define FW_VA_DEBLOCK_REQUIRED_MSG_SIZE_ALIGNMENT		(1)
+#define FW_VA_DEBLOCK_REQUIRED_MSG_SIZE_TYPE		uint8_t
+#define FW_VA_DEBLOCK_REQUIRED_MSG_SIZE_MASK		(0xFF)
+#define FW_VA_DEBLOCK_REQUIRED_MSG_SIZE_LSBMASK		(0xFF)
+#define FW_VA_DEBLOCK_REQUIRED_MSG_SIZE_OFFSET		(0x0000)
+#define FW_VA_DEBLOCK_REQUIRED_MSG_SIZE_SHIFT		(0)
+
+// FW_VA_DEBLOCK_REQUIRED     ID
+#define FW_VA_DEBLOCK_REQUIRED_ID_ALIGNMENT		(1)
+#define FW_VA_DEBLOCK_REQUIRED_ID_TYPE		uint8_t
+#define FW_VA_DEBLOCK_REQUIRED_ID_MASK		(0xFF)
+#define FW_VA_DEBLOCK_REQUIRED_ID_LSBMASK		(0xFF)
+#define FW_VA_DEBLOCK_REQUIRED_ID_OFFSET		(0x0001)
+#define FW_VA_DEBLOCK_REQUIRED_ID_SHIFT		(0)
+
+// FW_VA_DEBLOCK_REQUIRED     CONTEXT
+#define FW_VA_DEBLOCK_REQUIRED_CONTEXT_ALIGNMENT		(4)
+#define FW_VA_DEBLOCK_REQUIRED_CONTEXT_TYPE		uint32_t
+#define FW_VA_DEBLOCK_REQUIRED_CONTEXT_MASK		(0xFFFFFFFF)
+#define FW_VA_DEBLOCK_REQUIRED_CONTEXT_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_DEBLOCK_REQUIRED_CONTEXT_OFFSET		(0x0004)
+#define FW_VA_DEBLOCK_REQUIRED_CONTEXT_SHIFT		(0)
+
+#define FW_VA_HW_PANIC_SIZE		(12)
+
+// FW_VA_HW_PANIC     FLAGS
+#define FW_VA_HW_PANIC_FLAGS_ALIGNMENT		(2)
+#define FW_VA_HW_PANIC_FLAGS_TYPE		uint16_t
+#define FW_VA_HW_PANIC_FLAGS_MASK		(0xFFFF)
+#define FW_VA_HW_PANIC_FLAGS_LSBMASK		(0xFFFF)
+#define FW_VA_HW_PANIC_FLAGS_OFFSET		(0x0002)
+#define FW_VA_HW_PANIC_FLAGS_SHIFT		(0)
+
+// FW_VA_HW_PANIC     MSG_SIZE
+#define FW_VA_HW_PANIC_MSG_SIZE_ALIGNMENT		(1)
+#define FW_VA_HW_PANIC_MSG_SIZE_TYPE		uint8_t
+#define FW_VA_HW_PANIC_MSG_SIZE_MASK		(0xFF)
+#define FW_VA_HW_PANIC_MSG_SIZE_LSBMASK		(0xFF)
+#define FW_VA_HW_PANIC_MSG_SIZE_OFFSET		(0x0000)
+#define FW_VA_HW_PANIC_MSG_SIZE_SHIFT		(0)
+
+// FW_VA_HW_PANIC     ID
+#define FW_VA_HW_PANIC_ID_ALIGNMENT		(1)
+#define FW_VA_HW_PANIC_ID_TYPE		uint8_t
+#define FW_VA_HW_PANIC_ID_MASK		(0xFF)
+#define FW_VA_HW_PANIC_ID_LSBMASK		(0xFF)
+#define FW_VA_HW_PANIC_ID_OFFSET		(0x0001)
+#define FW_VA_HW_PANIC_ID_SHIFT		(0)
+
+// FW_VA_HW_PANIC     FENCE_VALUE
+#define FW_VA_HW_PANIC_FENCE_VALUE_ALIGNMENT		(4)
+#define FW_VA_HW_PANIC_FENCE_VALUE_TYPE		uint32_t
+#define FW_VA_HW_PANIC_FENCE_VALUE_MASK		(0xFFFFFFFF)
+#define FW_VA_HW_PANIC_FENCE_VALUE_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_HW_PANIC_FENCE_VALUE_OFFSET		(0x0004)
+#define FW_VA_HW_PANIC_FENCE_VALUE_SHIFT		(0)
+
+// FW_VA_HW_PANIC     IRQSTATUS
+#define FW_VA_HW_PANIC_IRQSTATUS_ALIGNMENT		(4)
+#define FW_VA_HW_PANIC_IRQSTATUS_TYPE		uint32_t
+#define FW_VA_HW_PANIC_IRQSTATUS_MASK		(0xFFFFFFFF)
+#define FW_VA_HW_PANIC_IRQSTATUS_LSBMASK		(0xFFFFFFFF)
+#define FW_VA_HW_PANIC_IRQSTATUS_OFFSET		(0x0008)
+#define FW_VA_HW_PANIC_IRQSTATUS_SHIFT		(0)
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_msvdxinit.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_msvdxinit.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,625 @@
+/**
+ * file psb_msvdxinit.c
+ * MSVDX initialization and mtx-firmware upload
+ *
+ */
+
+/**************************************************************************
+ *
+ * Copyright (c) 2007 Intel Corporation, Hillsboro, OR, USA
+ * Copyright (c) Imagination Technologies Limited, UK
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+
+#include "drmP.h"
+#include "drm.h"
+#include "psb_drv.h"
+#include "psb_msvdx.h"
+#include <linux/firmware.h>
+
+/*MSVDX FW header*/
+struct msvdx_fw
+{
+  uint32_t ver;
+  uint32_t text_size;
+  uint32_t data_size;
+  uint32_t data_location;
+};
+
+int
+psb_wait_for_register (struct drm_psb_private *dev_priv,
+		       uint32_t ui32Offset,
+		       uint32_t ui32Value, uint32_t ui32Enable)
+{
+  uint32_t ui32Temp;
+  uint32_t ui32PollCount = 1000;
+  while (ui32PollCount)
+    {
+      ui32Temp = PSB_RMSVDX32 (ui32Offset);
+      if (ui32Value == (ui32Temp & ui32Enable))	/* All the bits are reset   */
+	return 0;		/* So exit                      */
+
+      /* Wait a bit */
+      DRM_UDELAY (100);
+      ui32PollCount--;
+    }
+  PSB_DEBUG_GENERAL
+    ("MSVDX: Timeout while waiting for register %08x: expecting %08x (mask %08x), got %08x\n",
+     ui32Offset, ui32Value, ui32Enable, ui32Temp);
+  return 1;
+}
+
+int
+psb_poll_mtx_irq (struct drm_psb_private *dev_priv)
+{
+  int ret = 0;
+  uint32_t MtxInt = 0;
+  REGIO_WRITE_FIELD_LITE (MtxInt, MSVDX_INTERRUPT_STATUS, CR_MTX_IRQ, 1);
+
+  ret = psb_wait_for_register (dev_priv, MSVDX_INTERRUPT_STATUS, MtxInt,	/* Required value */
+			       MtxInt /* Enabled bits */ );
+  if (ret)
+    {
+      PSB_DEBUG_GENERAL
+	("MSVDX: Error Mtx did not return int within a resonable time\n");
+
+      return ret;
+    }
+
+  PSB_DEBUG_GENERAL ("MSVDX: Got MTX Int\n");
+
+  /* Got it so clear the bit */
+  PSB_WMSVDX32 (MtxInt, MSVDX_INTERRUPT_CLEAR);
+
+  return ret;
+}
+
+void
+psb_write_mtx_core_reg (struct drm_psb_private *dev_priv,
+			const uint32_t ui32CoreRegister,
+			const uint32_t ui32Val)
+{
+  uint32_t ui32Reg = 0;
+
+  /* Put data in MTX_RW_DATA */
+  PSB_WMSVDX32 (ui32Val, MSVDX_MTX_REGISTER_READ_WRITE_DATA);
+
+  /* DREADY is set to 0 and request a write */
+  ui32Reg = ui32CoreRegister;
+  REGIO_WRITE_FIELD_LITE (ui32Reg, MSVDX_MTX_REGISTER_READ_WRITE_REQUEST,
+			  MTX_RNW, 0);
+  REGIO_WRITE_FIELD_LITE (ui32Reg, MSVDX_MTX_REGISTER_READ_WRITE_REQUEST,
+			  MTX_DREADY, 0);
+  PSB_WMSVDX32 (ui32Reg, MSVDX_MTX_REGISTER_READ_WRITE_REQUEST);
+
+  psb_wait_for_register (dev_priv, MSVDX_MTX_REGISTER_READ_WRITE_REQUEST, MSVDX_MTX_REGISTER_READ_WRITE_REQUEST_MTX_DREADY_MASK,	/* Required Value */
+			 MSVDX_MTX_REGISTER_READ_WRITE_REQUEST_MTX_DREADY_MASK);
+}
+
+void
+psb_upload_fw (struct drm_psb_private *dev_priv, const uint32_t ui32DataMem,
+	       uint32_t ui32RamBankSize, uint32_t ui32Address,
+	       const unsigned int uiWords, const uint32_t * const pui32Data)
+{
+  uint32_t ui32Loop, ui32Ctrl, ui32RamId, ui32Addr, ui32CurrBank =
+    (uint32_t) ~ 0;
+  uint32_t ui32AccessControl;
+
+  /* Save the access control register... */
+  ui32AccessControl = PSB_RMSVDX32 (MSVDX_MTX_RAM_ACCESS_CONTROL);
+
+  /* Wait for MCMSTAT to become be idle 1 */
+  psb_wait_for_register (dev_priv, MSVDX_MTX_RAM_ACCESS_STATUS, 1,	/* Required Value */
+			 0xffffffff /* Enables */ );
+
+  for (ui32Loop = 0; ui32Loop < uiWords; ui32Loop++)
+    {
+      ui32RamId = ui32DataMem + (ui32Address / ui32RamBankSize);
+
+      if (ui32RamId != ui32CurrBank)
+	{
+	  ui32Addr = ui32Address >> 2;
+
+	  ui32Ctrl = 0;
+
+	  REGIO_WRITE_FIELD_LITE (ui32Ctrl,
+				  MSVDX_MTX_RAM_ACCESS_CONTROL,
+				  MTX_MCMID, ui32RamId);
+	  REGIO_WRITE_FIELD_LITE (ui32Ctrl,
+				  MSVDX_MTX_RAM_ACCESS_CONTROL,
+				  MTX_MCM_ADDR, ui32Addr);
+	  REGIO_WRITE_FIELD_LITE (ui32Ctrl,
+				  MSVDX_MTX_RAM_ACCESS_CONTROL, MTX_MCMAI, 1);
+
+	  PSB_WMSVDX32 (ui32Ctrl, MSVDX_MTX_RAM_ACCESS_CONTROL);
+
+	  ui32CurrBank = ui32RamId;
+	}
+      ui32Address += 4;
+
+      PSB_WMSVDX32 (pui32Data[ui32Loop], MSVDX_MTX_RAM_ACCESS_DATA_TRANSFER);
+
+      /* Wait for MCMSTAT to become be idle 1 */
+      psb_wait_for_register (dev_priv, MSVDX_MTX_RAM_ACCESS_STATUS, 1,	/* Required Value */
+			     0xffffffff /* Enables */ );
+    }
+  PSB_DEBUG_GENERAL ("MSVDX: Upload done\n");
+
+  /* Restore the access control register... */
+  PSB_WMSVDX32 (ui32AccessControl, MSVDX_MTX_RAM_ACCESS_CONTROL);
+}
+
+static int
+psb_verify_fw (struct drm_psb_private *dev_priv,
+	       const uint32_t ui32RamBankSize,
+	       const uint32_t ui32DataMem, uint32_t ui32Address,
+	       const uint32_t uiWords, const uint32_t * const pui32Data)
+{
+  uint32_t ui32Loop, ui32Ctrl, ui32RamId, ui32Addr, ui32CurrBank =
+    (uint32_t) ~ 0;
+  uint32_t ui32AccessControl;
+  int ret = 0;
+
+  /* Save the access control register... */
+  ui32AccessControl = PSB_RMSVDX32 (MSVDX_MTX_RAM_ACCESS_CONTROL);
+
+  /* Wait for MCMSTAT to become be idle 1 */
+  psb_wait_for_register (dev_priv, MSVDX_MTX_RAM_ACCESS_STATUS, 1,	/* Required Value */
+			 0xffffffff /* Enables */ );
+
+  for (ui32Loop = 0; ui32Loop < uiWords; ui32Loop++)
+    {
+      uint32_t ui32ReadBackVal;
+      ui32RamId = ui32DataMem + (ui32Address / ui32RamBankSize);
+
+      if (ui32RamId != ui32CurrBank)
+	{
+	  ui32Addr = ui32Address >> 2;
+	  ui32Ctrl = 0;
+	  REGIO_WRITE_FIELD_LITE (ui32Ctrl,
+				  MSVDX_MTX_RAM_ACCESS_CONTROL,
+				  MTX_MCMID, ui32RamId);
+	  REGIO_WRITE_FIELD_LITE (ui32Ctrl,
+				  MSVDX_MTX_RAM_ACCESS_CONTROL,
+				  MTX_MCM_ADDR, ui32Addr);
+	  REGIO_WRITE_FIELD_LITE (ui32Ctrl,
+				  MSVDX_MTX_RAM_ACCESS_CONTROL, MTX_MCMAI, 1);
+	  REGIO_WRITE_FIELD_LITE (ui32Ctrl,
+				  MSVDX_MTX_RAM_ACCESS_CONTROL, MTX_MCMR, 1);
+
+	  PSB_WMSVDX32 (ui32Ctrl, MSVDX_MTX_RAM_ACCESS_CONTROL);
+
+	  ui32CurrBank = ui32RamId;
+	}
+      ui32Address += 4;
+
+      /* Wait for MCMSTAT to become be idle 1 */
+      psb_wait_for_register (dev_priv, MSVDX_MTX_RAM_ACCESS_STATUS, 1,	/* Required Value */
+			     0xffffffff /* Enables */ );
+
+      ui32ReadBackVal = PSB_RMSVDX32 (MSVDX_MTX_RAM_ACCESS_DATA_TRANSFER);
+      if (pui32Data[ui32Loop] != ui32ReadBackVal)
+	{
+	  DRM_ERROR
+	    ("psb: Firmware validation fails at index=%08x\n", ui32Loop);
+	  ret = 1;
+	  break;
+	}
+    }
+
+  /* Restore the access control register... */
+  PSB_WMSVDX32 (ui32AccessControl, MSVDX_MTX_RAM_ACCESS_CONTROL);
+
+  return ret;
+}
+
+static uint32_t *
+msvdx_get_fw (struct drm_device *dev,
+	      const struct firmware **raw, uint8_t * name)
+{
+  int rc;
+  int *ptr = NULL;
+
+  rc = request_firmware (raw, name, &dev->pdev->dev);
+  if (rc < 0)
+    {
+      DRM_ERROR ("MSVDX: %s request_firmware failed: Reason %d\n", name, rc);
+      return NULL;
+    }
+
+  if ((*raw)->size < sizeof (struct msvdx_fw))
+    {
+      PSB_DEBUG_GENERAL ("MSVDX: %s is is not correct size(%zd)\n",
+			 name, (*raw)->size);
+      return NULL;
+    }
+
+  ptr = (int *) ((*raw))->data;
+
+  if (!ptr)
+    {
+      PSB_DEBUG_GENERAL ("MSVDX: Failed to load %s\n", name);
+      return NULL;
+    }
+  /*another sanity check... */
+  if ((*raw)->size !=
+      (sizeof (struct msvdx_fw) +
+       sizeof (uint32_t) * ((struct msvdx_fw *) ptr)->text_size +
+       sizeof (uint32_t) * ((struct msvdx_fw *) ptr)->data_size))
+    {
+      PSB_DEBUG_GENERAL ("MSVDX: %s is is not correct size(%zd)\n",
+			 name, (*raw)->size);
+      return NULL;
+    }
+  return ptr;
+}
+
+static int
+psb_setup_fw (struct drm_device *dev)
+{
+  struct drm_psb_private *dev_priv = dev->dev_private;
+  int ret = 0;
+
+  uint32_t ram_bank_size;
+  struct msvdx_fw *fw;
+  uint32_t *fw_ptr = NULL;
+  uint32_t *text_ptr = NULL;
+  uint32_t *data_ptr = NULL;
+  const struct firmware *raw = NULL;
+  /* todo : Assert the clock is on - if not turn it on to upload code */
+
+  PSB_DEBUG_GENERAL ("MSVDX: psb_setup_fw\n");
+
+  /* Reset MTX */
+  PSB_WMSVDX32 (MSVDX_MTX_SOFT_RESET_MTX_RESET_MASK, MSVDX_MTX_SOFT_RESET);
+
+  /* Initialses Communication controll area to 0 */
+  if(dev_priv->psb_rev_id >= POULSBO_D1)
+   {
+	PSB_DEBUG_GENERAL("MSVDX: Detected Poulsbo D1 or later revision.\n");
+	PSB_WMSVDX32 (MSVDX_DEVICE_NODE_FLAGS_DEFAULT_D1, MSVDX_COMMS_OFFSET_FLAGS);
+   }
+  else
+   {
+	PSB_DEBUG_GENERAL("MSVDX: Detected Poulsbo D0 or earlier revision.\n");
+        PSB_WMSVDX32 (MSVDX_DEVICE_NODE_FLAGS_DEFAULT_D0, MSVDX_COMMS_OFFSET_FLAGS);
+   }
+
+  PSB_WMSVDX32 (0, MSVDX_COMMS_MSG_COUNTER);
+  PSB_WMSVDX32 (0, MSVDX_COMMS_SIGNATURE);
+  PSB_WMSVDX32 (0, MSVDX_COMMS_TO_HOST_RD_INDEX);
+  PSB_WMSVDX32 (0, MSVDX_COMMS_TO_HOST_WRT_INDEX);
+  PSB_WMSVDX32 (0, MSVDX_COMMS_TO_MTX_RD_INDEX);
+  PSB_WMSVDX32 (0, MSVDX_COMMS_TO_MTX_WRT_INDEX);
+  PSB_WMSVDX32 (0, MSVDX_COMMS_FW_STATUS);
+
+  /* read register bank size */
+  {
+    uint32_t ui32BankSize, ui32Reg;
+    ui32Reg = PSB_RMSVDX32 (MSVDX_MTX_RAM_BANK);
+    ui32BankSize =
+      REGIO_READ_FIELD (ui32Reg, MSVDX_MTX_RAM_BANK, CR_MTX_RAM_BANK_SIZE);
+    ram_bank_size = (uint32_t) (1 << (ui32BankSize + 2));
+  }
+
+  PSB_DEBUG_GENERAL ("MSVDX: RAM bank size = %d bytes\n", ram_bank_size);
+
+  fw_ptr = msvdx_get_fw (dev, &raw, "msvdx_fw.bin");
+
+  if (!fw_ptr)
+    {
+      DRM_ERROR ("psb: No valid msvdx_fw.bin firmware found.\n");
+      ret = 1;
+      goto out;
+    }
+
+  fw = (struct msvdx_fw *) fw_ptr;
+  if (fw->ver != 0x02)
+    {
+      DRM_ERROR
+	("psb: msvdx_fw.bin firmware version mismatch, got version=%02x expected version=%02x\n",
+	 fw->ver, 0x02);
+      ret = 1;
+      goto out;
+    }
+
+  text_ptr = (uint32_t *) ((uint8_t *) fw_ptr + sizeof (struct msvdx_fw));
+  data_ptr = text_ptr + fw->text_size;
+
+  PSB_DEBUG_GENERAL ("MSVDX: Retrieved pointers for firmware\n");
+  PSB_DEBUG_GENERAL ("MSVDX: text_size: %d\n", fw->text_size);
+  PSB_DEBUG_GENERAL ("MSVDX: data_size: %d\n", fw->data_size);
+  PSB_DEBUG_GENERAL ("MSVDX: data_location: 0x%x\n", fw->data_location);
+  PSB_DEBUG_GENERAL ("MSVDX: First 4 bytes of text: 0x%x\n", *text_ptr);
+  PSB_DEBUG_GENERAL ("MSVDX: First 4 bytes of data: 0x%x\n", *data_ptr);
+
+  PSB_DEBUG_GENERAL ("MSVDX: Uploading firmware\n");
+  psb_upload_fw (dev_priv, MTX_CORE_CODE_MEM, ram_bank_size,
+		 PC_START_ADDRESS - MTX_CODE_BASE, fw->text_size, text_ptr);
+  psb_upload_fw (dev_priv, MTX_CORE_DATA_MEM, ram_bank_size,
+		 fw->data_location - MTX_DATA_BASE, fw->data_size, data_ptr);
+
+  /*todo :  Verify code upload possibly only in debug */
+  if (psb_verify_fw
+      (dev_priv, ram_bank_size, MTX_CORE_CODE_MEM,
+       PC_START_ADDRESS - MTX_CODE_BASE, fw->text_size, text_ptr))
+    {
+      /* Firmware code upload failed */
+      ret = 1;
+      goto out;
+    }
+  if (psb_verify_fw
+      (dev_priv, ram_bank_size, MTX_CORE_DATA_MEM,
+       fw->data_location - MTX_DATA_BASE, fw->data_size, data_ptr))
+    {
+      /* Firmware data upload failed */
+      ret = 1;
+      goto out;
+    }
+
+  /*      -- Set starting PC address      */
+  psb_write_mtx_core_reg (dev_priv, MTX_PC, PC_START_ADDRESS);
+
+  /*      -- Turn on the thread   */
+  PSB_WMSVDX32 (MSVDX_MTX_ENABLE_MTX_ENABLE_MASK, MSVDX_MTX_ENABLE);
+
+  /* Wait for the signature value to be written back */
+  ret = psb_wait_for_register (dev_priv, MSVDX_COMMS_SIGNATURE, MSVDX_COMMS_SIGNATURE_VALUE,	/* Required value */
+			       0xffffffff /* Enabled bits */ );
+  if (ret)
+    {
+      DRM_ERROR ("psb: MSVDX firmware fails to initialize.\n");
+      goto out;
+    }
+
+  PSB_DEBUG_GENERAL ("MSVDX: MTX Initial indications OK\n");
+  PSB_DEBUG_GENERAL ("MSVDX: MSVDX_COMMS_AREA_ADDR = %08x\n",
+		     MSVDX_COMMS_AREA_ADDR);
+out:
+  if (raw)
+    {
+      PSB_DEBUG_GENERAL ("MSVDX releasing firmware resouces....\n");
+      release_firmware (raw);
+    }
+  return ret;
+}
+
+static void
+psb_free_ccb (struct drm_buffer_object **ccb)
+{
+  drm_bo_usage_deref_unlocked (ccb);
+  *ccb = NULL;
+}
+
+/*******************************************************************************
+
+ @Function	psb_msvdx_reset
+
+ @Description
+
+ Reset chip and disable interrupts.
+
+ @Input psDeviceNode - device info. structure
+
+ @Return  0 - Success
+	  1 - Failure
+
+******************************************************************************/
+int
+psb_msvdx_reset (struct drm_psb_private *dev_priv)
+{
+  int ret = 0;
+
+  /* Issue software reset */
+  PSB_WMSVDX32 (msvdx_sw_reset_all, MSVDX_CONTROL);
+
+  ret = psb_wait_for_register (dev_priv, MSVDX_CONTROL, 0,	/* Required value */
+			       MSVDX_CONTROL_CR_MSVDX_SOFT_RESET_MASK
+			       /* Enabled bits */ );
+
+  if (!ret)
+    {
+      /* Clear interrupt enabled flag */
+      PSB_WMSVDX32 (0, MSVDX_HOST_INTERRUPT_ENABLE);
+
+      /* Clear any pending interrupt flags                                                                                    */
+      PSB_WMSVDX32 (0xFFFFFFFF, MSVDX_INTERRUPT_CLEAR);
+    }
+
+  mutex_destroy (&dev_priv->msvdx_mutex);
+
+  return ret;
+}
+
+static int
+psb_allocate_ccb (struct drm_device *dev,
+		  struct drm_buffer_object **ccb,
+		  uint32_t * base_addr, int size)
+{
+  int ret;
+  struct drm_bo_kmap_obj tmp_kmap;
+  int is_iomem;
+
+  ret = drm_buffer_object_create (dev, size,
+				  drm_bo_type_kernel,
+				  DRM_BO_FLAG_READ |
+				  DRM_PSB_FLAG_MEM_KERNEL |
+				  DRM_BO_FLAG_NO_EVICT,
+				  DRM_BO_HINT_DONT_FENCE, 0, 0, ccb);
+  if (ret)
+    {
+      PSB_DEBUG_GENERAL ("Failed to allocate CCB.\n");
+      *ccb = NULL;
+      return 1;
+    }
+
+  ret = drm_bo_kmap (*ccb, 0, (*ccb)->num_pages, &tmp_kmap);
+  if (ret)
+    {
+      PSB_DEBUG_GENERAL ("drm_bo_kmap failed ret: %d\n", ret);
+      drm_bo_usage_deref_unlocked (ccb);
+      *ccb = NULL;
+      return 1;
+    }
+
+  memset (drm_bmo_virtual (&tmp_kmap, &is_iomem), 0, size);
+  drm_bo_kunmap (&tmp_kmap);
+
+  *base_addr = (*ccb)->offset;
+  return 0;
+}
+
+int
+psb_msvdx_init (struct drm_device *dev)
+{
+  struct drm_psb_private *dev_priv = dev->dev_private;
+  uint32_t ui32Cmd;
+  int ret;
+
+  PSB_DEBUG_GENERAL ("MSVDX: psb_msvdx_init\n");
+
+  /*Initialize command msvdx queueing */
+  INIT_LIST_HEAD (&dev_priv->msvdx_queue);
+  mutex_init (&dev_priv->msvdx_mutex);
+  spin_lock_init (&dev_priv->msvdx_lock);
+  dev_priv->msvdx_busy = 0;
+
+  /*figure out the stepping*/
+  pci_read_config_byte(dev->pdev, PSB_REVID_OFFSET, &dev_priv->psb_rev_id );
+
+  /* Enable Clocks */
+  PSB_DEBUG_GENERAL ("Enabling clocks\n");
+  PSB_WMSVDX32 (clk_enable_all, MSVDX_MAN_CLK_ENABLE);
+
+  /* Enable MMU by removing all bypass bits */
+  PSB_WMSVDX32 (0, MSVDX_MMU_CONTROL0);
+
+  PSB_DEBUG_GENERAL ("MSVDX: Setting up RENDEC\n");
+  /* Allocate device virtual memory as required by rendec.... */
+  if (!dev_priv->ccb0)
+    {
+      ret =
+	psb_allocate_ccb (dev, &dev_priv->ccb0,
+			  &dev_priv->base_addr0, RENDEC_A_SIZE);
+      if (ret)
+	goto err_exit;
+    }
+
+  if (!dev_priv->ccb1)
+    {
+      ret =
+	psb_allocate_ccb (dev, &dev_priv->ccb1,
+			  &dev_priv->base_addr1, RENDEC_B_SIZE);
+      if (ret)
+	goto err_exit;
+    }
+
+  PSB_DEBUG_GENERAL ("MSVDX: RENDEC A: %08x RENDEC B: %08x\n",
+		     dev_priv->base_addr0, dev_priv->base_addr1);
+
+  PSB_WMSVDX32 (dev_priv->base_addr0, MSVDX_RENDEC_BASE_ADDR0);
+  PSB_WMSVDX32 (dev_priv->base_addr1, MSVDX_RENDEC_BASE_ADDR1);
+
+  ui32Cmd = 0;
+  REGIO_WRITE_FIELD (ui32Cmd, MSVDX_RENDEC_BUFFER_SIZE,
+		     RENDEC_BUFFER_SIZE0, RENDEC_A_SIZE / 4096);
+  REGIO_WRITE_FIELD (ui32Cmd, MSVDX_RENDEC_BUFFER_SIZE,
+		     RENDEC_BUFFER_SIZE1, RENDEC_B_SIZE / 4096);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_BUFFER_SIZE);
+
+  ui32Cmd = 0;
+  REGIO_WRITE_FIELD (ui32Cmd, MSVDX_RENDEC_CONTROL1,
+		     RENDEC_DECODE_START_SIZE, 0);
+  REGIO_WRITE_FIELD (ui32Cmd, MSVDX_RENDEC_CONTROL1, RENDEC_BURST_SIZE_W, 1);
+  REGIO_WRITE_FIELD (ui32Cmd, MSVDX_RENDEC_CONTROL1, RENDEC_BURST_SIZE_R, 1);
+  REGIO_WRITE_FIELD (ui32Cmd, MSVDX_RENDEC_CONTROL1,
+		     RENDEC_EXTERNAL_MEMORY, 1);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTROL1);
+
+  ui32Cmd = 0x00101010;
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTEXT0);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTEXT1);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTEXT2);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTEXT3);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTEXT4);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTEXT5);
+
+  ui32Cmd = 0;
+  REGIO_WRITE_FIELD (ui32Cmd, MSVDX_RENDEC_CONTROL0, RENDEC_INITIALISE, 1);
+  PSB_WMSVDX32 (ui32Cmd, MSVDX_RENDEC_CONTROL0);
+
+  ret = psb_setup_fw (dev);
+  if (ret)
+    goto err_exit;
+
+  PSB_WMSVDX32 (clk_enable_minimal, MSVDX_MAN_CLK_ENABLE);
+
+  return 0;
+
+err_exit:
+  if (dev_priv->ccb0)
+    psb_free_ccb (&dev_priv->ccb0);
+  if (dev_priv->ccb1)
+    psb_free_ccb (&dev_priv->ccb1);
+
+  return 1;
+}
+
+int
+psb_msvdx_uninit (struct drm_device *dev)
+{
+  struct drm_psb_private *dev_priv = dev->dev_private;
+
+  /*Reset MSVDX chip */
+  psb_msvdx_reset (dev_priv);
+
+//  PSB_WMSVDX32 (clk_enable_minimal, MSVDX_MAN_CLK_ENABLE);
+    printk("set the msvdx clock to 0 in the %s\n", __FUNCTION__);
+    PSB_WMSVDX32 (0, MSVDX_MAN_CLK_ENABLE);
+
+  /*Clean up resources...*/
+  if (dev_priv->ccb0)
+    psb_free_ccb (&dev_priv->ccb0);
+  if (dev_priv->ccb1)
+    psb_free_ccb (&dev_priv->ccb1);
+
+  return 0;
+}
+
+int psb_hw_info_ioctl(struct drm_device *dev, void *data,
+                            struct drm_file *file_priv)
+{
+    struct drm_psb_private *dev_priv = dev->dev_private;
+    struct drm_psb_hw_info *hw_info = data;
+    struct pci_dev * pci_root = pci_get_bus_and_slot(0, 0);
+
+    hw_info->rev_id = dev_priv->psb_rev_id;
+
+    /*read the fuse info to determine the caps*/
+    pci_write_config_dword(pci_root, 0xD0, PCI_PORT5_REG80_FFUSE);
+    pci_read_config_dword(pci_root, 0xD4, &hw_info->caps);
+
+    PSB_DEBUG_GENERAL("MSVDX: PSB caps: 0x%x\n", hw_info->caps);
+    return 0;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_reg.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_reg.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,562 @@
+/**************************************************************************
+ *
+ * Copyright (c) (2005-2007) Imagination Technologies Limited.
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ */
+#ifndef _PSB_REG_H_
+#define _PSB_REG_H_
+
+#define PSB_CR_CLKGATECTL                0x0000
+#define _PSB_C_CLKGATECTL_AUTO_MAN_REG   (1 << 24)
+#define _PSB_C_CLKGATECTL_USE_CLKG_SHIFT (20)
+#define _PSB_C_CLKGATECTL_USE_CLKG_MASK  (0x3 << 20)
+#define _PSB_C_CLKGATECTL_DPM_CLKG_SHIFT (16)
+#define _PSB_C_CLKGATECTL_DPM_CLKG_MASK  (0x3 << 16)
+#define _PSB_C_CLKGATECTL_TA_CLKG_SHIFT  (12)
+#define _PSB_C_CLKGATECTL_TA_CLKG_MASK   (0x3 << 12)
+#define _PSB_C_CLKGATECTL_TSP_CLKG_SHIFT (8)
+#define _PSB_C_CLKGATECTL_TSP_CLKG_MASK  (0x3 << 8)
+#define _PSB_C_CLKGATECTL_ISP_CLKG_SHIFT (4)
+#define _PSB_C_CLKGATECTL_ISP_CLKG_MASK  (0x3 << 4)
+#define _PSB_C_CLKGATECTL_2D_CLKG_SHIFT  (0)
+#define _PSB_C_CLKGATECTL_2D_CLKG_MASK   (0x3 << 0)
+#define _PSB_C_CLKGATECTL_CLKG_ENABLED   (0)
+#define _PSB_C_CLKGATECTL_CLKG_DISABLED  (1)
+#define _PSB_C_CLKGATECTL_CLKG_AUTO      (2)
+
+#define PSB_CR_CORE_ID                   0x0010
+#define _PSB_CC_ID_ID_SHIFT              (16)
+#define _PSB_CC_ID_ID_MASK               (0xFFFF << 16)
+#define _PSB_CC_ID_CONFIG_SHIFT          (0)
+#define _PSB_CC_ID_CONFIG_MASK           (0xFFFF << 0)
+
+#define PSB_CR_CORE_REVISION               0x0014
+#define _PSB_CC_REVISION_DESIGNER_SHIFT    (24)
+#define _PSB_CC_REVISION_DESIGNER_MASK     (0xFF << 24)
+#define _PSB_CC_REVISION_MAJOR_SHIFT       (16)
+#define _PSB_CC_REVISION_MAJOR_MASK        (0xFF << 16)
+#define _PSB_CC_REVISION_MINOR_SHIFT       (8)
+#define _PSB_CC_REVISION_MINOR_MASK        (0xFF << 8)
+#define _PSB_CC_REVISION_MAINTENANCE_SHIFT (0)
+#define _PSB_CC_REVISION_MAINTENANCE_MASK  (0xFF << 0)
+
+#define PSB_CR_DESIGNER_REV_FIELD1       0x0018
+
+#define PSB_CR_SOFT_RESET                0x0080
+#define _PSB_CS_RESET_TSP_RESET          (1 << 6)
+#define _PSB_CS_RESET_ISP_RESET          (1 << 5)
+#define _PSB_CS_RESET_USE_RESET          (1 << 4)
+#define _PSB_CS_RESET_TA_RESET           (1 << 3)
+#define _PSB_CS_RESET_DPM_RESET          (1 << 2)
+#define _PSB_CS_RESET_TWOD_RESET         (1 << 1)
+#define _PSB_CS_RESET_BIF_RESET          (1 << 0)
+
+#define PSB_CR_DESIGNER_REV_FIELD2       0x001C
+
+#define PSB_CR_EVENT_HOST_ENABLE2        0x0110
+
+#define PSB_CR_EVENT_STATUS2             0x0118
+
+#define PSB_CR_EVENT_HOST_CLEAR2         0x0114
+#define _PSB_CE2_BIF_REQUESTER_FAULT     (1 << 4)
+
+#define PSB_CR_EVENT_STATUS              0x012C
+
+#define PSB_CR_EVENT_HOST_ENABLE         0x0130
+
+#define PSB_CR_EVENT_HOST_CLEAR          0x0134
+#define _PSB_CE_MASTER_INTERRUPT         (1 << 31)
+#define _PSB_CE_TA_DPM_FAULT             (1 << 28)
+#define _PSB_CE_TWOD_COMPLETE            (1 << 27)
+#define _PSB_CE_DPM_OUT_OF_MEMORY_ZLS    (1 << 25)
+#define _PSB_CE_DPM_TA_MEM_FREE          (1 << 24)
+#define _PSB_CE_PIXELBE_END_RENDER       (1 << 18)
+#define _PSB_CE_SW_EVENT                 (1 << 14)
+#define _PSB_CE_TA_FINISHED              (1 << 13)
+#define _PSB_CE_TA_TERMINATE             (1 << 12)
+#define _PSB_CE_DPM_REACHED_MEM_THRESH   (1 << 3)
+#define _PSB_CE_DPM_OUT_OF_MEMORY_GBL    (1 << 2)
+#define _PSB_CE_DPM_OUT_OF_MEMORY_MT     (1 << 1)
+#define _PSB_CE_DPM_3D_MEM_FREE          (1 << 0)
+
+
+#define PSB_USE_OFFSET_MASK              0x0007FFFF
+#define PSB_USE_OFFSET_SIZE              (PSB_USE_OFFSET_MASK + 1)
+#define PSB_CR_USE_CODE_BASE0            0x0A0C
+#define PSB_CR_USE_CODE_BASE1            0x0A10
+#define PSB_CR_USE_CODE_BASE2            0x0A14
+#define PSB_CR_USE_CODE_BASE3            0x0A18
+#define PSB_CR_USE_CODE_BASE4            0x0A1C
+#define PSB_CR_USE_CODE_BASE5            0x0A20
+#define PSB_CR_USE_CODE_BASE6            0x0A24
+#define PSB_CR_USE_CODE_BASE7            0x0A28
+#define PSB_CR_USE_CODE_BASE8            0x0A2C
+#define PSB_CR_USE_CODE_BASE9            0x0A30
+#define PSB_CR_USE_CODE_BASE10           0x0A34
+#define PSB_CR_USE_CODE_BASE11           0x0A38
+#define PSB_CR_USE_CODE_BASE12           0x0A3C
+#define PSB_CR_USE_CODE_BASE13           0x0A40
+#define PSB_CR_USE_CODE_BASE14           0x0A44
+#define PSB_CR_USE_CODE_BASE15           0x0A48
+#define PSB_CR_USE_CODE_BASE(_i) (0x0A0C + ((_i) << 2))
+#define _PSB_CUC_BASE_DM_SHIFT           (25)
+#define _PSB_CUC_BASE_DM_MASK            (0x3 << 25)
+#define _PSB_CUC_BASE_ADDR_SHIFT         (0) // 1024-bit aligned address?
+#define _PSB_CUC_BASE_ADDR_ALIGNSHIFT    (7)
+#define _PSB_CUC_BASE_ADDR_MASK          (0x1FFFFFF << 0)
+#define _PSB_CUC_DM_VERTEX		 (0)
+#define _PSB_CUC_DM_PIXEL		 (1)
+#define _PSB_CUC_DM_RESERVED		 (2)
+#define _PSB_CUC_DM_EDM	                 (3)
+
+#define PSB_CR_PDS_EXEC_BASE             0x0AB8
+#define _PSB_CR_PDS_EXEC_BASE_ADDR_SHIFT (20) // 1MB aligned address
+#define _PSB_CR_PDS_EXEC_BASE_ADDR_ALIGNSHIFT (20)
+
+#define PSB_CR_EVENT_KICKER              0x0AC4
+#define _PSB_CE_KICKER_ADDRESS_SHIFT     (4) // 128-bit aligned address
+
+#define PSB_CR_EVENT_KICK                0x0AC8
+#define _PSB_CE_KICK_NOW                 (1 << 0)
+
+
+#define PSB_CR_BIF_DIR_LIST_BASE1        0x0C38
+
+#define PSB_CR_BIF_CTRL                  0x0C00
+#define _PSB_CB_CTRL_CLEAR_FAULT         (1 << 4)
+#define _PSB_CB_CTRL_INVALDC             (1 << 3)
+#define _PSB_CB_CTRL_FLUSH               (1 << 2)
+
+#define PSB_CR_BIF_INT_STAT              0x0C04
+
+#define PSB_CR_BIF_FAULT                 0x0C08
+#define _PSB_CBI_STAT_PF_N_RW            (1 << 14)
+#define _PSB_CBI_STAT_FAULT_SHIFT        (0)
+#define _PSB_CBI_STAT_FAULT_MASK         (0x3FFF << 0)
+#define _PSB_CBI_STAT_FAULT_CACHE        (1 << 1)
+#define _PSB_CBI_STAT_FAULT_TA           (1 << 2)
+#define _PSB_CBI_STAT_FAULT_VDM          (1 << 3)
+#define _PSB_CBI_STAT_FAULT_2D           (1 << 4)
+#define _PSB_CBI_STAT_FAULT_PBE          (1 << 5)
+#define _PSB_CBI_STAT_FAULT_TSP          (1 << 6)
+#define _PSB_CBI_STAT_FAULT_ISP          (1 << 7)
+#define _PSB_CBI_STAT_FAULT_USSEPDS      (1 << 8)
+#define _PSB_CBI_STAT_FAULT_HOST         (1 << 9)
+
+#define PSB_CR_BIF_BANK0                 0x0C78
+
+#define PSB_CR_BIF_BANK1                 0x0C7C
+
+#define PSB_CR_BIF_DIR_LIST_BASE0        0x0C84
+
+#define PSB_CR_BIF_TWOD_REQ_BASE         0x0C88
+#define PSB_CR_BIF_3D_REQ_BASE           0x0CAC
+
+#define PSB_CR_2D_SOCIF                  0x0E18
+#define _PSB_C2_SOCIF_FREESPACE_SHIFT    (0)
+#define _PSB_C2_SOCIF_FREESPACE_MASK     (0xFF << 0)
+#define _PSB_C2_SOCIF_EMPTY              (0x80 << 0)
+
+#define PSB_CR_2D_BLIT_STATUS            0x0E04
+#define _PSB_C2B_STATUS_BUSY             (1 << 24)
+#define _PSB_C2B_STATUS_COMPLETE_SHIFT   (0)
+#define _PSB_C2B_STATUS_COMPLETE_MASK    (0xFFFFFF << 0)
+
+/*
+ * 2D defs.
+ */
+
+/*
+ * 2D Slave Port Data : Block Header's Object Type
+ */
+
+#define	PSB_2D_CLIP_BH                   (0x00000000)
+#define	PSB_2D_PAT_BH                    (0x10000000)
+#define	PSB_2D_CTRL_BH                   (0x20000000)
+#define	PSB_2D_SRC_OFF_BH                (0x30000000)
+#define	PSB_2D_MASK_OFF_BH               (0x40000000)
+#define	PSB_2D_RESERVED1_BH              (0x50000000)
+#define	PSB_2D_RESERVED2_BH              (0x60000000)
+#define	PSB_2D_FENCE_BH                  (0x70000000)
+#define	PSB_2D_BLIT_BH                   (0x80000000)
+#define	PSB_2D_SRC_SURF_BH               (0x90000000)
+#define	PSB_2D_DST_SURF_BH               (0xA0000000)
+#define	PSB_2D_PAT_SURF_BH               (0xB0000000)
+#define	PSB_2D_SRC_PAL_BH                (0xC0000000)
+#define	PSB_2D_PAT_PAL_BH                (0xD0000000)
+#define	PSB_2D_MASK_SURF_BH              (0xE0000000)
+#define	PSB_2D_FLUSH_BH                  (0xF0000000)
+
+/*
+ * Clip Definition block (PSB_2D_CLIP_BH)
+ */
+#define PSB_2D_CLIPCOUNT_MAX             (1)
+#define PSB_2D_CLIPCOUNT_MASK            (0x00000000)
+#define PSB_2D_CLIPCOUNT_CLRMASK         (0xFFFFFFFF)
+#define PSB_2D_CLIPCOUNT_SHIFT           (0)
+// clip rectangle min & max
+#define PSB_2D_CLIP_XMAX_MASK            (0x00FFF000)
+#define PSB_2D_CLIP_XMAX_CLRMASK         (0xFF000FFF)
+#define PSB_2D_CLIP_XMAX_SHIFT           (12)
+#define PSB_2D_CLIP_XMIN_MASK            (0x00000FFF)
+#define PSB_2D_CLIP_XMIN_CLRMASK         (0x00FFF000)
+#define PSB_2D_CLIP_XMIN_SHIFT           (0)
+// clip rectangle offset
+#define PSB_2D_CLIP_YMAX_MASK            (0x00FFF000)
+#define PSB_2D_CLIP_YMAX_CLRMASK         (0xFF000FFF)
+#define PSB_2D_CLIP_YMAX_SHIFT           (12)
+#define PSB_2D_CLIP_YMIN_MASK            (0x00000FFF)
+#define PSB_2D_CLIP_YMIN_CLRMASK         (0x00FFF000)
+#define PSB_2D_CLIP_YMIN_SHIFT           (0)
+
+/*
+ * Pattern Control (PSB_2D_PAT_BH)
+ */
+#define PSB_2D_PAT_HEIGHT_MASK           (0x0000001F)
+#define PSB_2D_PAT_HEIGHT_SHIFT          (0)
+#define PSB_2D_PAT_WIDTH_MASK            (0x000003E0)
+#define PSB_2D_PAT_WIDTH_SHIFT           (5)
+#define PSB_2D_PAT_YSTART_MASK           (0x00007C00)
+#define PSB_2D_PAT_YSTART_SHIFT          (10)
+#define PSB_2D_PAT_XSTART_MASK           (0x000F8000)
+#define PSB_2D_PAT_XSTART_SHIFT          (15)
+
+/*
+ * 2D Control block (PSB_2D_CTRL_BH)
+ */
+// Present Flags
+#define PSB_2D_SRCCK_CTRL                (0x00000001)
+#define PSB_2D_DSTCK_CTRL                (0x00000002)
+#define PSB_2D_ALPHA_CTRL                (0x00000004)
+// Colour Key Colour (SRC/DST)
+#define PSB_2D_CK_COL_MASK               (0xFFFFFFFF)
+#define PSB_2D_CK_COL_CLRMASK            (0x00000000)
+#define PSB_2D_CK_COL_SHIFT              (0)
+// Colour Key Mask (SRC/DST)
+#define PSB_2D_CK_MASK_MASK              (0xFFFFFFFF)
+#define PSB_2D_CK_MASK_CLRMASK           (0x00000000)
+#define PSB_2D_CK_MASK_SHIFT             (0)
+// Alpha Control (Alpha/RGB)
+#define PSB_2D_GBLALPHA_MASK             (0x000FF000)
+#define PSB_2D_GBLALPHA_CLRMASK          (0xFFF00FFF)
+#define PSB_2D_GBLALPHA_SHIFT            (12)
+#define PSB_2D_SRCALPHA_OP_MASK          (0x00700000)
+#define PSB_2D_SRCALPHA_OP_CLRMASK       (0xFF8FFFFF)
+#define PSB_2D_SRCALPHA_OP_SHIFT         (20)
+#define PSB_2D_SRCALPHA_OP_ONE           (0x00000000)
+#define PSB_2D_SRCALPHA_OP_SRC           (0x00100000)
+#define PSB_2D_SRCALPHA_OP_DST           (0x00200000)
+#define PSB_2D_SRCALPHA_OP_SG            (0x00300000)
+#define PSB_2D_SRCALPHA_OP_DG            (0x00400000)
+#define PSB_2D_SRCALPHA_OP_GBL           (0x00500000)
+#define PSB_2D_SRCALPHA_OP_ZERO          (0x00600000)
+#define PSB_2D_SRCALPHA_INVERT           (0x00800000)
+#define PSB_2D_SRCALPHA_INVERT_CLR       (0xFF7FFFFF)
+#define PSB_2D_DSTALPHA_OP_MASK          (0x07000000)
+#define PSB_2D_DSTALPHA_OP_CLRMASK       (0xF8FFFFFF)
+#define PSB_2D_DSTALPHA_OP_SHIFT         (24)
+#define PSB_2D_DSTALPHA_OP_ONE           (0x00000000)
+#define PSB_2D_DSTALPHA_OP_SRC           (0x01000000)
+#define PSB_2D_DSTALPHA_OP_DST           (0x02000000)
+#define PSB_2D_DSTALPHA_OP_SG            (0x03000000)
+#define PSB_2D_DSTALPHA_OP_DG            (0x04000000)
+#define PSB_2D_DSTALPHA_OP_GBL           (0x05000000)
+#define PSB_2D_DSTALPHA_OP_ZERO          (0x06000000)
+#define PSB_2D_DSTALPHA_INVERT           (0x08000000)
+#define PSB_2D_DSTALPHA_INVERT_CLR       (0xF7FFFFFF)
+
+#define PSB_2D_PRE_MULTIPLICATION_ENABLE  (0x10000000)
+#define PSB_2D_PRE_MULTIPLICATION_CLRMASK (0xEFFFFFFF)
+#define PSB_2D_ZERO_SOURCE_ALPHA_ENABLE   (0x20000000)
+#define PSB_2D_ZERO_SOURCE_ALPHA_CLRMASK  (0xDFFFFFFF)
+
+/*
+ *Source Offset (PSB_2D_SRC_OFF_BH)
+ */
+#define PSB_2D_SRCOFF_XSTART_MASK        ((0x00000FFF) << 12)
+#define PSB_2D_SRCOFF_XSTART_SHIFT       (12)
+#define PSB_2D_SRCOFF_YSTART_MASK        (0x00000FFF)
+#define PSB_2D_SRCOFF_YSTART_SHIFT       (0)
+
+/*
+ * Mask Offset (PSB_2D_MASK_OFF_BH)
+ */
+#define PSB_2D_MASKOFF_XSTART_MASK       ((0x00000FFF) << 12)
+#define PSB_2D_MASKOFF_XSTART_SHIFT      (12)
+#define PSB_2D_MASKOFF_YSTART_MASK       (0x00000FFF)
+#define PSB_2D_MASKOFF_YSTART_SHIFT      (0)
+
+/*
+ * 2D Fence (see PSB_2D_FENCE_BH): bits 0:27 are ignored
+ */
+
+/*
+ *Blit Rectangle (PSB_2D_BLIT_BH)
+ */
+
+#define PSB_2D_ROT_MASK                  (3<<25)
+#define PSB_2D_ROT_CLRMASK               (~PSB_2D_ROT_MASK)
+#define PSB_2D_ROT_NONE                  (0<<25)
+#define PSB_2D_ROT_90DEGS                (1<<25)
+#define PSB_2D_ROT_180DEGS               (2<<25)
+#define PSB_2D_ROT_270DEGS               (3<<25)
+
+#define PSB_2D_COPYORDER_MASK            (3<<23)
+#define PSB_2D_COPYORDER_CLRMASK         (~PSB_2D_COPYORDER_MASK)
+#define PSB_2D_COPYORDER_TL2BR           (0<<23)
+#define PSB_2D_COPYORDER_BR2TL           (1<<23)
+#define PSB_2D_COPYORDER_TR2BL           (2<<23)
+#define PSB_2D_COPYORDER_BL2TR           (3<<23)
+
+#define PSB_2D_DSTCK_CLRMASK             (0xFF9FFFFF)
+#define PSB_2D_DSTCK_DISABLE             (0x00000000)
+#define PSB_2D_DSTCK_PASS                (0x00200000)
+#define PSB_2D_DSTCK_REJECT              (0x00400000)
+
+#define PSB_2D_SRCCK_CLRMASK             (0xFFE7FFFF)
+#define PSB_2D_SRCCK_DISABLE             (0x00000000)
+#define PSB_2D_SRCCK_PASS                (0x00080000)
+#define PSB_2D_SRCCK_REJECT              (0x00100000)
+
+#define PSB_2D_CLIP_ENABLE               (0x00040000)
+
+#define PSB_2D_ALPHA_ENABLE              (0x00020000)
+
+#define PSB_2D_PAT_CLRMASK               (0xFFFEFFFF)
+#define PSB_2D_PAT_MASK                  (0x00010000)
+#define PSB_2D_USE_PAT                   (0x00010000)
+#define PSB_2D_USE_FILL                  (0x00000000)
+/*
+ * Tungsten Graphics note on rop codes: If rop A and rop B are
+ * identical, the mask surface will not be read and need not be
+ * set up.
+ */
+
+#define PSB_2D_ROP3B_MASK                (0x0000FF00)
+#define PSB_2D_ROP3B_CLRMASK             (0xFFFF00FF)
+#define PSB_2D_ROP3B_SHIFT               (8)
+// rop code A
+#define PSB_2D_ROP3A_MASK                (0x000000FF)
+#define PSB_2D_ROP3A_CLRMASK             (0xFFFFFF00)
+#define PSB_2D_ROP3A_SHIFT               (0)
+
+#define PSB_2D_ROP4_MASK                 (0x0000FFFF)
+/*
+ *	DWORD0:	(Only pass if Pattern control == Use Fill Colour)
+ *	Fill Colour RGBA8888
+ */
+#define PSB_2D_FILLCOLOUR_MASK           (0xFFFFFFFF)
+#define PSB_2D_FILLCOLOUR_SHIFT          (0)
+/*
+ *	DWORD1: (Always Present)
+ *	X Start (Dest)
+ *	Y Start (Dest)
+ */
+#define PSB_2D_DST_XSTART_MASK           (0x00FFF000)
+#define PSB_2D_DST_XSTART_CLRMASK        (0xFF000FFF)
+#define PSB_2D_DST_XSTART_SHIFT          (12)
+#define PSB_2D_DST_YSTART_MASK           (0x00000FFF)
+#define PSB_2D_DST_YSTART_CLRMASK        (0xFFFFF000)
+#define PSB_2D_DST_YSTART_SHIFT          (0)
+/*
+ *	DWORD2: (Always Present)
+ *	X Size (Dest)
+ *	Y Size (Dest)
+ */
+#define PSB_2D_DST_XSIZE_MASK            (0x00FFF000)
+#define PSB_2D_DST_XSIZE_CLRMASK         (0xFF000FFF)
+#define PSB_2D_DST_XSIZE_SHIFT           (12)
+#define PSB_2D_DST_YSIZE_MASK            (0x00000FFF)
+#define PSB_2D_DST_YSIZE_CLRMASK         (0xFFFFF000)
+#define PSB_2D_DST_YSIZE_SHIFT           (0)
+
+/*
+ * Source Surface (PSB_2D_SRC_SURF_BH)
+ */
+/*
+ *      WORD 0
+ */
+
+#define PSB_2D_SRC_FORMAT_MASK           (0x00078000)
+#define PSB_2D_SRC_1_PAL                 (0x00000000)
+#define PSB_2D_SRC_2_PAL                 (0x00008000)
+#define PSB_2D_SRC_4_PAL                 (0x00010000)
+#define PSB_2D_SRC_8_PAL                 (0x00018000)
+#define PSB_2D_SRC_8_ALPHA               (0x00020000)
+#define PSB_2D_SRC_4_ALPHA               (0x00028000)
+#define PSB_2D_SRC_332RGB                (0x00030000)
+#define PSB_2D_SRC_4444ARGB              (0x00038000)
+#define PSB_2D_SRC_555RGB                (0x00040000)
+#define PSB_2D_SRC_1555ARGB              (0x00048000)
+#define PSB_2D_SRC_565RGB                (0x00050000)
+#define PSB_2D_SRC_0888ARGB              (0x00058000)
+#define PSB_2D_SRC_8888ARGB              (0x00060000)
+#define PSB_2D_SRC_8888UYVY              (0x00068000)
+#define PSB_2D_SRC_RESERVED              (0x00070000)
+#define PSB_2D_SRC_1555ARGB_LOOKUP       (0x00078000)
+
+
+#define PSB_2D_SRC_STRIDE_MASK           (0x00007FFF)
+#define PSB_2D_SRC_STRIDE_CLRMASK        (0xFFFF8000)
+#define PSB_2D_SRC_STRIDE_SHIFT          (0)
+/*
+ *  WORD 1 - Base Address
+ */
+#define PSB_2D_SRC_ADDR_MASK             (0x0FFFFFFC)
+#define PSB_2D_SRC_ADDR_CLRMASK          (0x00000003)
+#define PSB_2D_SRC_ADDR_SHIFT            (2)
+#define PSB_2D_SRC_ADDR_ALIGNSHIFT       (2)
+
+/*
+ * Pattern Surface (PSB_2D_PAT_SURF_BH)
+ */
+/*
+ *  WORD 0
+ */
+
+#define PSB_2D_PAT_FORMAT_MASK           (0x00078000)
+#define PSB_2D_PAT_1_PAL                 (0x00000000)
+#define PSB_2D_PAT_2_PAL                 (0x00008000)
+#define PSB_2D_PAT_4_PAL                 (0x00010000)
+#define PSB_2D_PAT_8_PAL                 (0x00018000)
+#define PSB_2D_PAT_8_ALPHA               (0x00020000)
+#define PSB_2D_PAT_4_ALPHA               (0x00028000)
+#define PSB_2D_PAT_332RGB                (0x00030000)
+#define PSB_2D_PAT_4444ARGB              (0x00038000)
+#define PSB_2D_PAT_555RGB                (0x00040000)
+#define PSB_2D_PAT_1555ARGB              (0x00048000)
+#define PSB_2D_PAT_565RGB                (0x00050000)
+#define PSB_2D_PAT_0888ARGB              (0x00058000)
+#define PSB_2D_PAT_8888ARGB              (0x00060000)
+
+#define PSB_2D_PAT_STRIDE_MASK           (0x00007FFF)
+#define PSB_2D_PAT_STRIDE_CLRMASK        (0xFFFF8000)
+#define PSB_2D_PAT_STRIDE_SHIFT          (0)
+/*
+ *  WORD 1 - Base Address
+ */
+#define PSB_2D_PAT_ADDR_MASK             (0x0FFFFFFC)
+#define PSB_2D_PAT_ADDR_CLRMASK          (0x00000003)
+#define PSB_2D_PAT_ADDR_SHIFT            (2)
+#define PSB_2D_PAT_ADDR_ALIGNSHIFT       (2)
+
+/*
+ * Destination Surface (PSB_2D_DST_SURF_BH)
+ */
+/*
+ * WORD 0
+ */
+
+#define PSB_2D_DST_FORMAT_MASK           (0x00078000)
+#define PSB_2D_DST_332RGB                (0x00030000)
+#define PSB_2D_DST_4444ARGB              (0x00038000)
+#define PSB_2D_DST_555RGB                (0x00040000)
+#define PSB_2D_DST_1555ARGB              (0x00048000)
+#define PSB_2D_DST_565RGB                (0x00050000)
+#define PSB_2D_DST_0888ARGB              (0x00058000)
+#define PSB_2D_DST_8888ARGB              (0x00060000)
+#define PSB_2D_DST_8888AYUV              (0x00070000)
+
+#define PSB_2D_DST_STRIDE_MASK           (0x00007FFF)
+#define PSB_2D_DST_STRIDE_CLRMASK        (0xFFFF8000)
+#define PSB_2D_DST_STRIDE_SHIFT          (0)
+/*
+ * WORD 1 - Base Address
+ */
+#define PSB_2D_DST_ADDR_MASK             (0x0FFFFFFC)
+#define PSB_2D_DST_ADDR_CLRMASK          (0x00000003)
+#define PSB_2D_DST_ADDR_SHIFT            (2)
+#define PSB_2D_DST_ADDR_ALIGNSHIFT       (2)
+
+/*
+ * Mask Surface (PSB_2D_MASK_SURF_BH)
+ */
+/*
+ * WORD 0
+ */
+#define PSB_2D_MASK_STRIDE_MASK          (0x00007FFF)
+#define PSB_2D_MASK_STRIDE_CLRMASK       (0xFFFF8000)
+#define PSB_2D_MASK_STRIDE_SHIFT         (0)
+/*
+ *  WORD 1 - Base Address
+ */
+#define PSB_2D_MASK_ADDR_MASK            (0x0FFFFFFC)
+#define PSB_2D_MASK_ADDR_CLRMASK         (0x00000003)
+#define PSB_2D_MASK_ADDR_SHIFT           (2)
+#define PSB_2D_MASK_ADDR_ALIGNSHIFT      (2)
+
+/*
+ * Source Palette (PSB_2D_SRC_PAL_BH)
+ */
+
+#define PSB_2D_SRCPAL_ADDR_SHIFT         (0)
+#define PSB_2D_SRCPAL_ADDR_CLRMASK       (0xF0000007)
+#define PSB_2D_SRCPAL_ADDR_MASK          (0x0FFFFFF8)
+#define PSB_2D_SRCPAL_BYTEALIGN          (1024)
+
+/*
+ * Pattern Palette (PSB_2D_PAT_PAL_BH)
+ */
+
+#define PSB_2D_PATPAL_ADDR_SHIFT         (0)
+#define PSB_2D_PATPAL_ADDR_CLRMASK       (0xF0000007)
+#define PSB_2D_PATPAL_ADDR_MASK          (0x0FFFFFF8)
+#define PSB_2D_PATPAL_BYTEALIGN          (1024)
+
+/*
+ * Rop3 Codes (2 LS bytes)
+ */
+
+#define PSB_2D_ROP3_SRCCOPY              (0xCCCC)
+#define PSB_2D_ROP3_PATCOPY              (0xF0F0)
+#define PSB_2D_ROP3_WHITENESS            (0xFFFF)
+#define PSB_2D_ROP3_BLACKNESS            (0x0000)
+#define PSB_2D_ROP3_SRC                  (0xCC)
+#define PSB_2D_ROP3_PAT                  (0xF0)
+#define PSB_2D_ROP3_DST                  (0xAA)
+
+
+/*
+ * Sizes.
+ */
+
+#define PSB_SCENE_HW_COOKIE_SIZE 16
+#define PSB_TA_MEM_HW_COOKIE_SIZE 16
+
+/*
+ * Scene stuff.
+ */
+
+#define PSB_NUM_HW_SCENES          2
+
+/*
+ * Scheduler completion actions.
+ */
+
+#define PSB_RASTER_BLOCK 0
+#define PSB_RASTER       1
+#define PSB_RETURN       2
+#define PSB_TA       3
+
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_regman.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_regman.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,175 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ */
+
+#include "drmP.h"
+#include "psb_drv.h"
+
+struct psb_use_reg {
+	struct drm_reg reg;
+	struct drm_psb_private *dev_priv;
+	uint32_t reg_seq;
+	uint32_t base;
+	uint32_t data_master;
+};
+
+struct psb_use_reg_data {
+	uint32_t base;
+	uint32_t size;
+	uint32_t data_master;
+};
+
+static int psb_use_reg_reusable(const struct drm_reg *reg, const void *data)
+{
+	struct psb_use_reg *use_reg =
+	    container_of(reg, struct psb_use_reg, reg);
+	struct psb_use_reg_data *use_data = (struct psb_use_reg_data *)data;
+
+	return ((use_reg->base <= use_data->base) &&
+		(use_reg->base + PSB_USE_OFFSET_SIZE >
+		 use_data->base + use_data->size) &&
+		use_reg->data_master == use_data->data_master);
+}
+
+static int psb_use_reg_set(struct psb_use_reg *use_reg,
+			   const struct psb_use_reg_data *use_data)
+{
+	struct drm_psb_private *dev_priv = use_reg->dev_priv;
+
+	if (use_reg->reg.fence == NULL)
+		use_reg->data_master = use_data->data_master;
+
+	if (use_reg->reg.fence == NULL &&
+	    !psb_use_reg_reusable(&use_reg->reg, (const void *)use_data)) {
+
+		use_reg->base = use_data->base & ~PSB_USE_OFFSET_MASK;
+		use_reg->data_master = use_data->data_master;
+
+		if (!psb_use_reg_reusable(&use_reg->reg,
+					  (const void *)use_data)) {
+			DRM_ERROR("USE base mechanism didn't support "
+				  "buffer size or alignment\n");
+			return -EINVAL;
+		}
+
+		PSB_WSGX32(PSB_ALPL(use_reg->base, _PSB_CUC_BASE_ADDR) |
+			   (use_reg->data_master << _PSB_CUC_BASE_DM_SHIFT),
+			   PSB_CR_USE_CODE_BASE(use_reg->reg_seq));
+	}
+	return 0;
+
+}
+
+int psb_grab_use_base(struct drm_psb_private *dev_priv,
+		      unsigned long base,
+		      unsigned long size,
+		      unsigned int data_master,
+		      uint32_t fence_class,
+		      uint32_t fence_type,
+		      int no_wait,
+		      int interruptible, int *r_reg, uint32_t * r_offset)
+{
+	struct psb_use_reg_data use_data = {
+		.base = base,
+		.size = size,
+		.data_master = data_master
+	};
+	int ret;
+
+	struct drm_reg *reg;
+	struct psb_use_reg *use_reg;
+
+	ret = drm_regs_alloc(&dev_priv->use_manager,
+			     (const void *)&use_data,
+			     fence_class,
+			     fence_type, interruptible, no_wait, &reg);
+	if (ret)
+		return ret;
+
+	use_reg = container_of(reg, struct psb_use_reg, reg);
+	ret = psb_use_reg_set(use_reg, &use_data);
+
+	if (ret)
+		return ret;
+
+	*r_reg = use_reg->reg_seq;
+	*r_offset = base - use_reg->base;
+
+	return 0;
+};
+
+static void psb_use_reg_destroy(struct drm_reg *reg)
+{
+	struct psb_use_reg *use_reg =
+	    container_of(reg, struct psb_use_reg, reg);
+	struct drm_psb_private *dev_priv = use_reg->dev_priv;
+
+	PSB_WSGX32(PSB_ALPL(0, _PSB_CUC_BASE_ADDR),
+		   PSB_CR_USE_CODE_BASE(use_reg->reg_seq));
+
+	drm_free(use_reg, sizeof(*use_reg), DRM_MEM_DRIVER);
+}
+
+int psb_init_use_base(struct drm_psb_private *dev_priv,
+		      unsigned int reg_start, unsigned int reg_num)
+{
+	struct psb_use_reg *use_reg;
+	int i;
+	int ret = 0;
+
+	mutex_lock(&dev_priv->cmdbuf_mutex);
+
+	drm_regs_init(&dev_priv->use_manager,
+		      &psb_use_reg_reusable, &psb_use_reg_destroy);
+
+	for (i = reg_start; i < reg_start + reg_num; ++i) {
+		use_reg = drm_calloc(1, sizeof(*use_reg), DRM_MEM_DRIVER);
+		if (!use_reg) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		use_reg->dev_priv = dev_priv;
+		use_reg->reg_seq = i;
+		use_reg->base = 0;
+		use_reg->data_master = _PSB_CUC_DM_PIXEL;
+
+		PSB_WSGX32(PSB_ALPL(use_reg->base, _PSB_CUC_BASE_ADDR) |
+			   (use_reg->data_master << _PSB_CUC_BASE_DM_SHIFT),
+			   PSB_CR_USE_CODE_BASE(use_reg->reg_seq));
+
+		drm_regs_add(&dev_priv->use_manager, &use_reg->reg);
+	}
+      out:
+	mutex_unlock(&dev_priv->cmdbuf_mutex);
+
+	return ret;
+
+}
+
+void psb_takedown_use_base(struct drm_psb_private *dev_priv)
+{
+	mutex_lock(&dev_priv->cmdbuf_mutex);
+	drm_regs_free(&dev_priv->use_manager);
+	mutex_unlock(&dev_priv->cmdbuf_mutex);
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_reset.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_reset.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,374 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors:
+ * Thomas Hellstrom <thomas-at-tungstengraphics-dot-com>
+ */
+
+#include "drmP.h"
+#include "psb_drv.h"
+#include "psb_reg.h"
+#include "psb_scene.h"
+#include "psb_msvdx.h"
+
+#define PSB_2D_TIMEOUT_MSEC 100
+
+void psb_reset(struct drm_psb_private *dev_priv, int reset_2d)
+{
+	uint32_t val;
+
+	val = _PSB_CS_RESET_BIF_RESET |
+	    _PSB_CS_RESET_DPM_RESET |
+	    _PSB_CS_RESET_TA_RESET |
+	    _PSB_CS_RESET_USE_RESET |
+	    _PSB_CS_RESET_ISP_RESET | _PSB_CS_RESET_TSP_RESET;
+
+	if (reset_2d)
+		val |= _PSB_CS_RESET_TWOD_RESET;
+
+	PSB_WSGX32(val, PSB_CR_SOFT_RESET);
+	(void)PSB_RSGX32(PSB_CR_SOFT_RESET);
+
+	msleep(1);
+
+	PSB_WSGX32(0, PSB_CR_SOFT_RESET);
+	wmb();
+	PSB_WSGX32(PSB_RSGX32(PSB_CR_BIF_CTRL) | _PSB_CB_CTRL_CLEAR_FAULT,
+		   PSB_CR_BIF_CTRL);
+	wmb();
+	(void)PSB_RSGX32(PSB_CR_BIF_CTRL);
+
+	msleep(1);
+	PSB_WSGX32(PSB_RSGX32(PSB_CR_BIF_CTRL) & ~_PSB_CB_CTRL_CLEAR_FAULT,
+		   PSB_CR_BIF_CTRL);
+	(void)PSB_RSGX32(PSB_CR_BIF_CTRL);
+}
+
+void psb_print_pagefault(struct drm_psb_private *dev_priv)
+{
+	uint32_t val;
+	uint32_t addr;
+
+	val = PSB_RSGX32(PSB_CR_BIF_INT_STAT);
+	addr = PSB_RSGX32(PSB_CR_BIF_FAULT);
+
+	if (val) {
+		if (val & _PSB_CBI_STAT_PF_N_RW)
+			DRM_ERROR("Poulsbo MMU page fault:\n");
+		else
+			DRM_ERROR("Poulsbo MMU read / write "
+				  "protection fault:\n");
+
+		if (val & _PSB_CBI_STAT_FAULT_CACHE)
+			DRM_ERROR("\tCache requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_TA)
+			DRM_ERROR("\tTA requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_VDM)
+			DRM_ERROR("\tVDM requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_2D)
+			DRM_ERROR("\t2D requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_PBE)
+			DRM_ERROR("\tPBE requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_TSP)
+			DRM_ERROR("\tTSP requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_ISP)
+			DRM_ERROR("\tISP requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_USSEPDS)
+			DRM_ERROR("\tUSSEPDS requestor.\n");
+		if (val & _PSB_CBI_STAT_FAULT_HOST)
+			DRM_ERROR("\tHost requestor.\n");
+
+		DRM_ERROR("\tMMU failing address is 0x%08x.\n", (unsigned)addr);
+	}
+}
+
+void psb_schedule_watchdog(struct drm_psb_private *dev_priv)
+{
+	struct timer_list *wt = &dev_priv->watchdog_timer;
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&dev_priv->watchdog_lock, irq_flags);
+	if (dev_priv->timer_available && !timer_pending(wt)) {
+		wt->expires = jiffies + PSB_WATCHDOG_DELAY;
+		add_timer(wt);
+	}
+	spin_unlock_irqrestore(&dev_priv->watchdog_lock, irq_flags);
+}
+
+#if 0
+static void psb_seq_lockup_idle(struct drm_psb_private *dev_priv,
+				unsigned int engine, int *lockup, int *idle)
+{
+	uint32_t received_seq;
+
+	received_seq = dev_priv->comm[engine << 4];
+	spin_lock(&dev_priv->sequence_lock);
+	*idle = (received_seq == dev_priv->sequence[engine]);
+	spin_unlock(&dev_priv->sequence_lock);
+
+	if (*idle) {
+		dev_priv->idle[engine] = 1;
+		*lockup = 0;
+		return;
+	}
+
+	if (dev_priv->idle[engine]) {
+		dev_priv->idle[engine] = 0;
+		dev_priv->last_sequence[engine] = received_seq;
+		*lockup = 0;
+		return;
+	}
+
+	*lockup = (dev_priv->last_sequence[engine] == received_seq);
+}
+
+#endif
+static void psb_watchdog_func(unsigned long data)
+{
+	struct drm_psb_private *dev_priv = (struct drm_psb_private *)data;
+	int lockup;
+	int msvdx_lockup;
+	int msvdx_idle;
+	int lockup_2d;
+	int idle_2d;
+	int idle;
+	unsigned long irq_flags;
+
+	psb_scheduler_lockup(dev_priv, &lockup, &idle);
+	psb_msvdx_lockup(dev_priv, &msvdx_lockup, &msvdx_idle);
+#if 0
+	psb_seq_lockup_idle(dev_priv, PSB_ENGINE_2D, &lockup_2d, &idle_2d);
+#else
+	lockup_2d = 0;
+	idle_2d = 1;
+#endif
+	if (lockup || msvdx_lockup || lockup_2d) {
+		spin_lock_irqsave(&dev_priv->watchdog_lock, irq_flags);
+		dev_priv->timer_available = 0;
+		spin_unlock_irqrestore(&dev_priv->watchdog_lock, irq_flags);
+		if (lockup) {
+			psb_print_pagefault(dev_priv);
+			schedule_work(&dev_priv->watchdog_wq);
+		}
+		if (msvdx_lockup)
+			schedule_work(&dev_priv->msvdx_watchdog_wq);
+	}
+	if (!idle || !msvdx_idle || !idle_2d)
+		psb_schedule_watchdog(dev_priv);
+}
+
+void psb_msvdx_flush_cmd_queue(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	struct psb_msvdx_cmd_queue *msvdx_cmd;
+	struct list_head *list, *next;
+	/*Flush the msvdx cmd queue and signal all fences in the queue */
+	list_for_each_safe(list, next, &dev_priv->msvdx_queue) {
+		msvdx_cmd = list_entry(list, struct psb_msvdx_cmd_queue, head);
+		PSB_DEBUG_GENERAL("MSVDXQUE: flushing sequence:%d\n",
+				  msvdx_cmd->sequence);
+		dev_priv->msvdx_current_sequence = msvdx_cmd->sequence;
+		psb_fence_error(dev, PSB_ENGINE_VIDEO,
+				dev_priv->msvdx_current_sequence,
+				DRM_FENCE_TYPE_EXE, DRM_CMD_HANG);
+		list_del(list);
+		kfree(msvdx_cmd->cmd);
+		drm_free(msvdx_cmd, sizeof(struct psb_msvdx_cmd_queue),
+			 DRM_MEM_DRIVER);
+	}
+}
+
+static void psb_msvdx_reset_wq(struct work_struct *work)
+{
+	struct drm_psb_private *dev_priv =
+	    container_of(work, struct drm_psb_private, msvdx_watchdog_wq);
+
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+
+	mutex_lock(&dev_priv->msvdx_mutex);
+	dev_priv->msvdx_needs_reset = 1;
+	dev_priv->msvdx_current_sequence++;
+	PSB_DEBUG_GENERAL
+	    ("MSVDXFENCE: incremented msvdx_current_sequence to :%d\n",
+	     dev_priv->msvdx_current_sequence);
+
+	psb_fence_error(scheduler->dev, PSB_ENGINE_VIDEO,
+			dev_priv->msvdx_current_sequence, DRM_FENCE_TYPE_EXE,
+			DRM_CMD_HANG);
+
+	spin_lock_irqsave(&dev_priv->watchdog_lock, irq_flags);
+	dev_priv->timer_available = 1;
+	spin_unlock_irqrestore(&dev_priv->watchdog_lock, irq_flags);
+
+	spin_lock_irqsave(&dev_priv->msvdx_lock, irq_flags);
+	psb_msvdx_flush_cmd_queue(scheduler->dev);
+	spin_unlock_irqrestore(&dev_priv->msvdx_lock, irq_flags);
+
+	psb_schedule_watchdog(dev_priv);
+	mutex_unlock(&dev_priv->msvdx_mutex);
+}
+
+static int psb_xhw_mmu_reset(struct drm_psb_private *dev_priv)
+{
+	struct psb_xhw_buf buf;
+	uint32_t bif_ctrl;
+
+	INIT_LIST_HEAD(&buf.head);
+	psb_mmu_set_pd_context(psb_mmu_get_default_pd(dev_priv->mmu), 0);
+	bif_ctrl = PSB_RSGX32(PSB_CR_BIF_CTRL);
+	PSB_WSGX32(bif_ctrl |
+		   _PSB_CB_CTRL_CLEAR_FAULT |
+		   _PSB_CB_CTRL_INVALDC, PSB_CR_BIF_CTRL);
+	(void)PSB_RSGX32(PSB_CR_BIF_CTRL);
+	msleep(1);
+	PSB_WSGX32(bif_ctrl, PSB_CR_BIF_CTRL);
+	(void)PSB_RSGX32(PSB_CR_BIF_CTRL);
+	return psb_xhw_reset_dpm(dev_priv, &buf);
+}
+
+/*
+ * Block command submission and reset hardware and schedulers.
+ */
+
+static void psb_reset_wq(struct work_struct *work)
+{
+	struct drm_psb_private *dev_priv =
+	    container_of(work, struct drm_psb_private, watchdog_wq);
+	int lockup_2d;
+	int idle_2d;
+	unsigned long irq_flags;
+	int ret;
+	int reset_count = 0;
+	struct psb_xhw_buf buf;
+	uint32_t xhw_lockup;
+
+	/*
+	 * Block command submission.
+	 */
+
+	mutex_lock(&dev_priv->reset_mutex);
+
+	INIT_LIST_HEAD(&buf.head);
+	if (psb_xhw_check_lockup(dev_priv, &buf, &xhw_lockup) == 0) {
+		if (xhw_lockup == 0 && psb_extend_raster_timeout(dev_priv) == 0) {
+			/*
+			 * no lockup, just re-schedule
+			 */
+			spin_lock_irqsave(&dev_priv->watchdog_lock, irq_flags);
+			dev_priv->timer_available = 1;
+			spin_unlock_irqrestore(&dev_priv->watchdog_lock,
+					       irq_flags);
+			psb_schedule_watchdog(dev_priv);
+			mutex_unlock(&dev_priv->reset_mutex);
+			return;
+		}
+	}
+#if 0
+	msleep(PSB_2D_TIMEOUT_MSEC);
+
+	psb_seq_lockup_idle(dev_priv, PSB_ENGINE_2D, &lockup_2d, &idle_2d);
+
+	if (lockup_2d) {
+		uint32_t seq_2d;
+		spin_lock(&dev_priv->sequence_lock);
+		seq_2d = dev_priv->sequence[PSB_ENGINE_2D];
+		spin_unlock(&dev_priv->sequence_lock);
+		psb_fence_error(dev_priv->scheduler.dev,
+				PSB_ENGINE_2D,
+				seq_2d, DRM_FENCE_TYPE_EXE, -EBUSY);
+		DRM_INFO("Resetting 2D engine.\n");
+	}
+
+	psb_reset(dev_priv, lockup_2d);
+#else
+	(void)lockup_2d;
+	(void)idle_2d;
+	psb_reset(dev_priv, 0);
+#endif
+	(void)psb_xhw_mmu_reset(dev_priv);
+	DRM_INFO("Resetting scheduler.\n");
+	psb_scheduler_pause(dev_priv);
+	psb_scheduler_reset(dev_priv, -EBUSY);
+	psb_scheduler_ta_mem_check(dev_priv);
+
+	while (dev_priv->ta_mem &&
+	       !dev_priv->force_ta_mem_load && ++reset_count < 10) {
+
+		/*
+		 * TA memory is currently fenced so offsets
+		 * are valid. Reload offsets into the dpm now.
+		 */
+
+		struct psb_xhw_buf buf;
+		INIT_LIST_HEAD(&buf.head);
+
+		msleep(100);
+		DRM_INFO("Trying to reload TA memory.\n");
+		ret = psb_xhw_ta_mem_load(dev_priv, &buf,
+					  PSB_TA_MEM_FLAG_TA |
+					  PSB_TA_MEM_FLAG_RASTER |
+					  PSB_TA_MEM_FLAG_HOSTA |
+					  PSB_TA_MEM_FLAG_HOSTD |
+					  PSB_TA_MEM_FLAG_INIT,
+					  dev_priv->ta_mem->ta_memory->offset,
+					  dev_priv->ta_mem->hw_data->offset,
+					  dev_priv->ta_mem->hw_cookie);
+		if (!ret)
+			break;
+
+		psb_reset(dev_priv, 0);
+		(void)psb_xhw_mmu_reset(dev_priv);
+	}
+
+	psb_scheduler_restart(dev_priv);
+	spin_lock_irqsave(&dev_priv->watchdog_lock, irq_flags);
+	dev_priv->timer_available = 1;
+	spin_unlock_irqrestore(&dev_priv->watchdog_lock, irq_flags);
+	mutex_unlock(&dev_priv->reset_mutex);
+}
+
+void psb_watchdog_init(struct drm_psb_private *dev_priv)
+{
+	struct timer_list *wt = &dev_priv->watchdog_timer;
+	unsigned long irq_flags;
+
+	dev_priv->watchdog_lock = SPIN_LOCK_UNLOCKED;
+	spin_lock_irqsave(&dev_priv->watchdog_lock, irq_flags);
+	init_timer(wt);
+	INIT_WORK(&dev_priv->watchdog_wq, &psb_reset_wq);
+	INIT_WORK(&dev_priv->msvdx_watchdog_wq, &psb_msvdx_reset_wq);
+	wt->data = (unsigned long)dev_priv;
+	wt->function = &psb_watchdog_func;
+	dev_priv->timer_available = 1;
+	spin_unlock_irqrestore(&dev_priv->watchdog_lock, irq_flags);
+}
+
+void psb_watchdog_takedown(struct drm_psb_private *dev_priv)
+{
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&dev_priv->watchdog_lock, irq_flags);
+	dev_priv->timer_available = 0;
+	spin_unlock_irqrestore(&dev_priv->watchdog_lock, irq_flags);
+	(void)del_timer_sync(&dev_priv->watchdog_timer);
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_scene.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_scene.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,531 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics.com>
+ */
+
+#include "drmP.h"
+#include "psb_drv.h"
+#include "psb_scene.h"
+
+void psb_clear_scene_atomic(struct psb_scene *scene)
+{
+	int i;
+	struct page *page;
+	void *v;
+
+	for (i = 0; i < scene->clear_num_pages; ++i) {
+		page = drm_ttm_get_page(scene->hw_data->ttm,
+					scene->clear_p_start + i);
+		if (in_irq())
+			v = kmap_atomic(page, KM_IRQ0);
+		else
+			v = kmap_atomic(page, KM_USER0);
+
+		memset(v, 0, PAGE_SIZE);
+
+		if (in_irq())
+			kunmap_atomic(v, KM_IRQ0);
+		else
+			kunmap_atomic(v, KM_USER0);
+	}
+}
+
+int psb_clear_scene(struct psb_scene *scene)
+{
+	struct drm_bo_kmap_obj bmo;
+	int is_iomem;
+	void *addr;
+
+	int ret = drm_bo_kmap(scene->hw_data, scene->clear_p_start,
+			      scene->clear_num_pages, &bmo);
+
+	PSB_DEBUG_RENDER("Scene clear\n");
+	if (ret)
+		return ret;
+
+	addr = drm_bmo_virtual(&bmo, &is_iomem);
+	BUG_ON(is_iomem);
+	memset(addr, 0, scene->clear_num_pages << PAGE_SHIFT);
+	drm_bo_kunmap(&bmo);
+
+	return 0;
+}
+
+static void psb_destroy_scene_devlocked(struct psb_scene *scene)
+{
+	if (!scene)
+		return;
+
+	PSB_DEBUG_RENDER("Scene destroy\n");
+	drm_bo_usage_deref_locked(&scene->hw_data);
+	drm_free(scene, sizeof(*scene), DRM_MEM_DRIVER);
+}
+
+void psb_scene_unref_devlocked(struct psb_scene **scene)
+{
+	struct psb_scene *tmp_scene = *scene;
+
+	PSB_DEBUG_RENDER("Scene unref\n");
+	*scene = NULL;
+	if (atomic_dec_and_test(&tmp_scene->ref_count)) {
+		psb_scheduler_remove_scene_refs(tmp_scene);
+		psb_destroy_scene_devlocked(tmp_scene);
+	}
+}
+
+struct psb_scene *psb_scene_ref(struct psb_scene *src)
+{
+	PSB_DEBUG_RENDER("Scene ref\n");
+	atomic_inc(&src->ref_count);
+	return src;
+}
+
+static struct psb_scene *psb_alloc_scene(struct drm_device *dev,
+					 uint32_t w, uint32_t h)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	int ret = -EINVAL;
+	struct psb_scene *scene;
+	uint32_t bo_size;
+	struct psb_xhw_buf buf;
+
+	PSB_DEBUG_RENDER("Alloc scene w %u h %u\n", w, h);
+
+	scene = drm_calloc(1, sizeof(*scene), DRM_MEM_DRIVER);
+
+	if (!scene) {
+		DRM_ERROR("Out of memory allocating scene object.\n");
+		return NULL;
+	}
+
+	scene->dev = dev;
+	scene->w = w;
+	scene->h = h;
+	scene->hw_scene = NULL;
+	atomic_set(&scene->ref_count, 1);
+
+	INIT_LIST_HEAD(&buf.head);
+	ret = psb_xhw_scene_info(dev_priv, &buf, scene->w, scene->h,
+				 scene->hw_cookie, &bo_size,
+				 &scene->clear_p_start,
+				 &scene->clear_num_pages);
+	if (ret)
+		goto out_err;
+
+	ret = drm_buffer_object_create(dev, bo_size, drm_bo_type_kernel,
+				       DRM_PSB_FLAG_MEM_MMU |
+				       DRM_BO_FLAG_READ |
+				       DRM_BO_FLAG_CACHED |
+				       PSB_BO_FLAG_SCENE |
+				       DRM_BO_FLAG_WRITE,
+				       DRM_BO_HINT_DONT_FENCE,
+				       0, 0, &scene->hw_data);
+	if (ret)
+		goto out_err;
+
+	return scene;
+      out_err:
+	drm_free(scene, sizeof(*scene), DRM_MEM_DRIVER);
+	return NULL;
+}
+
+int psb_validate_scene_pool(struct psb_scene_pool *pool, uint64_t flags,
+			    uint64_t mask,
+			    uint32_t hint,
+			    uint32_t w,
+			    uint32_t h,
+			    int final_pass, struct psb_scene **scene_p)
+{
+	struct drm_device *dev = pool->dev;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct psb_scene *scene = pool->scenes[pool->cur_scene];
+	int ret;
+	unsigned long irq_flags;
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	uint32_t bin_pt_offset;
+	uint32_t bin_param_offset;
+
+	PSB_DEBUG_RENDER("Validate scene pool. Scene %u\n", pool->cur_scene);
+
+	if (unlikely(!dev_priv->ta_mem)) {
+		dev_priv->ta_mem =
+		    psb_alloc_ta_mem(dev, dev_priv->ta_mem_pages);
+		if (!dev_priv->ta_mem)
+			return -ENOMEM;
+
+		bin_pt_offset = ~0;
+		bin_param_offset = ~0;
+	} else {
+		bin_pt_offset = dev_priv->ta_mem->hw_data->offset;
+		bin_param_offset = dev_priv->ta_mem->ta_memory->offset;
+	}
+
+	pool->w = w;
+	pool->h = h;
+	if (scene && (scene->w != pool->w || scene->h != pool->h)) {
+		spin_lock_irqsave(&scheduler->lock, irq_flags);
+		if (scene->flags & PSB_SCENE_FLAG_DIRTY) {
+			spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+			DRM_ERROR("Trying to resize a dirty scene.\n");
+			return -EINVAL;
+		}
+		spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+		mutex_lock(&dev->struct_mutex);
+		psb_scene_unref_devlocked(&pool->scenes[pool->cur_scene]);
+		mutex_unlock(&dev->struct_mutex);
+		scene = NULL;
+	}
+
+	if (!scene) {
+		pool->scenes[pool->cur_scene] = scene =
+		    psb_alloc_scene(pool->dev, pool->w, pool->h);
+
+		if (!scene)
+			return -ENOMEM;
+
+		scene->flags = PSB_SCENE_FLAG_CLEARED;
+	}
+
+	/*
+	 * FIXME: We need atomic bit manipulation here for the
+	 * scheduler. For now use the spinlock.
+	 */
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	if (!(scene->flags & PSB_SCENE_FLAG_CLEARED)) {
+		spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+		PSB_DEBUG_RENDER("Waiting to clear scene memory.\n");
+		mutex_lock(&scene->hw_data->mutex);
+		ret = drm_bo_wait(scene->hw_data, 0, 0, 0);
+		mutex_unlock(&scene->hw_data->mutex);
+		if (ret)
+			return ret;
+
+		ret = psb_clear_scene(scene);
+
+		if (ret)
+			return ret;
+		spin_lock_irqsave(&scheduler->lock, irq_flags);
+		scene->flags |= PSB_SCENE_FLAG_CLEARED;
+	}
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+
+	ret = drm_bo_do_validate(scene->hw_data, flags, mask, hint,
+				 PSB_ENGINE_TA, 0, NULL);
+	if (ret)
+		return ret;
+	ret = drm_bo_do_validate(dev_priv->ta_mem->hw_data, 0, 0, 0,
+				 PSB_ENGINE_TA, 0, NULL);
+	if (ret)
+		return ret;
+	ret = drm_bo_do_validate(dev_priv->ta_mem->ta_memory, 0, 0, 0,
+				 PSB_ENGINE_TA, 0, NULL);
+	if (ret)
+		return ret;
+
+	if (unlikely(bin_param_offset !=
+		     dev_priv->ta_mem->ta_memory->offset ||
+		     bin_pt_offset !=
+		     dev_priv->ta_mem->hw_data->offset ||
+		     dev_priv->force_ta_mem_load)) {
+
+		struct psb_xhw_buf buf;
+
+		INIT_LIST_HEAD(&buf.head);
+		ret = psb_xhw_ta_mem_load(dev_priv, &buf,
+					  PSB_TA_MEM_FLAG_TA |
+					  PSB_TA_MEM_FLAG_RASTER |
+					  PSB_TA_MEM_FLAG_HOSTA |
+					  PSB_TA_MEM_FLAG_HOSTD |
+					  PSB_TA_MEM_FLAG_INIT,
+					  dev_priv->ta_mem->ta_memory->offset,
+					  dev_priv->ta_mem->hw_data->offset,
+					  dev_priv->ta_mem->hw_cookie);
+		if (ret)
+			return ret;
+
+		dev_priv->force_ta_mem_load = 0;
+	}
+
+	if (final_pass) {
+
+		/*
+		 * Clear the scene on next use. Advance the scene counter.
+		 */
+
+		spin_lock_irqsave(&scheduler->lock, irq_flags);
+		scene->flags &= ~PSB_SCENE_FLAG_CLEARED;
+		spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+		pool->cur_scene = (pool->cur_scene + 1) % pool->num_scenes;
+	}
+
+	*scene_p = psb_scene_ref(scene);
+	return 0;
+}
+
+static void psb_scene_pool_destroy_devlocked(struct psb_scene_pool *pool)
+{
+	int i;
+
+	if (!pool)
+		return;
+
+	PSB_DEBUG_RENDER("Scene pool destroy.\n");
+	for (i = 0; i < pool->num_scenes; ++i) {
+		PSB_DEBUG_RENDER("scenes %d is 0x%08lx\n", i,
+				 (unsigned long)pool->scenes[i]);
+		if (pool->scenes[i])
+			psb_scene_unref_devlocked(&pool->scenes[i]);
+	}
+	drm_free(pool, sizeof(*pool), DRM_MEM_DRIVER);
+}
+
+void psb_scene_pool_unref_devlocked(struct psb_scene_pool **pool)
+{
+	struct psb_scene_pool *tmp_pool = *pool;
+	struct drm_device *dev = tmp_pool->dev;
+
+	PSB_DEBUG_RENDER("Scene pool unref\n");
+	(void)dev;
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	*pool = NULL;
+	if (--tmp_pool->ref_count == 0)
+		psb_scene_pool_destroy_devlocked(tmp_pool);
+}
+
+struct psb_scene_pool *psb_scene_pool_ref_devlocked(struct psb_scene_pool *src)
+{
+	++src->ref_count;
+	return src;
+}
+
+/*
+ * Callback for user object manager.
+ */
+
+static void psb_scene_pool_destroy(struct drm_file *priv,
+				   struct drm_user_object *base)
+{
+	struct psb_scene_pool *pool =
+	    drm_user_object_entry(base, struct psb_scene_pool, user);
+
+	psb_scene_pool_unref_devlocked(&pool);
+}
+
+struct psb_scene_pool *psb_scene_pool_lookup_devlocked(struct drm_file *priv,
+						       uint32_t handle,
+						       int check_owner)
+{
+	struct drm_user_object *uo;
+	struct psb_scene_pool *pool;
+
+	uo = drm_lookup_user_object(priv, handle);
+	if (!uo || (uo->type != PSB_USER_OBJECT_SCENE_POOL)) {
+		DRM_ERROR("Could not find scene pool object 0x%08x\n", handle);
+		return NULL;
+	}
+
+	if (check_owner && priv != uo->owner) {
+		if (!drm_lookup_ref_object(priv, uo, _DRM_REF_USE))
+			return NULL;
+	}
+
+	pool = drm_user_object_entry(uo, struct psb_scene_pool, user);
+	return psb_scene_pool_ref_devlocked(pool);
+}
+
+struct psb_scene_pool *psb_scene_pool_alloc(struct drm_file *priv,
+					    int shareable,
+					    uint32_t num_scenes,
+					    uint32_t w, uint32_t h)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct psb_scene_pool *pool;
+	int ret;
+
+	PSB_DEBUG_RENDER("Scene pool alloc\n");
+	pool = drm_calloc(1, sizeof(*pool), DRM_MEM_DRIVER);
+	if (!pool) {
+		DRM_ERROR("Out of memory allocating scene pool object.\n");
+		return NULL;
+	}
+	pool->w = w;
+	pool->h = h;
+	pool->dev = dev;
+	pool->num_scenes = num_scenes;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = drm_add_user_object(priv, &pool->user, shareable);
+	if (ret)
+		goto out_err;
+
+	pool->user.type = PSB_USER_OBJECT_SCENE_POOL;
+	pool->user.remove = &psb_scene_pool_destroy;
+	pool->ref_count = 2;
+	mutex_unlock(&dev->struct_mutex);
+	return pool;
+      out_err:
+	drm_free(pool, sizeof(*pool), DRM_MEM_DRIVER);
+	return NULL;
+}
+
+/*
+ * Code to support multiple ta memory buffers.
+ */
+
+static void psb_destroy_ta_mem_devlocked(struct psb_ta_mem *ta_mem)
+{
+	if (!ta_mem)
+		return;
+
+	drm_bo_usage_deref_locked(&ta_mem->hw_data);
+	drm_bo_usage_deref_locked(&ta_mem->ta_memory);
+	drm_free(ta_mem, sizeof(*ta_mem), DRM_MEM_DRIVER);
+}
+
+void psb_ta_mem_unref_devlocked(struct psb_ta_mem **ta_mem)
+{
+	struct psb_ta_mem *tmp_ta_mem = *ta_mem;
+	struct drm_device *dev = tmp_ta_mem->dev;
+
+	(void)dev;
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	*ta_mem = NULL;
+	if (--tmp_ta_mem->ref_count == 0)
+		psb_destroy_ta_mem_devlocked(tmp_ta_mem);
+}
+
+void psb_ta_mem_ref_devlocked(struct psb_ta_mem **dst, struct psb_ta_mem *src)
+{
+	struct drm_device *dev = src->dev;
+
+	(void)dev;
+	DRM_ASSERT_LOCKED(&dev->struct_mutex);
+	*dst = src;
+	++src->ref_count;
+}
+
+struct psb_ta_mem *psb_alloc_ta_mem(struct drm_device *dev, uint32_t pages)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	int ret = -EINVAL;
+	struct psb_ta_mem *ta_mem;
+	uint32_t bo_size;
+	struct psb_xhw_buf buf;
+
+	INIT_LIST_HEAD(&buf.head);
+
+	ta_mem = drm_calloc(1, sizeof(*ta_mem), DRM_MEM_DRIVER);
+
+	if (!ta_mem) {
+		DRM_ERROR("Out of memory allocating parameter memory.\n");
+		return NULL;
+	}
+
+	ret = psb_xhw_ta_mem_info(dev_priv, &buf, pages,
+				  ta_mem->hw_cookie, &bo_size);
+	if (ret == -ENOMEM) {
+		DRM_ERROR("Parameter memory size is too small.\n");
+		DRM_INFO("Attempted to use %u kiB of parameter memory.\n",
+			 (unsigned int)(pages * (PAGE_SIZE / 1024)));
+		DRM_INFO("The Xpsb driver thinks this is too small and\n");
+		DRM_INFO("suggests %u kiB. Check the psb DRM\n",
+			 (unsigned int)(bo_size / 1024));
+		DRM_INFO("\"ta_mem_size\" parameter!\n");
+	}
+	if (ret)
+		goto out_err0;
+
+	bo_size = pages * PAGE_SIZE;
+	ta_mem->dev = dev;
+	ret = drm_buffer_object_create(dev, bo_size, drm_bo_type_kernel,
+				       DRM_PSB_FLAG_MEM_MMU | DRM_BO_FLAG_READ |
+				       DRM_BO_FLAG_WRITE |
+				       PSB_BO_FLAG_SCENE,
+				       DRM_BO_HINT_DONT_FENCE, 0, 0,
+				       &ta_mem->hw_data);
+	if (ret)
+		goto out_err0;
+
+	ret =
+	    drm_buffer_object_create(dev, pages << PAGE_SHIFT,
+				     drm_bo_type_kernel,
+				     DRM_PSB_FLAG_MEM_RASTGEOM |
+				     DRM_BO_FLAG_READ |
+				     DRM_BO_FLAG_WRITE |
+				     PSB_BO_FLAG_SCENE,
+				     DRM_BO_HINT_DONT_FENCE, 0,
+				     1024 * 1024 >> PAGE_SHIFT,
+				     &ta_mem->ta_memory);
+	if (ret)
+		goto out_err1;
+
+	ta_mem->ref_count = 1;
+	return ta_mem;
+      out_err1:
+	drm_bo_usage_deref_unlocked(&ta_mem->hw_data);
+      out_err0:
+	drm_free(ta_mem, sizeof(*ta_mem), DRM_MEM_DRIVER);
+	return NULL;
+}
+
+int drm_psb_scene_unref_ioctl(struct drm_device *dev,
+			      void *data, struct drm_file *file_priv)
+{
+	struct drm_psb_scene *scene = (struct drm_psb_scene *)data;
+	struct drm_user_object *uo;
+	struct drm_ref_object *ro;
+	int ret = 0;
+
+	mutex_lock(&dev->struct_mutex);
+	if (!scene->handle_valid)
+		goto out_unlock;
+
+	uo = drm_lookup_user_object(file_priv, scene->handle);
+	if (!uo) {
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+	if (uo->type != PSB_USER_OBJECT_SCENE_POOL) {
+		DRM_ERROR("Not a scene pool object.\n");
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+	if (uo->owner != file_priv) {
+		DRM_ERROR("Not owner of scene pool object.\n");
+		ret = -EPERM;
+		goto out_unlock;
+	}
+
+	scene->handle_valid = 0;
+	ro = drm_lookup_ref_object(file_priv, uo, _DRM_REF_USE);
+	BUG_ON(!ro);
+	drm_remove_ref_object(file_priv, ro);
+
+      out_unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_scene.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_scene.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,112 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics.com>
+ */
+
+#ifndef _PSB_SCENE_H_
+#define _PSB_SCENE_H_
+
+#define PSB_USER_OBJECT_SCENE_POOL    drm_driver_type0
+#define PSB_USER_OBJECT_TA_MEM       drm_driver_type1
+#define PSB_MAX_NUM_SCENES            8
+
+struct psb_hw_scene;
+struct psb_hw_ta_mem;
+
+struct psb_scene_pool {
+	struct drm_device *dev;
+	struct drm_user_object user;
+	uint32_t ref_count;
+	uint32_t w;
+	uint32_t h;
+	uint32_t cur_scene;
+	struct psb_scene *scenes[PSB_MAX_NUM_SCENES];
+	uint32_t num_scenes;
+};
+
+struct psb_scene {
+	struct drm_device *dev;
+	atomic_t ref_count;
+	uint32_t hw_cookie[PSB_SCENE_HW_COOKIE_SIZE];
+	uint32_t bo_size;
+	uint32_t w;
+	uint32_t h;
+	struct psb_ta_mem *ta_mem;
+	struct psb_hw_scene *hw_scene;
+	struct drm_buffer_object *hw_data;
+	uint32_t flags;
+	uint32_t clear_p_start;
+	uint32_t clear_num_pages;
+};
+
+struct psb_scene_entry {
+	struct list_head head;
+	struct psb_scene *scene;
+};
+
+struct psb_user_scene {
+	struct drm_device *dev;
+	struct drm_user_object user;
+};
+
+struct psb_ta_mem {
+	struct drm_device *dev;
+	struct drm_user_object user;
+	uint32_t ref_count;
+	uint32_t hw_cookie[PSB_TA_MEM_HW_COOKIE_SIZE];
+	uint32_t bo_size;
+	struct drm_buffer_object *ta_memory;
+	struct drm_buffer_object *hw_data;
+	int is_deallocating;
+	int deallocating_scheduled;
+};
+
+extern struct psb_scene_pool *psb_scene_pool_alloc(struct drm_file *priv,
+						   int shareable,
+						   uint32_t num_scenes,
+						   uint32_t w, uint32_t h);
+extern void psb_scene_pool_unref_devlocked(struct psb_scene_pool **pool);
+extern struct psb_scene_pool *psb_scene_pool_lookup_devlocked(struct drm_file
+							      *priv,
+							      uint32_t handle,
+							      int check_owner);
+extern int psb_validate_scene_pool(struct psb_scene_pool *pool, uint64_t flags,
+				   uint64_t mask, uint32_t hint, uint32_t w,
+				   uint32_t h, int final_pass,
+				   struct psb_scene **scene_p);
+extern void psb_scene_unref_devlocked(struct psb_scene **scene);
+extern struct psb_scene *psb_scene_ref(struct psb_scene *src);
+extern int drm_psb_scene_unref_ioctl(struct drm_device *dev,
+				     void *data, struct drm_file *file_priv);
+
+static inline uint32_t psb_scene_pool_handle(struct psb_scene_pool *pool)
+{
+	return pool->user.hash.key;
+}
+extern struct psb_ta_mem *psb_alloc_ta_mem(struct drm_device *dev,
+					   uint32_t pages);
+extern void psb_ta_mem_ref_devlocked(struct psb_ta_mem **dst,
+				     struct psb_ta_mem *src);
+extern void psb_ta_mem_unref_devlocked(struct psb_ta_mem **ta_mem);
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_schedule.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_schedule.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,1445 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics.com>
+ */
+
+#include "drmP.h"
+#include "psb_drm.h"
+#include "psb_drv.h"
+#include "psb_reg.h"
+#include "psb_scene.h"
+
+#define PSB_ALLOWED_RASTER_RUNTIME (DRM_HZ * 20)
+#define PSB_RASTER_TIMEOUT (DRM_HZ / 2)
+#define PSB_TA_TIMEOUT (DRM_HZ / 5)
+
+#undef PSB_SOFTWARE_WORKAHEAD
+
+#ifdef PSB_STABLE_SETTING
+
+/*
+ * Software blocks completely while the engines are working so there can be no
+ * overlap.
+ */
+
+#define PSB_WAIT_FOR_RASTER_COMPLETION
+#define PSB_WAIT_FOR_TA_COMPLETION
+
+#elif defined(PSB_PARANOID_SETTING)
+/*
+ * Software blocks "almost" while the engines are working so there can be no
+ * overlap.
+ */
+
+#define PSB_WAIT_FOR_RASTER_COMPLETION
+#define PSB_WAIT_FOR_TA_COMPLETION
+#define PSB_BE_PARANOID
+
+#elif defined(PSB_SOME_OVERLAP_BUT_LOCKUP)
+/*
+ * Software leaps ahead while the rasterizer is running and prepares
+ * a new ta job that can be scheduled before the rasterizer has
+ * finished.
+ */
+
+#define PSB_WAIT_FOR_TA_COMPLETION
+
+#elif defined(PSB_SOFTWARE_WORKAHEAD)
+/*
+ * Don't sync, but allow software to work ahead. and queue a number of jobs.
+ * But block overlapping in the scheduler.
+ */
+
+#define PSB_BLOCK_OVERLAP
+#define ONLY_ONE_JOB_IN_RASTER_QUEUE
+
+#endif
+
+/*
+ * Avoid pixelbe pagefaults on C0.
+ */
+#if 0
+#define PSB_BLOCK_OVERLAP
+#endif
+
+static void psb_dispatch_ta(struct drm_psb_private *dev_priv,
+			    struct psb_scheduler *scheduler,
+			    uint32_t reply_flag);
+static void psb_dispatch_raster(struct drm_psb_private *dev_priv,
+				struct psb_scheduler *scheduler,
+				uint32_t reply_flag);
+
+#ifdef FIX_TG_16
+
+static void psb_2d_atomic_unlock(struct drm_psb_private *dev_priv);
+static int psb_2d_trylock(struct drm_psb_private *dev_priv);
+static int psb_check_2d_idle(struct drm_psb_private *dev_priv);
+
+#endif
+
+void psb_scheduler_lockup(struct drm_psb_private *dev_priv,
+			  int *lockup, int *idle)
+{
+	unsigned long irq_flags;
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+
+	*lockup = 0;
+	*idle = 1;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+
+	if (scheduler->current_task[PSB_SCENE_ENGINE_TA] != NULL &&
+	    time_after_eq(jiffies, scheduler->ta_end_jiffies)) {
+		*lockup = 1;
+	}
+	if (!*lockup
+	    && (scheduler->current_task[PSB_SCENE_ENGINE_RASTER] != NULL)
+	    && time_after_eq(jiffies, scheduler->raster_end_jiffies)) {
+		*lockup = 1;
+	}
+	if (!*lockup)
+		*idle = scheduler->idle;
+
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+}
+
+static inline void psb_set_idle(struct psb_scheduler *scheduler)
+{
+	scheduler->idle =
+	    (scheduler->current_task[PSB_SCENE_ENGINE_RASTER] == NULL) &&
+	    (scheduler->current_task[PSB_SCENE_ENGINE_TA] == NULL);
+	if (scheduler->idle)
+		wake_up(&scheduler->idle_queue);
+}
+
+/*
+ * Call with the scheduler spinlock held.
+ * Assigns a scene context to either the ta or the rasterizer,
+ * flushing out other scenes to memory if necessary.
+ */
+
+static int psb_set_scene_fire(struct psb_scheduler *scheduler,
+			      struct psb_scene *scene,
+			      int engine, struct psb_task *task)
+{
+	uint32_t flags = 0;
+	struct psb_hw_scene *hw_scene;
+	struct drm_device *dev = scene->dev;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	hw_scene = scene->hw_scene;
+	if (hw_scene && hw_scene->last_scene == scene) {
+
+		/*
+		 * Reuse the last hw scene context and delete it from the
+		 * free list.
+		 */
+
+		PSB_DEBUG_RENDER("Reusing hw scene %d.\n",
+				 hw_scene->context_number);
+		if (scene->flags & PSB_SCENE_FLAG_DIRTY) {
+
+			/*
+			 * No hw context initialization to be done.
+			 */
+
+			flags |= PSB_SCENE_FLAG_SETUP_ONLY;
+		}
+
+		list_del_init(&hw_scene->head);
+
+	} else {
+		struct list_head *list;
+		hw_scene = NULL;
+
+		/*
+		 * Grab a new hw scene context.
+		 */
+
+		list_for_each(list, &scheduler->hw_scenes) {
+			hw_scene = list_entry(list, struct psb_hw_scene, head);
+			break;
+		}
+		BUG_ON(!hw_scene);
+		PSB_DEBUG_RENDER("New hw scene %d.\n",
+				 hw_scene->context_number);
+
+		list_del_init(list);
+	}
+	scene->hw_scene = hw_scene;
+	hw_scene->last_scene = scene;
+
+	flags |= PSB_SCENE_FLAG_SETUP;
+
+	/*
+	 * Switch context and setup the engine.
+	 */
+
+	return psb_xhw_scene_bind_fire(dev_priv,
+				       &task->buf,
+				       task->flags,
+				       hw_scene->context_number,
+				       scene->hw_cookie,
+				       task->oom_cmds,
+				       task->oom_cmd_size,
+				       scene->hw_data->offset,
+				       engine, flags | scene->flags);
+}
+
+static inline void psb_report_fence(struct psb_scheduler *scheduler,
+				    uint32_t class,
+				    uint32_t sequence,
+				    uint32_t type, int call_handler)
+{
+	struct psb_scheduler_seq *seq = &scheduler->seq[type];
+
+	seq->sequence = sequence;
+	seq->reported = 0;
+	if (call_handler)
+		psb_fence_handler(scheduler->dev, class);
+}
+
+static void psb_schedule_raster(struct drm_psb_private *dev_priv,
+				struct psb_scheduler *scheduler);
+
+static void psb_schedule_ta(struct drm_psb_private *dev_priv,
+			    struct psb_scheduler *scheduler)
+{
+	struct psb_task *task = NULL;
+	struct list_head *list, *next;
+	int pushed_raster_task = 0;
+
+	PSB_DEBUG_RENDER("schedule ta\n");
+
+	if (scheduler->idle_count != 0)
+		return;
+
+	if (scheduler->current_task[PSB_SCENE_ENGINE_TA] != NULL)
+		return;
+
+	if (scheduler->ta_state)
+		return;
+
+	/*
+	 * Skip the ta stage for rasterization-only
+	 * tasks. They arrive here to make sure we're rasterizing
+	 * tasks in the correct order.
+	 */
+
+	list_for_each_safe(list, next, &scheduler->ta_queue) {
+		task = list_entry(list, struct psb_task, head);
+		if (task->task_type != psb_raster_task)
+			break;
+
+		list_del_init(list);
+		list_add_tail(list, &scheduler->raster_queue);
+		psb_report_fence(scheduler, task->engine, task->sequence,
+				 _PSB_FENCE_TA_DONE_SHIFT, 1);
+		task = NULL;
+		pushed_raster_task = 1;
+	}
+
+	if (pushed_raster_task)
+		psb_schedule_raster(dev_priv, scheduler);
+
+	if (!task)
+		return;
+
+	/*
+	 * Still waiting for a vistest?
+	 */
+
+	if (scheduler->feedback_task == task)
+		return;
+
+#ifdef ONLY_ONE_JOB_IN_RASTER_QUEUE
+
+	/*
+	 * Block ta from trying to use both hardware contexts
+	 * without the rasterizer starting to render from one of them.
+	 */
+
+	if (!list_empty(&scheduler->raster_queue)) {
+		return;
+	}
+#endif
+
+#ifdef PSB_BLOCK_OVERLAP
+	/*
+	 * Make sure rasterizer isn't doing anything.
+	 */
+	if (scheduler->current_task[PSB_SCENE_ENGINE_RASTER] != NULL)
+		return;
+#endif
+	if (list_empty(&scheduler->hw_scenes))
+		return;
+
+#ifdef FIX_TG_16
+	if (psb_check_2d_idle(dev_priv))
+		return;
+#endif
+
+	list_del_init(&task->head);
+	if (task->flags & PSB_FIRE_FLAG_XHW_OOM)
+		scheduler->ta_state = 1;
+
+	scheduler->current_task[PSB_SCENE_ENGINE_TA] = task;
+	scheduler->idle = 0;
+	scheduler->ta_end_jiffies = jiffies + PSB_TA_TIMEOUT;
+
+	task->reply_flags = (task->flags & PSB_FIRE_FLAG_XHW_OOM) ?
+	    0x00000000 : PSB_RF_FIRE_TA;
+
+	(void)psb_reg_submit(dev_priv, task->ta_cmds, task->ta_cmd_size);
+	psb_set_scene_fire(scheduler, task->scene, PSB_SCENE_ENGINE_TA, task);
+	psb_schedule_watchdog(dev_priv);
+}
+
+static int psb_fire_raster(struct psb_scheduler *scheduler,
+			   struct psb_task *task)
+{
+	struct drm_device *dev = scheduler->dev;
+	struct drm_psb_private *dev_priv = (struct drm_psb_private *)
+	    dev->dev_private;
+
+	PSB_DEBUG_RENDER("Fire raster %d\n", task->sequence);
+
+	return psb_xhw_fire_raster(dev_priv, &task->buf, task->flags);
+}
+
+/*
+ * Take the first rasterization task from the hp raster queue or from the
+ * raster queue and fire the rasterizer.
+ */
+
+static void psb_schedule_raster(struct drm_psb_private *dev_priv,
+				struct psb_scheduler *scheduler)
+{
+	struct psb_task *task;
+	struct list_head *list;
+
+	if (scheduler->idle_count != 0)
+		return;
+
+	if (scheduler->current_task[PSB_SCENE_ENGINE_RASTER] != NULL) {
+		PSB_DEBUG_RENDER("Raster busy.\n");
+		return;
+	}
+#ifdef PSB_BLOCK_OVERLAP
+	if (scheduler->current_task[PSB_SCENE_ENGINE_TA] != NULL) {
+		PSB_DEBUG_RENDER("TA busy.\n");
+		return;
+	}
+#endif
+
+	if (!list_empty(&scheduler->hp_raster_queue))
+		list = scheduler->hp_raster_queue.next;
+	else if (!list_empty(&scheduler->raster_queue))
+		list = scheduler->raster_queue.next;
+	else {
+		PSB_DEBUG_RENDER("Nothing in list\n");
+		return;
+	}
+
+	task = list_entry(list, struct psb_task, head);
+
+	/*
+	 * Sometimes changing ZLS format requires an ISP reset.
+	 * Doesn't seem to consume too much time.
+	 */
+
+	if (task->scene)
+		PSB_WSGX32(_PSB_CS_RESET_ISP_RESET, PSB_CR_SOFT_RESET);
+
+	scheduler->current_task[PSB_SCENE_ENGINE_RASTER] = task;
+
+	list_del_init(list);
+	scheduler->idle = 0;
+	scheduler->raster_end_jiffies = jiffies + PSB_RASTER_TIMEOUT;
+	scheduler->total_raster_jiffies = 0;
+
+	if (task->scene)
+		PSB_WSGX32(0, PSB_CR_SOFT_RESET);
+
+	(void)psb_reg_submit(dev_priv, task->raster_cmds,
+			     task->raster_cmd_size);
+
+	if (task->scene) {
+		task->reply_flags = (task->flags & PSB_FIRE_FLAG_XHW_OOM) ?
+		    0x00000000 : PSB_RF_FIRE_RASTER;
+		psb_set_scene_fire(scheduler,
+				   task->scene, PSB_SCENE_ENGINE_RASTER, task);
+	} else {
+		task->reply_flags = PSB_RF_DEALLOC | PSB_RF_FIRE_RASTER;
+		psb_fire_raster(scheduler, task);
+	}
+	psb_schedule_watchdog(dev_priv);
+}
+
+int psb_extend_raster_timeout(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+	int ret;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	scheduler->total_raster_jiffies +=
+	    jiffies - scheduler->raster_end_jiffies + PSB_RASTER_TIMEOUT;
+	scheduler->raster_end_jiffies = jiffies + PSB_RASTER_TIMEOUT;
+	ret = (scheduler->total_raster_jiffies > PSB_ALLOWED_RASTER_RUNTIME) ?
+	    -EBUSY : 0;
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+	return ret;
+}
+
+/*
+ * TA done handler.
+ */
+
+static void psb_ta_done(struct drm_psb_private *dev_priv,
+			struct psb_scheduler *scheduler)
+{
+	struct psb_task *task = scheduler->current_task[PSB_SCENE_ENGINE_TA];
+	struct psb_scene *scene = task->scene;
+
+	PSB_DEBUG_RENDER("TA done %u\n", task->sequence);
+
+	switch (task->ta_complete_action) {
+	case PSB_RASTER_BLOCK:
+		scheduler->ta_state = 1;
+		scene->flags |=
+		    (PSB_SCENE_FLAG_DIRTY | PSB_SCENE_FLAG_COMPLETE);
+		list_add_tail(&task->head, &scheduler->raster_queue);
+		break;
+	case PSB_RASTER:
+		scene->flags |=
+		    (PSB_SCENE_FLAG_DIRTY | PSB_SCENE_FLAG_COMPLETE);
+		list_add_tail(&task->head, &scheduler->raster_queue);
+		break;
+	case PSB_RETURN:
+		scheduler->ta_state = 0;
+		scene->flags |= PSB_SCENE_FLAG_DIRTY;
+		list_add_tail(&scene->hw_scene->head, &scheduler->hw_scenes);
+
+		break;
+	}
+
+	scheduler->current_task[PSB_SCENE_ENGINE_TA] = NULL;
+
+#ifdef FIX_TG_16
+	psb_2d_atomic_unlock(dev_priv);
+#endif
+
+	if (task->ta_complete_action != PSB_RASTER_BLOCK)
+		psb_report_fence(scheduler, task->engine, task->sequence,
+				 _PSB_FENCE_TA_DONE_SHIFT, 1);
+
+	psb_schedule_raster(dev_priv, scheduler);
+	psb_schedule_ta(dev_priv, scheduler);
+	psb_set_idle(scheduler);
+
+	if (task->ta_complete_action != PSB_RETURN)
+		return;
+
+	list_add_tail(&task->head, &scheduler->task_done_queue);
+	schedule_delayed_work(&scheduler->wq, 1);
+}
+
+/*
+ * Rasterizer done handler.
+ */
+
+static void psb_raster_done(struct drm_psb_private *dev_priv,
+			    struct psb_scheduler *scheduler)
+{
+	struct psb_task *task =
+	    scheduler->current_task[PSB_SCENE_ENGINE_RASTER];
+	struct psb_scene *scene = task->scene;
+	uint32_t complete_action = task->raster_complete_action;
+
+	PSB_DEBUG_RENDER("Raster done %u\n", task->sequence);
+
+	scheduler->current_task[PSB_SCENE_ENGINE_RASTER] = NULL;
+
+	if (complete_action != PSB_RASTER)
+		psb_schedule_raster(dev_priv, scheduler);
+
+	if (scene) {
+		if (task->feedback.page) {
+			if (unlikely(scheduler->feedback_task)) {
+				/*
+				 * This should never happen, since the previous
+				 * feedback query will return before the next
+				 * raster task is fired.
+				 */
+				DRM_ERROR("Feedback task busy.\n");
+			}
+			scheduler->feedback_task = task;
+			psb_xhw_vistest(dev_priv, &task->buf);
+		}
+		switch (complete_action) {
+		case PSB_RETURN:
+			scene->flags &=
+			    ~(PSB_SCENE_FLAG_DIRTY | PSB_SCENE_FLAG_COMPLETE);
+			list_add_tail(&scene->hw_scene->head,
+				      &scheduler->hw_scenes);
+			psb_report_fence(scheduler, task->engine,
+					 task->sequence,
+					 _PSB_FENCE_SCENE_DONE_SHIFT, 1);
+			if (task->flags & PSB_FIRE_FLAG_XHW_OOM) {
+				scheduler->ta_state = 0;
+			}
+			break;
+		case PSB_RASTER:
+			list_add(&task->head, &scheduler->raster_queue);
+			task->raster_complete_action = PSB_RETURN;
+			psb_schedule_raster(dev_priv, scheduler);
+			break;
+		case PSB_TA:
+			list_add(&task->head, &scheduler->ta_queue);
+			scheduler->ta_state = 0;
+			task->raster_complete_action = PSB_RETURN;
+			task->ta_complete_action = PSB_RASTER;
+			break;
+
+		}
+	}
+	psb_schedule_ta(dev_priv, scheduler);
+	psb_set_idle(scheduler);
+
+	if (complete_action == PSB_RETURN) {
+		if (task->scene == NULL) {
+			psb_report_fence(scheduler, task->engine,
+					 task->sequence,
+					 _PSB_FENCE_RASTER_DONE_SHIFT, 1);
+		}
+		if (!task->feedback.page) {
+			list_add_tail(&task->head, &scheduler->task_done_queue);
+			schedule_delayed_work(&scheduler->wq, 1);
+		}
+	}
+}
+
+void psb_scheduler_pause(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	scheduler->idle_count++;
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+}
+
+void psb_scheduler_restart(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	if (--scheduler->idle_count == 0) {
+		psb_schedule_ta(dev_priv, scheduler);
+		psb_schedule_raster(dev_priv, scheduler);
+	}
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+}
+
+int psb_scheduler_idle(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+	int ret;
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	ret = scheduler->idle_count != 0 && scheduler->idle;
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+	return ret;
+}
+
+int psb_scheduler_finished(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+	int ret;
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	ret = (scheduler->idle &&
+	       list_empty(&scheduler->raster_queue) &&
+	       list_empty(&scheduler->ta_queue) &&
+	       list_empty(&scheduler->hp_raster_queue));
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+	return ret;
+}
+
+static void psb_ta_oom(struct drm_psb_private *dev_priv,
+		       struct psb_scheduler *scheduler)
+{
+
+	struct psb_task *task = scheduler->current_task[PSB_SCENE_ENGINE_TA];
+	if (!task)
+		return;
+
+	if (task->aborting)
+		return;
+	task->aborting = 1;
+
+	DRM_INFO("Info: TA out of parameter memory.\n");
+
+	(void)psb_xhw_ta_oom(dev_priv, &task->buf, task->scene->hw_cookie);
+}
+
+static void psb_ta_oom_reply(struct drm_psb_private *dev_priv,
+			     struct psb_scheduler *scheduler)
+{
+
+	struct psb_task *task = scheduler->current_task[PSB_SCENE_ENGINE_TA];
+	uint32_t flags;
+	if (!task)
+		return;
+
+	psb_xhw_ta_oom_reply(dev_priv, &task->buf,
+			     task->scene->hw_cookie,
+			     &task->ta_complete_action,
+			     &task->raster_complete_action, &flags);
+	task->flags |= flags;
+	task->aborting = 0;
+	psb_dispatch_ta(dev_priv, scheduler, PSB_RF_OOM_REPLY);
+}
+
+static void psb_ta_hw_scene_freed(struct drm_psb_private *dev_priv,
+				  struct psb_scheduler *scheduler)
+{
+	DRM_ERROR("TA hw scene freed.\n");
+}
+
+static void psb_vistest_reply(struct drm_psb_private *dev_priv,
+			      struct psb_scheduler *scheduler)
+{
+	struct psb_task *task = scheduler->feedback_task;
+	uint8_t *feedback_map;
+	uint32_t add;
+	uint32_t cur;
+	struct drm_psb_vistest *vistest;
+	int i;
+
+	scheduler->feedback_task = NULL;
+	if (!task) {
+		DRM_ERROR("No Poulsbo feedback task.\n");
+		return;
+	}
+	if (!task->feedback.page) {
+		DRM_ERROR("No Poulsbo feedback page.\n");
+		goto out;
+	}
+
+	if (in_irq())
+		feedback_map = kmap_atomic(task->feedback.page, KM_IRQ0);
+	else
+		feedback_map = kmap_atomic(task->feedback.page, KM_USER0);
+
+	/*
+	 * Loop over all requested vistest components here.
+	 * Only one (vistest) currently.
+	 */
+
+	vistest = (struct drm_psb_vistest *)
+	    (feedback_map + task->feedback.offset);
+
+	for (i = 0; i < PSB_HW_FEEDBACK_SIZE; ++i) {
+		add = task->buf.arg.arg.feedback[i];
+		cur = vistest->vt[i];
+
+		/*
+		 * Vistest saturates.
+		 */
+
+		vistest->vt[i] = (cur + add < cur) ? ~0 : cur + add;
+	}
+	if (in_irq())
+		kunmap_atomic(feedback_map, KM_IRQ0);
+	else
+		kunmap_atomic(feedback_map, KM_USER0);
+      out:
+	psb_report_fence(scheduler, task->engine, task->sequence,
+			 _PSB_FENCE_FEEDBACK_SHIFT, 1);
+
+	if (list_empty(&task->head)) {
+		list_add_tail(&task->head, &scheduler->task_done_queue);
+		schedule_delayed_work(&scheduler->wq, 1);
+	} else
+		psb_schedule_ta(dev_priv, scheduler);
+}
+
+static void psb_ta_fire_reply(struct drm_psb_private *dev_priv,
+			      struct psb_scheduler *scheduler)
+{
+	struct psb_task *task = scheduler->current_task[PSB_SCENE_ENGINE_TA];
+
+	psb_xhw_fire_reply(dev_priv, &task->buf, task->scene->hw_cookie);
+
+	psb_dispatch_ta(dev_priv, scheduler, PSB_RF_FIRE_TA);
+}
+
+static void psb_raster_fire_reply(struct drm_psb_private *dev_priv,
+				  struct psb_scheduler *scheduler)
+{
+	struct psb_task *task =
+	    scheduler->current_task[PSB_SCENE_ENGINE_RASTER];
+	uint32_t reply_flags;
+
+	if (!task) {
+		DRM_ERROR("Null task.\n");
+		return;
+	}
+
+	task->raster_complete_action = task->buf.arg.arg.sb.rca;
+	psb_xhw_fire_reply(dev_priv, &task->buf, task->scene->hw_cookie);
+
+	reply_flags = PSB_RF_FIRE_RASTER;
+	if (task->raster_complete_action == PSB_RASTER)
+		reply_flags |= PSB_RF_DEALLOC;
+
+	psb_dispatch_raster(dev_priv, scheduler, reply_flags);
+}
+
+static int psb_user_interrupt(struct drm_psb_private *dev_priv,
+			      struct psb_scheduler *scheduler)
+{
+	uint32_t type;
+	int ret;
+	unsigned long irq_flags;
+
+	/*
+	 * Xhw cannot write directly to the comm page, so
+	 * do it here. Firmware would have written directly.
+	 */
+
+	ret = psb_xhw_handler(dev_priv);
+	if (unlikely(ret))
+		return ret;
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	type = dev_priv->comm[PSB_COMM_USER_IRQ];
+	dev_priv->comm[PSB_COMM_USER_IRQ] = 0;
+	if (dev_priv->comm[PSB_COMM_USER_IRQ_LOST]) {
+		dev_priv->comm[PSB_COMM_USER_IRQ_LOST] = 0;
+		DRM_ERROR("Lost Poulsbo hardware event.\n");
+	}
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+
+	if (type == 0)
+		return 0;
+
+	switch (type) {
+	case PSB_UIRQ_VISTEST:
+		psb_vistest_reply(dev_priv, scheduler);
+		break;
+	case PSB_UIRQ_OOM_REPLY:
+		psb_ta_oom_reply(dev_priv, scheduler);
+		break;
+	case PSB_UIRQ_FIRE_TA_REPLY:
+		psb_ta_fire_reply(dev_priv, scheduler);
+		break;
+	case PSB_UIRQ_FIRE_RASTER_REPLY:
+		psb_raster_fire_reply(dev_priv, scheduler);
+		break;
+	default:
+		DRM_ERROR("Unknown Poulsbo hardware event. %d\n", type);
+	}
+	return 0;
+}
+
+int psb_forced_user_interrupt(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+	int ret;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	ret = psb_user_interrupt(dev_priv, scheduler);
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+	return ret;
+}
+
+static void psb_dispatch_ta(struct drm_psb_private *dev_priv,
+			    struct psb_scheduler *scheduler,
+			    uint32_t reply_flag)
+{
+	struct psb_task *task = scheduler->current_task[PSB_SCENE_ENGINE_TA];
+	uint32_t flags;
+	uint32_t mask;
+
+	task->reply_flags |= reply_flag;
+	flags = task->reply_flags;
+	mask = PSB_RF_FIRE_TA;
+
+	if (!(flags & mask))
+		return;
+
+	mask = PSB_RF_TA_DONE;
+	if ((flags & mask) == mask) {
+		task->reply_flags &= ~mask;
+		psb_ta_done(dev_priv, scheduler);
+	}
+
+	mask = PSB_RF_OOM;
+	if ((flags & mask) == mask) {
+		task->reply_flags &= ~mask;
+		psb_ta_oom(dev_priv, scheduler);
+	}
+
+	mask = (PSB_RF_OOM_REPLY | PSB_RF_TERMINATE);
+	if ((flags & mask) == mask) {
+		task->reply_flags &= ~mask;
+		psb_ta_done(dev_priv, scheduler);
+	}
+}
+
+static void psb_dispatch_raster(struct drm_psb_private *dev_priv,
+				struct psb_scheduler *scheduler,
+				uint32_t reply_flag)
+{
+	struct psb_task *task =
+	    scheduler->current_task[PSB_SCENE_ENGINE_RASTER];
+	uint32_t flags;
+	uint32_t mask;
+
+	task->reply_flags |= reply_flag;
+	flags = task->reply_flags;
+	mask = PSB_RF_FIRE_RASTER;
+
+	if (!(flags & mask))
+		return;
+
+	/*
+	 * For rasterizer-only tasks, don't report fence done here,
+	 * as this is time consuming and the rasterizer wants a new
+	 * task immediately. For other tasks, the hardware is probably
+	 * still busy deallocating TA memory, so we can report
+	 * fence done in parallel.
+	 */
+
+	if (task->raster_complete_action == PSB_RETURN &&
+	    (reply_flag & PSB_RF_RASTER_DONE) && task->scene != NULL) {
+		psb_report_fence(scheduler, task->engine, task->sequence,
+				 _PSB_FENCE_RASTER_DONE_SHIFT, 1);
+	}
+
+	mask = PSB_RF_RASTER_DONE | PSB_RF_DEALLOC;
+	if ((flags & mask) == mask) {
+		task->reply_flags &= ~mask;
+		psb_raster_done(dev_priv, scheduler);
+	}
+}
+
+void psb_scheduler_handler(struct drm_psb_private *dev_priv, uint32_t status)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+
+	spin_lock(&scheduler->lock);
+
+	if (status & _PSB_CE_PIXELBE_END_RENDER) {
+		psb_dispatch_raster(dev_priv, scheduler, PSB_RF_RASTER_DONE);
+	}
+	if (status & _PSB_CE_DPM_3D_MEM_FREE) {
+		psb_dispatch_raster(dev_priv, scheduler, PSB_RF_DEALLOC);
+	}
+	if (status & _PSB_CE_TA_FINISHED) {
+		psb_dispatch_ta(dev_priv, scheduler, PSB_RF_TA_DONE);
+	}
+	if (status & _PSB_CE_TA_TERMINATE) {
+		psb_dispatch_ta(dev_priv, scheduler, PSB_RF_TERMINATE);
+	}
+	if (status & (_PSB_CE_DPM_REACHED_MEM_THRESH |
+		      _PSB_CE_DPM_OUT_OF_MEMORY_GBL |
+		      _PSB_CE_DPM_OUT_OF_MEMORY_MT)) {
+		psb_dispatch_ta(dev_priv, scheduler, PSB_RF_OOM);
+	}
+	if (status & _PSB_CE_DPM_TA_MEM_FREE) {
+		psb_ta_hw_scene_freed(dev_priv, scheduler);
+	}
+	if (status & _PSB_CE_SW_EVENT) {
+		psb_user_interrupt(dev_priv, scheduler);
+	}
+	spin_unlock(&scheduler->lock);
+}
+
+static void psb_free_task_wq(struct work_struct *work)
+{
+	struct psb_scheduler *scheduler =
+	    container_of(work, struct psb_scheduler, wq.work);
+
+	struct drm_device *dev = scheduler->dev;
+	struct list_head *list, *next;
+	unsigned long irq_flags;
+	struct psb_task *task;
+
+	if (!mutex_trylock(&scheduler->task_wq_mutex))
+		return;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	list_for_each_safe(list, next, &scheduler->task_done_queue) {
+		task = list_entry(list, struct psb_task, head);
+		list_del_init(list);
+		spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+
+		PSB_DEBUG_RENDER("Checking Task %d: Scene 0x%08lx, "
+				 "Feedback bo 0x%08lx, done %d\n",
+				 task->sequence, (unsigned long)task->scene,
+				 (unsigned long)task->feedback.bo,
+				 atomic_read(&task->buf.done));
+
+		if (task->scene) {
+			mutex_lock(&dev->struct_mutex);
+			PSB_DEBUG_RENDER("Unref scene %d\n", task->sequence);
+			psb_scene_unref_devlocked(&task->scene);
+			if (task->feedback.bo) {
+				PSB_DEBUG_RENDER("Unref feedback bo %d\n",
+						 task->sequence);
+				drm_bo_usage_deref_locked(&task->feedback.bo);
+			}
+			mutex_unlock(&dev->struct_mutex);
+		}
+
+		if (atomic_read(&task->buf.done)) {
+			PSB_DEBUG_RENDER("Deleting task %d\n", task->sequence);
+			drm_free(task, sizeof(*task), DRM_MEM_DRIVER);
+			task = NULL;
+		}
+		spin_lock_irqsave(&scheduler->lock, irq_flags);
+		if (task != NULL)
+			list_add(list, &scheduler->task_done_queue);
+	}
+	if (!list_empty(&scheduler->task_done_queue)) {
+		PSB_DEBUG_RENDER("Rescheduling wq\n");
+		schedule_delayed_work(&scheduler->wq, 1);
+	}
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+
+	mutex_unlock(&scheduler->task_wq_mutex);
+}
+
+/*
+ * Check if any of the tasks in the queues is using a scene.
+ * In that case we know the TA memory buffer objects are
+ * fenced and will not be evicted until that fence is signaled.
+ */
+
+void psb_scheduler_ta_mem_check(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+	struct psb_task *task;
+	struct psb_task *next_task;
+
+	dev_priv->force_ta_mem_load = 1;
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	list_for_each_entry_safe(task, next_task, &scheduler->ta_queue, head) {
+		if (task->scene) {
+			dev_priv->force_ta_mem_load = 0;
+			break;
+		}
+	}
+	list_for_each_entry_safe(task, next_task, &scheduler->raster_queue,
+				 head) {
+		if (task->scene) {
+			dev_priv->force_ta_mem_load = 0;
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+}
+
+void psb_scheduler_reset(struct drm_psb_private *dev_priv, int error_condition)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long wait_jiffies;
+	unsigned long cur_jiffies;
+	struct psb_task *task;
+	struct psb_task *next_task;
+	unsigned long irq_flags;
+
+	psb_scheduler_pause(dev_priv);
+	if (!psb_scheduler_idle(dev_priv)) {
+		spin_lock_irqsave(&scheduler->lock, irq_flags);
+
+		cur_jiffies = jiffies;
+		wait_jiffies = cur_jiffies;
+		if (scheduler->current_task[PSB_SCENE_ENGINE_TA] &&
+		    time_after_eq(scheduler->ta_end_jiffies, wait_jiffies))
+			wait_jiffies = scheduler->ta_end_jiffies;
+		if (scheduler->current_task[PSB_SCENE_ENGINE_RASTER] &&
+		    time_after_eq(scheduler->raster_end_jiffies, wait_jiffies))
+			wait_jiffies = scheduler->raster_end_jiffies;
+
+		wait_jiffies -= cur_jiffies;
+		spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+
+		(void)wait_event_timeout(scheduler->idle_queue,
+					 psb_scheduler_idle(dev_priv),
+					 wait_jiffies);
+	}
+
+	if (!psb_scheduler_idle(dev_priv)) {
+		spin_lock_irqsave(&scheduler->lock, irq_flags);
+		task = scheduler->current_task[PSB_SCENE_ENGINE_RASTER];
+		if (task) {
+			DRM_ERROR("Detected Poulsbo rasterizer lockup.\n");
+			if (task->engine == PSB_ENGINE_HPRAST) {
+				psb_fence_error(scheduler->dev,
+						PSB_ENGINE_HPRAST,
+						task->sequence,
+						_PSB_FENCE_TYPE_RASTER_DONE,
+						error_condition);
+
+				list_del(&task->head);
+				psb_xhw_clean_buf(dev_priv, &task->buf);
+				list_add_tail(&task->head,
+					      &scheduler->task_done_queue);
+			} else {
+				list_add(&task->head, &scheduler->raster_queue);
+			}
+		}
+		scheduler->current_task[PSB_SCENE_ENGINE_RASTER] = NULL;
+		task = scheduler->current_task[PSB_SCENE_ENGINE_TA];
+		if (task) {
+			DRM_ERROR("Detected Poulsbo ta lockup.\n");
+			list_add_tail(&task->head, &scheduler->raster_queue);
+#ifdef FIX_TG_16
+			psb_2d_atomic_unlock(dev_priv);
+#endif
+		}
+		scheduler->current_task[PSB_SCENE_ENGINE_TA] = NULL;
+		scheduler->ta_state = 0;
+
+#ifdef FIX_TG_16
+		atomic_set(&dev_priv->ta_wait_2d, 0);
+		atomic_set(&dev_priv->ta_wait_2d_irq, 0);
+		wake_up(&dev_priv->queue_2d);
+#endif
+		spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+	}
+
+	/*
+	 * Empty raster queue.
+	 */
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	list_for_each_entry_safe(task, next_task, &scheduler->raster_queue,
+				 head) {
+		struct psb_scene *scene = task->scene;
+
+		psb_fence_error(scheduler->dev,
+				task->engine,
+				task->sequence,
+				_PSB_FENCE_TYPE_TA_DONE |
+				_PSB_FENCE_TYPE_RASTER_DONE |
+				_PSB_FENCE_TYPE_SCENE_DONE |
+				_PSB_FENCE_TYPE_FEEDBACK, error_condition);
+		if (scene) {
+			scene->flags = 0;
+			if (scene->hw_scene) {
+				list_add_tail(&scene->hw_scene->head,
+					      &scheduler->hw_scenes);
+				scene->hw_scene = NULL;
+			}
+		}
+
+		psb_xhw_clean_buf(dev_priv, &task->buf);
+		list_del(&task->head);
+		list_add_tail(&task->head, &scheduler->task_done_queue);
+	}
+
+	schedule_delayed_work(&scheduler->wq, 1);
+	scheduler->idle = 1;
+	wake_up(&scheduler->idle_queue);
+
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+	psb_scheduler_restart(dev_priv);
+
+}
+
+int psb_scheduler_init(struct drm_device *dev, struct psb_scheduler *scheduler)
+{
+	struct psb_hw_scene *hw_scene;
+	int i;
+
+	memset(scheduler, 0, sizeof(*scheduler));
+	scheduler->dev = dev;
+	mutex_init(&scheduler->task_wq_mutex);
+	scheduler->lock = SPIN_LOCK_UNLOCKED;
+	scheduler->idle = 1;
+
+	INIT_LIST_HEAD(&scheduler->ta_queue);
+	INIT_LIST_HEAD(&scheduler->raster_queue);
+	INIT_LIST_HEAD(&scheduler->hp_raster_queue);
+	INIT_LIST_HEAD(&scheduler->hw_scenes);
+	INIT_LIST_HEAD(&scheduler->task_done_queue);
+	INIT_DELAYED_WORK(&scheduler->wq, &psb_free_task_wq);
+	init_waitqueue_head(&scheduler->idle_queue);
+
+	for (i = 0; i < PSB_NUM_HW_SCENES; ++i) {
+		hw_scene = &scheduler->hs[i];
+		hw_scene->context_number = i;
+		list_add_tail(&hw_scene->head, &scheduler->hw_scenes);
+	}
+
+	for (i = 0; i < _PSB_ENGINE_TA_FENCE_TYPES; ++i) {
+		scheduler->seq[i].reported = 0;
+	}
+
+	return 0;
+}
+
+/*
+ * Scene references maintained by the scheduler are not refcounted.
+ * Remove all references to a particular scene here.
+ */
+
+void psb_scheduler_remove_scene_refs(struct psb_scene *scene)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)scene->dev->dev_private;
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	struct psb_hw_scene *hw_scene;
+	unsigned long irq_flags;
+	unsigned int i;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	for (i = 0; i < PSB_NUM_HW_SCENES; ++i) {
+		hw_scene = &scheduler->hs[i];
+		if (hw_scene->last_scene == scene) {
+			BUG_ON(list_empty(&hw_scene->head));
+			hw_scene->last_scene = NULL;
+		}
+	}
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+}
+
+void psb_scheduler_takedown(struct psb_scheduler *scheduler)
+{
+	flush_scheduled_work();
+}
+
+static int psb_setup_task_devlocked(struct drm_device *dev,
+				    struct drm_psb_cmdbuf_arg *arg,
+				    struct drm_buffer_object *raster_cmd_buffer,
+				    struct drm_buffer_object *ta_cmd_buffer,
+				    struct drm_buffer_object *oom_cmd_buffer,
+				    struct psb_scene *scene,
+				    enum psb_task_type task_type,
+				    uint32_t engine,
+				    uint32_t flags, struct psb_task **task_p)
+{
+	struct psb_task *task;
+	int ret;
+
+	if (ta_cmd_buffer && arg->ta_size > PSB_MAX_TA_CMDS) {
+		DRM_ERROR("Too many ta cmds %d.\n", arg->ta_size);
+		return -EINVAL;
+	}
+	if (raster_cmd_buffer && arg->cmdbuf_size > PSB_MAX_RASTER_CMDS) {
+		DRM_ERROR("Too many raster cmds %d.\n", arg->cmdbuf_size);
+		return -EINVAL;
+	}
+	if (oom_cmd_buffer && arg->oom_size > PSB_MAX_OOM_CMDS) {
+		DRM_ERROR("Too many raster cmds %d.\n", arg->oom_size);
+		return -EINVAL;
+	}
+
+	task = drm_calloc(1, sizeof(*task), DRM_MEM_DRIVER);
+	if (!task)
+		return -ENOMEM;
+
+	atomic_set(&task->buf.done, 1);
+	task->engine = engine;
+	INIT_LIST_HEAD(&task->head);
+	INIT_LIST_HEAD(&task->buf.head);
+	if (ta_cmd_buffer && arg->ta_size != 0) {
+		task->ta_cmd_size = arg->ta_size;
+		ret = psb_submit_copy_cmdbuf(dev, ta_cmd_buffer,
+					     arg->ta_offset,
+					     arg->ta_size,
+					     PSB_ENGINE_TA, task->ta_cmds);
+		if (ret)
+			goto out_err;
+	}
+	if (raster_cmd_buffer) {
+		task->raster_cmd_size = arg->cmdbuf_size;
+		ret = psb_submit_copy_cmdbuf(dev, raster_cmd_buffer,
+					     arg->cmdbuf_offset,
+					     arg->cmdbuf_size,
+					     PSB_ENGINE_TA, task->raster_cmds);
+		if (ret)
+			goto out_err;
+	}
+	if (oom_cmd_buffer && arg->oom_size != 0) {
+		task->oom_cmd_size = arg->oom_size;
+		ret = psb_submit_copy_cmdbuf(dev, oom_cmd_buffer,
+					     arg->oom_offset,
+					     arg->oom_size,
+					     PSB_ENGINE_TA, task->oom_cmds);
+		if (ret)
+			goto out_err;
+	}
+	task->task_type = task_type;
+	task->flags = flags;
+	if (scene)
+		task->scene = psb_scene_ref(scene);
+
+	*task_p = task;
+	return 0;
+      out_err:
+	drm_free(task, sizeof(*task), DRM_MEM_DRIVER);
+	*task_p = NULL;
+	return ret;
+}
+
+int psb_cmdbuf_ta(struct drm_file *priv,
+		  struct drm_psb_cmdbuf_arg *arg,
+		  struct drm_buffer_object *cmd_buffer,
+		  struct drm_buffer_object *ta_buffer,
+		  struct drm_buffer_object *oom_buffer,
+		  struct psb_scene *scene,
+		  struct psb_feedback_info *feedback,
+		  struct drm_fence_arg *fence_arg)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	struct drm_fence_object *fence = NULL;
+	struct psb_task *task = NULL;
+	int ret;
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+
+	PSB_DEBUG_RENDER("Cmdbuf ta\n");
+
+	ret = mutex_lock_interruptible(&dev_priv->reset_mutex);
+	if (ret)
+		return -EAGAIN;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = psb_setup_task_devlocked(dev, arg, cmd_buffer, ta_buffer,
+				       oom_buffer, scene,
+				       psb_ta_task, PSB_ENGINE_TA,
+				       PSB_FIRE_FLAG_RASTER_DEALLOC, &task);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (ret)
+		goto out_err;
+
+	task->feedback = *feedback;
+
+	/*
+	 * Hand the task over to the scheduler.
+	 */
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	task->sequence = psb_fence_advance_sequence(dev, PSB_ENGINE_TA);
+
+	psb_report_fence(scheduler, PSB_ENGINE_TA, task->sequence, 0, 1);
+
+	task->ta_complete_action = PSB_RASTER;
+	task->raster_complete_action = PSB_RETURN;
+
+	list_add_tail(&task->head, &scheduler->ta_queue);
+	PSB_DEBUG_RENDER("queued ta %u\n", task->sequence);
+
+	psb_schedule_ta(dev_priv, scheduler);
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+
+	psb_fence_or_sync(priv, PSB_ENGINE_TA, arg, fence_arg, &fence);
+	drm_regs_fence(&dev_priv->use_manager, fence);
+	if (fence)
+	  fence_arg->signaled |= 0x1;
+
+      out_err:
+	if (ret && ret != -EAGAIN)
+		DRM_ERROR("TA task queue job failed.\n");
+
+	if (fence) {
+#ifdef PSB_WAIT_FOR_TA_COMPLETION
+		drm_fence_object_wait(fence, 1, 1, DRM_FENCE_TYPE_EXE |
+				      _PSB_FENCE_TYPE_TA_DONE);
+#ifdef PSB_BE_PARANOID
+		drm_fence_object_wait(fence, 1, 1, DRM_FENCE_TYPE_EXE |
+				      _PSB_FENCE_TYPE_SCENE_DONE);
+#endif
+#endif
+		drm_fence_usage_deref_unlocked(&fence);
+	}
+	mutex_unlock(&dev_priv->reset_mutex);
+
+	return ret;
+}
+
+int psb_cmdbuf_raster(struct drm_file *priv,
+		      struct drm_psb_cmdbuf_arg *arg,
+		      struct drm_buffer_object *cmd_buffer,
+		      struct drm_fence_arg *fence_arg)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	struct drm_fence_object *fence = NULL;
+	struct psb_task *task = NULL;
+	int ret;
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+
+	PSB_DEBUG_RENDER("Cmdbuf Raster\n");
+
+	ret = mutex_lock_interruptible(&dev_priv->reset_mutex);
+	if (ret)
+		return -EAGAIN;
+
+	mutex_lock(&dev->struct_mutex);
+	ret = psb_setup_task_devlocked(dev, arg, cmd_buffer, NULL, NULL,
+				       NULL, psb_raster_task,
+				       PSB_ENGINE_TA, 0, &task);
+	mutex_unlock(&dev->struct_mutex);
+
+	if (ret)
+		goto out_err;
+
+	/*
+	 * Hand the task over to the scheduler.
+	 */
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	task->sequence = psb_fence_advance_sequence(dev, PSB_ENGINE_TA);
+	psb_report_fence(scheduler, PSB_ENGINE_TA, task->sequence, 0, 1);
+	task->ta_complete_action = PSB_RASTER;
+	task->raster_complete_action = PSB_RETURN;
+
+	list_add_tail(&task->head, &scheduler->ta_queue);
+	PSB_DEBUG_RENDER("queued raster %u\n", task->sequence);
+	psb_schedule_ta(dev_priv, scheduler);
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+
+	psb_fence_or_sync(priv, PSB_ENGINE_TA, arg, fence_arg, &fence);
+	drm_regs_fence(&dev_priv->use_manager, fence);
+	if (fence)
+	  fence_arg->signaled |= 0x1;
+      out_err:
+	if (ret && ret != -EAGAIN)
+		DRM_ERROR("Raster task queue job failed.\n");
+
+	if (fence) {
+#ifdef PSB_WAIT_FOR_RASTER_COMPLETION
+		drm_fence_object_wait(fence, 1, 1, fence->type);
+#endif
+		drm_fence_usage_deref_unlocked(&fence);
+	}
+
+	mutex_unlock(&dev_priv->reset_mutex);
+
+	return ret;
+}
+
+#ifdef FIX_TG_16
+
+static int psb_check_2d_idle(struct drm_psb_private *dev_priv)
+{
+	if (psb_2d_trylock(dev_priv)) {
+		if ((PSB_RSGX32(PSB_CR_2D_SOCIF) == _PSB_C2_SOCIF_EMPTY) &&
+		    !((PSB_RSGX32(PSB_CR_2D_BLIT_STATUS) &
+		       _PSB_C2B_STATUS_BUSY))) {
+			return 0;
+		}
+		if (atomic_cmpxchg(&dev_priv->ta_wait_2d_irq, 0, 1) == 0)
+			psb_2D_irq_on(dev_priv);
+
+		PSB_WSGX32(PSB_2D_FENCE_BH, PSB_SGX_2D_SLAVE_PORT);
+		PSB_WSGX32(PSB_2D_FLUSH_BH, PSB_SGX_2D_SLAVE_PORT);
+		(void)PSB_RSGX32(PSB_SGX_2D_SLAVE_PORT);
+
+		psb_2d_atomic_unlock(dev_priv);
+	}
+
+	atomic_set(&dev_priv->ta_wait_2d, 1);
+	return -EBUSY;
+}
+
+static void psb_atomic_resume_ta_2d_idle(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+
+	if (atomic_cmpxchg(&dev_priv->ta_wait_2d, 1, 0) == 1) {
+		psb_schedule_ta(dev_priv, scheduler);
+		if (atomic_read(&dev_priv->waiters_2d) != 0)
+			wake_up(&dev_priv->queue_2d);
+	}
+}
+
+void psb_resume_ta_2d_idle(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	if (atomic_cmpxchg(&dev_priv->ta_wait_2d_irq, 1, 0) == 1) {
+		atomic_set(&dev_priv->ta_wait_2d, 0);
+		psb_2D_irq_off(dev_priv);
+		psb_schedule_ta(dev_priv, scheduler);
+		if (atomic_read(&dev_priv->waiters_2d) != 0)
+			wake_up(&dev_priv->queue_2d);
+	}
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+}
+
+/*
+ * 2D locking functions. Can't use a mutex since the trylock() and
+ * unlock() methods need to be accessible from interrupt context.
+ */
+
+static int psb_2d_trylock(struct drm_psb_private *dev_priv)
+{
+	return (atomic_cmpxchg(&dev_priv->lock_2d, 0, 1) == 0);
+}
+
+static void psb_2d_atomic_unlock(struct drm_psb_private *dev_priv)
+{
+	atomic_set(&dev_priv->lock_2d, 0);
+	if (atomic_read(&dev_priv->waiters_2d) != 0)
+		wake_up(&dev_priv->queue_2d);
+}
+
+void psb_2d_unlock(struct drm_psb_private *dev_priv)
+{
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&scheduler->lock, irq_flags);
+	psb_2d_atomic_unlock(dev_priv);
+	if (atomic_read(&dev_priv->ta_wait_2d) != 0)
+		psb_atomic_resume_ta_2d_idle(dev_priv);
+	spin_unlock_irqrestore(&scheduler->lock, irq_flags);
+}
+
+void psb_2d_lock(struct drm_psb_private *dev_priv)
+{
+	atomic_inc(&dev_priv->waiters_2d);
+	wait_event(dev_priv->queue_2d, atomic_read(&dev_priv->ta_wait_2d) == 0);
+	wait_event(dev_priv->queue_2d, psb_2d_trylock(dev_priv));
+	atomic_dec(&dev_priv->waiters_2d);
+}
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_schedule.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_schedule.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,170 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellstrom <thomas-at-tungstengraphics.com>
+ */
+
+#ifndef _PSB_SCHEDULE_H_
+#define _PSB_SCHEDULE_H_
+
+#include "drmP.h"
+
+enum psb_task_type {
+	psb_ta_midscene_task,
+	psb_ta_task,
+	psb_raster_task,
+	psb_freescene_task
+};
+
+#define PSB_MAX_TA_CMDS 60
+#define PSB_MAX_RASTER_CMDS 60
+#define PSB_MAX_OOM_CMDS 6
+
+struct psb_xhw_buf {
+	struct list_head head;
+	int copy_back;
+	atomic_t done;
+	struct drm_psb_xhw_arg arg;
+
+};
+
+struct psb_feedback_info {
+	struct drm_buffer_object *bo;
+	struct page *page;
+	uint32_t offset;
+};
+
+struct psb_task {
+	struct list_head head;
+	struct psb_scene *scene;
+	struct psb_feedback_info feedback;
+	enum psb_task_type task_type;
+	uint32_t engine;
+	uint32_t sequence;
+	uint32_t ta_cmds[PSB_MAX_TA_CMDS];
+	uint32_t raster_cmds[PSB_MAX_RASTER_CMDS];
+	uint32_t oom_cmds[PSB_MAX_OOM_CMDS];
+	uint32_t ta_cmd_size;
+	uint32_t raster_cmd_size;
+	uint32_t oom_cmd_size;
+	uint32_t feedback_offset;
+	uint32_t ta_complete_action;
+	uint32_t raster_complete_action;
+	uint32_t hw_cookie;
+	uint32_t flags;
+	uint32_t reply_flags;
+	uint32_t aborting;
+	struct psb_xhw_buf buf;
+};
+
+struct psb_hw_scene {
+	struct list_head head;
+	uint32_t context_number;
+
+	/*
+	 * This pointer does not refcount the last_scene_buffer,
+	 * so we must make sure it is set to NULL before destroying
+	 * the corresponding task.
+	 */
+
+	struct psb_scene *last_scene;
+};
+
+struct psb_scene;
+struct drm_psb_private;
+
+struct psb_scheduler_seq {
+	uint32_t sequence;
+	int reported;
+};
+
+struct psb_scheduler {
+	struct drm_device *dev;
+	struct psb_scheduler_seq seq[_PSB_ENGINE_TA_FENCE_TYPES];
+	struct psb_hw_scene hs[PSB_NUM_HW_SCENES];
+	struct mutex task_wq_mutex;
+	spinlock_t lock;
+	struct list_head hw_scenes;
+	struct list_head ta_queue;
+	struct list_head raster_queue;
+	struct list_head hp_raster_queue;
+	struct list_head task_done_queue;
+	struct psb_task *current_task[PSB_SCENE_NUM_ENGINES];
+	struct psb_task *feedback_task;
+	int ta_state;
+	struct psb_hw_scene *pending_hw_scene;
+	uint32_t pending_hw_scene_seq;
+	struct delayed_work wq;
+	struct psb_scene_pool *pool;
+	uint32_t idle_count;
+	int idle;
+	wait_queue_head_t idle_queue;
+	unsigned long ta_end_jiffies;
+	unsigned long raster_end_jiffies;
+	unsigned long total_raster_jiffies;
+};
+
+#define PSB_RF_FIRE_TA       (1 << 0)
+#define PSB_RF_OOM           (1 << 1)
+#define PSB_RF_OOM_REPLY     (1 << 2)
+#define PSB_RF_TERMINATE     (1 << 3)
+#define PSB_RF_TA_DONE       (1 << 4)
+#define PSB_RF_FIRE_RASTER   (1 << 5)
+#define PSB_RF_RASTER_DONE   (1 << 6)
+#define PSB_RF_DEALLOC       (1 << 7)
+
+extern struct psb_scene_pool *psb_alloc_scene_pool(struct drm_file *priv,
+						   int shareable, uint32_t w,
+						   uint32_t h);
+extern uint32_t psb_scene_handle(struct psb_scene *scene);
+extern int psb_scheduler_init(struct drm_device *dev,
+			      struct psb_scheduler *scheduler);
+extern void psb_scheduler_takedown(struct psb_scheduler *scheduler);
+extern int psb_cmdbuf_ta(struct drm_file *priv,
+			 struct drm_psb_cmdbuf_arg *arg,
+			 struct drm_buffer_object *cmd_buffer,
+			 struct drm_buffer_object *ta_buffer,
+			 struct drm_buffer_object *oom_buffer,
+			 struct psb_scene *scene,
+			 struct psb_feedback_info *feedback,
+			 struct drm_fence_arg *fence_arg);
+extern int psb_cmdbuf_raster(struct drm_file *priv,
+			     struct drm_psb_cmdbuf_arg *arg,
+			     struct drm_buffer_object *cmd_buffer,
+			     struct drm_fence_arg *fence_arg);
+extern void psb_scheduler_handler(struct drm_psb_private *dev_priv,
+				  uint32_t status);
+extern void psb_scheduler_pause(struct drm_psb_private *dev_priv);
+extern void psb_scheduler_restart(struct drm_psb_private *dev_priv);
+extern int psb_scheduler_idle(struct drm_psb_private *dev_priv);
+extern int psb_scheduler_finished(struct drm_psb_private *dev_priv);
+
+extern void psb_scheduler_lockup(struct drm_psb_private *dev_priv,
+				 int *lockup, int *idle);
+extern void psb_scheduler_reset(struct drm_psb_private *dev_priv,
+				int error_condition);
+extern int psb_forced_user_interrupt(struct drm_psb_private *dev_priv);
+extern void psb_scheduler_remove_scene_refs(struct psb_scene *scene);
+extern void psb_scheduler_ta_mem_check(struct drm_psb_private *dev_priv);
+extern int psb_extend_raster_timeout(struct drm_psb_private *dev_priv);
+
+#endif
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_setup.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_setup.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,17 @@
+#include "drmP.h"
+#include "drm.h"
+#include "drm_crtc.h"
+#include "drm_edid.h"
+#include "intel_drv.h"
+#include "psb_drv.h"
+#include "i915_reg.h"
+#include "intel_crt.c"
+
+/* Fixed name */
+#define ACPI_EDID_LCD	"\\_SB_.PCI0.GFX0.DD04._DDC"
+#define ACPI_DOD	"\\_SB_.PCI0.GFX0._DOD"
+
+#include "intel_lvds.c"
+#include "intel_sdvo.c"
+#include "intel_display.c"
+#include "intel_modes.c"
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_sgx.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_sgx.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,1422 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ */
+
+#include "drmP.h"
+#include "psb_drv.h"
+#include "psb_drm.h"
+#include "psb_reg.h"
+#include "psb_scene.h"
+
+#include "psb_msvdx.h"
+
+int psb_submit_video_cmdbuf(struct drm_device *dev,
+			    struct drm_buffer_object *cmd_buffer,
+			    unsigned long cmd_offset, unsigned long cmd_size,
+			    struct drm_fence_object *fence);
+
+struct psb_dstbuf_cache {
+	unsigned int dst;
+	uint32_t *use_page;
+	unsigned int use_index;
+	uint32_t use_background;
+	struct drm_buffer_object *dst_buf;
+	unsigned long dst_offset;
+	uint32_t *dst_page;
+	unsigned int dst_page_offset;
+	struct drm_bo_kmap_obj dst_kmap;
+	int dst_is_iomem;
+};
+
+struct psb_buflist_item {
+	struct drm_buffer_object *bo;
+	void __user *data;
+	int ret;
+	int presumed_offset_correct;
+};
+
+
+#define PSB_REG_GRAN_SHIFT 2
+#define PSB_REG_GRANULARITY (1 << PSB_REG_GRAN_SHIFT)
+#define PSB_MAX_REG 0x1000
+
+static const uint32_t disallowed_ranges[][2] = {
+	{0x0000, 0x0200},
+	{0x0208, 0x0214},
+	{0x021C, 0x0224},
+	{0x0230, 0x0234},
+	{0x0248, 0x024C},
+	{0x0254, 0x0358},
+	{0x0428, 0x0428},
+	{0x0430, 0x043C},
+	{0x0498, 0x04B4},
+	{0x04CC, 0x04D8},
+	{0x04E0, 0x07FC},
+	{0x0804, 0x0A58},
+	{0x0A68, 0x0A80},
+	{0x0AA0, 0x0B1C},
+	{0x0B2C, 0x0CAC},
+	{0x0CB4, PSB_MAX_REG - PSB_REG_GRANULARITY}
+};
+
+static uint32_t psb_disallowed_regs[PSB_MAX_REG /
+				    (PSB_REG_GRANULARITY *
+				     (sizeof(uint32_t) << 3))];
+
+static inline int psb_disallowed(uint32_t reg)
+{
+	reg >>= PSB_REG_GRAN_SHIFT;
+	return ((psb_disallowed_regs[reg >> 5] & (1 << (reg & 31))) != 0);
+}
+
+void psb_init_disallowed(void)
+{
+	int i;
+	uint32_t reg, tmp;
+	static int initialized = 0;
+
+	if (initialized)
+		return;
+
+	initialized = 1;
+	memset(psb_disallowed_regs, 0, sizeof(psb_disallowed_regs));
+
+	for (i = 0; i < (sizeof(disallowed_ranges) / (2 * sizeof(uint32_t)));
+	     ++i) {
+		for (reg = disallowed_ranges[i][0];
+		     reg <= disallowed_ranges[i][1]; reg += 4) {
+			tmp = reg >> 2;
+			psb_disallowed_regs[tmp >> 5] |= (1 << (tmp & 31));
+		}
+	}
+}
+
+static int psb_memcpy_check(uint32_t * dst, const uint32_t * src, uint32_t size)
+{
+	size >>= 3;
+	while (size--) {
+		if (unlikely((*src >= 0x1000) || psb_disallowed(*src))) {
+			DRM_ERROR("Forbidden SGX register access: "
+				  "0x%04x.\n", *src);
+			return -EPERM;
+		}
+		*dst++ = *src++;
+		*dst++ = *src++;
+	}
+	return 0;
+}
+
+static int psb_2d_wait_available(struct drm_psb_private *dev_priv,
+				 unsigned size)
+{
+	uint32_t avail = PSB_RSGX32(PSB_CR_2D_SOCIF);
+	int ret = 0;
+
+      retry:
+	if (avail < size) {
+#if 0
+		/* We'd ideally
+		 * like to have an IRQ-driven event here.
+		 */
+
+		psb_2D_irq_on(dev_priv);
+		DRM_WAIT_ON(ret, dev_priv->event_2d_queue, DRM_HZ,
+			    ((avail = PSB_RSGX32(PSB_CR_2D_SOCIF)) >= size));
+		psb_2D_irq_off(dev_priv);
+		if (ret == 0)
+			return 0;
+		if (ret == -EINTR) {
+			ret = 0;
+			goto retry;
+		}
+#else
+		avail = PSB_RSGX32(PSB_CR_2D_SOCIF);
+		goto retry;
+#endif
+	}
+	return ret;
+}
+
+int psb_2d_submit(struct drm_psb_private *dev_priv, uint32_t * cmdbuf,
+		  unsigned size)
+{
+	int ret = 0;
+	int i;
+	unsigned submit_size;
+
+	while (size > 0) {
+		submit_size = (size < 0x60) ? size : 0x60;
+		size -= submit_size;
+		ret = psb_2d_wait_available(dev_priv, submit_size);
+		if (ret)
+			return ret;
+
+		submit_size <<= 2;
+
+		for (i = 0; i < submit_size; i += 4) {
+			PSB_WSGX32(*cmdbuf++, PSB_SGX_2D_SLAVE_PORT + i);
+		}
+		(void)PSB_RSGX32(PSB_SGX_2D_SLAVE_PORT + i - 4);
+	}
+	return 0;
+}
+
+int psb_blit_sequence(struct drm_psb_private *dev_priv, uint32_t sequence)
+{
+	uint32_t buffer[8];
+	uint32_t *bufp = buffer;
+	int ret;
+
+	*bufp++ = PSB_2D_FENCE_BH;
+
+	*bufp++ = PSB_2D_DST_SURF_BH |
+	    PSB_2D_DST_8888ARGB | (4 << PSB_2D_DST_STRIDE_SHIFT);
+	*bufp++ = dev_priv->comm_mmu_offset - dev_priv->mmu_2d_offset;
+
+	*bufp++ = PSB_2D_BLIT_BH |
+	    PSB_2D_ROT_NONE |
+	    PSB_2D_COPYORDER_TL2BR |
+	    PSB_2D_DSTCK_DISABLE |
+	    PSB_2D_SRCCK_DISABLE | PSB_2D_USE_FILL | PSB_2D_ROP3_PATCOPY;
+
+	*bufp++ = sequence << PSB_2D_FILLCOLOUR_SHIFT;
+	*bufp++ = (0 << PSB_2D_DST_XSTART_SHIFT) |
+	    (0 << PSB_2D_DST_YSTART_SHIFT);
+	*bufp++ = (1 << PSB_2D_DST_XSIZE_SHIFT) | (1 << PSB_2D_DST_YSIZE_SHIFT);
+
+	*bufp++ = PSB_2D_FLUSH_BH;
+
+	psb_2d_lock(dev_priv);
+	ret = psb_2d_submit(dev_priv, buffer, bufp - buffer);
+	psb_2d_unlock(dev_priv);
+
+	if (!ret)
+		psb_schedule_watchdog(dev_priv);
+	return ret;
+}
+
+int psb_emit_2d_copy_blit(struct drm_device *dev,
+			  uint32_t src_offset,
+			  uint32_t dst_offset, uint32_t pages, int direction)
+{
+	uint32_t cur_pages;
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	uint32_t buf[10];
+	uint32_t *bufp;
+	uint32_t xstart;
+	uint32_t ystart;
+	uint32_t blit_cmd;
+	uint32_t pg_add;
+	int ret = 0;
+
+	if (!dev_priv)
+		return 0;
+
+	if (direction) {
+		pg_add = (pages - 1) << PAGE_SHIFT;
+		src_offset += pg_add;
+		dst_offset += pg_add;
+	}
+
+	blit_cmd = PSB_2D_BLIT_BH |
+	    PSB_2D_ROT_NONE |
+	    PSB_2D_DSTCK_DISABLE |
+	    PSB_2D_SRCCK_DISABLE |
+	    PSB_2D_USE_PAT |
+	    PSB_2D_ROP3_SRCCOPY |
+	    (direction ? PSB_2D_COPYORDER_BR2TL : PSB_2D_COPYORDER_TL2BR);
+	xstart = (direction) ? ((PAGE_SIZE - 1) >> 2) : 0;
+
+	psb_2d_lock(dev_priv);
+	while (pages > 0) {
+		cur_pages = pages;
+		if (cur_pages > 2048)
+			cur_pages = 2048;
+		pages -= cur_pages;
+		ystart = (direction) ? cur_pages - 1 : 0;
+
+		bufp = buf;
+		*bufp++ = PSB_2D_FENCE_BH;
+
+		*bufp++ = PSB_2D_DST_SURF_BH | PSB_2D_DST_8888ARGB |
+		    (PAGE_SIZE << PSB_2D_DST_STRIDE_SHIFT);
+		*bufp++ = dst_offset;
+		*bufp++ = PSB_2D_SRC_SURF_BH | PSB_2D_SRC_8888ARGB |
+		    (PAGE_SIZE << PSB_2D_SRC_STRIDE_SHIFT);
+		*bufp++ = src_offset;
+		*bufp++ =
+		    PSB_2D_SRC_OFF_BH | (xstart << PSB_2D_SRCOFF_XSTART_SHIFT) |
+		    (ystart << PSB_2D_SRCOFF_YSTART_SHIFT);
+		*bufp++ = blit_cmd;
+		*bufp++ = (xstart << PSB_2D_DST_XSTART_SHIFT) |
+		    (ystart << PSB_2D_DST_YSTART_SHIFT);
+		*bufp++ = ((PAGE_SIZE >> 2) << PSB_2D_DST_XSIZE_SHIFT) |
+		    (cur_pages << PSB_2D_DST_YSIZE_SHIFT);
+
+		ret = psb_2d_submit(dev_priv, buf, bufp - buf);
+		if (ret)
+			goto out;
+		pg_add = (cur_pages << PAGE_SHIFT) * ((direction) ? -1 : 1);
+		src_offset += pg_add;
+		dst_offset += pg_add;
+	}
+      out:
+	psb_2d_unlock(dev_priv);
+	return ret;
+}
+
+void psb_init_2d(struct drm_psb_private *dev_priv)
+{
+	dev_priv->sequence_lock = SPIN_LOCK_UNLOCKED;
+	psb_reset(dev_priv, 1);
+	dev_priv->mmu_2d_offset = dev_priv->pg->gatt_start;
+	PSB_WSGX32(dev_priv->mmu_2d_offset, PSB_CR_BIF_TWOD_REQ_BASE);
+	(void)PSB_RSGX32(PSB_CR_BIF_TWOD_REQ_BASE);
+}
+
+int psb_idle_2d(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	unsigned long _end = jiffies + DRM_HZ;
+	int busy = 0;
+
+	/*
+	 * First idle the 2D engine.
+	 */
+
+	if (dev_priv->engine_lockup_2d)
+		return -EBUSY;
+
+	if ((PSB_RSGX32(PSB_CR_2D_SOCIF) == _PSB_C2_SOCIF_EMPTY) &&
+	    ((PSB_RSGX32(PSB_CR_2D_BLIT_STATUS) & _PSB_C2B_STATUS_BUSY) == 0))
+		goto out;
+
+	do {
+		busy = (PSB_RSGX32(PSB_CR_2D_SOCIF) != _PSB_C2_SOCIF_EMPTY);
+	} while (busy && !time_after_eq(jiffies, _end));
+
+	if (busy)
+		busy = (PSB_RSGX32(PSB_CR_2D_SOCIF) != _PSB_C2_SOCIF_EMPTY);
+	if (busy)
+		goto out;
+
+	do {
+		busy =
+		    ((PSB_RSGX32(PSB_CR_2D_BLIT_STATUS) & _PSB_C2B_STATUS_BUSY)
+		     != 0);
+	} while (busy && !time_after_eq(jiffies, _end));
+	if (busy)
+		busy =
+		    ((PSB_RSGX32(PSB_CR_2D_BLIT_STATUS) & _PSB_C2B_STATUS_BUSY)
+		     != 0);
+
+      out:
+	if (busy)
+		dev_priv->engine_lockup_2d = 1;
+
+	return (busy) ? -EBUSY : 0;
+}
+
+int psb_idle_3d(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	struct psb_scheduler *scheduler = &dev_priv->scheduler;
+	int ret;
+
+	ret = wait_event_timeout(scheduler->idle_queue,
+				 psb_scheduler_finished(dev_priv), DRM_HZ * 10);
+
+	return (ret < 1) ? -EBUSY : 0;
+}
+
+static void psb_dereference_buffers_locked(struct psb_buflist_item *buffers,
+					   unsigned num_buffers)
+{
+	while (num_buffers--)
+		drm_bo_usage_deref_locked(&((buffers++)->bo));
+
+}
+
+static int psb_check_presumed(struct drm_bo_op_arg *arg,
+			      struct drm_buffer_object *bo,
+			      uint32_t __user * data, int *presumed_ok)
+{
+	struct drm_bo_op_req *req = &arg->d.req;
+	uint32_t hint_offset;
+	uint32_t hint = req->bo_req.hint;
+
+	*presumed_ok = 0;
+
+	if (!(hint & DRM_BO_HINT_PRESUMED_OFFSET))
+		return 0;
+	if (bo->mem.mem_type == DRM_BO_MEM_LOCAL) {
+		*presumed_ok = 1;
+		return 0;
+	}
+	if (bo->offset == req->bo_req.presumed_offset) {
+		*presumed_ok = 1;
+		return 0;
+	}
+
+	/*
+	 * We need to turn off the HINT_PRESUMED_OFFSET for this buffer in
+	 * the user-space IOCTL argument list, since the buffer has moved,
+	 * we're about to apply relocations and we might subsequently
+	 * hit an -EAGAIN. In that case the argument list will be reused by
+	 * user-space, but the presumed offset is no longer valid.
+	 *
+	 * Needless to say, this is a bit ugly.
+	 */
+
+	hint_offset = (uint32_t *) & req->bo_req.hint - (uint32_t *) arg;
+	hint &= ~DRM_BO_HINT_PRESUMED_OFFSET;
+	return __put_user(hint, data + hint_offset);
+}
+
+static int psb_validate_buffer_list(struct drm_file *file_priv,
+				    unsigned fence_class,
+				    unsigned long data,
+				    struct psb_buflist_item *buffers,
+				    unsigned *num_buffers)
+{
+	struct drm_bo_op_arg arg;
+	struct drm_bo_op_req *req = &arg.d.req;
+	int ret = 0;
+	unsigned buf_count = 0;
+	struct psb_buflist_item *item = buffers;
+
+	do {
+		if (buf_count >= *num_buffers) {
+			DRM_ERROR("Buffer count exceeded %d\n.", *num_buffers);
+			ret = -EINVAL;
+			goto out_err;
+		}
+		item = buffers + buf_count;
+		item->bo = NULL;
+
+		if (copy_from_user(&arg, (void __user *)data, sizeof(arg))) {
+			ret = -EFAULT;
+			DRM_ERROR("Error copying validate list.\n"
+				  "\tbuffer %d, user addr 0x%08lx %d\n",
+				  buf_count, (unsigned long)data, sizeof(arg));
+			goto out_err;
+		}
+
+		ret = 0;
+		if (req->op != drm_bo_validate) {
+			DRM_ERROR
+			    ("Buffer object operation wasn't \"validate\".\n");
+			ret = -EINVAL;
+			goto out_err;
+		}
+
+		item->ret = 0;
+		item->data = (void *)__user data;
+		ret = drm_bo_handle_validate(file_priv,
+					     req->bo_req.handle,
+					     fence_class,
+					     req->bo_req.flags,
+					     req->bo_req.mask,
+					     req->bo_req.hint,
+					     0, NULL, &item->bo);
+		if (ret)
+			goto out_err;
+
+		PSB_DEBUG_GENERAL("Validated buffer at 0x%08lx\n",
+				  buffers[buf_count].bo->offset);
+
+		buf_count++;
+
+
+		ret = psb_check_presumed(&arg, item->bo,
+					 (uint32_t __user *)
+					 (unsigned long) data,
+					 &item->presumed_offset_correct);
+
+		if (ret)
+			goto out_err;
+
+		data = arg.next;
+	} while (data);
+
+	*num_buffers = buf_count;
+
+	return 0;
+      out_err:
+
+	*num_buffers = buf_count;
+	item->ret = (ret != -EAGAIN) ? ret : 0;
+	return ret;
+}
+
+int
+psb_reg_submit(struct drm_psb_private *dev_priv, uint32_t * regs,
+	       unsigned int cmds)
+{
+	int i;
+
+	/*
+	 * cmds is 32-bit words.
+	 */
+
+	cmds >>= 1;
+	for (i = 0; i < cmds; ++i) {
+		PSB_WSGX32(regs[1], regs[0]);
+		regs += 2;
+	}
+	wmb();
+	return 0;
+}
+
+/*
+ * Security: Block user-space writing to MMU mapping registers.
+ * This is important for security and brings Poulsbo DRM
+ * up to par with the other DRM drivers. Using this,
+ * user-space should not be able to map arbitrary memory
+ * pages to graphics memory, but all user-space processes
+ * basically have access to all buffer objects mapped to
+ * graphics memory.
+ */
+
+int
+psb_submit_copy_cmdbuf(struct drm_device *dev,
+		       struct drm_buffer_object *cmd_buffer,
+		       unsigned long cmd_offset,
+		       unsigned long cmd_size,
+		       int engine, uint32_t * copy_buffer)
+{
+	unsigned long cmd_end = cmd_offset + (cmd_size << 2);
+	struct drm_psb_private *dev_priv = dev->dev_private;
+	unsigned long cmd_page_offset = cmd_offset - (cmd_offset & PAGE_MASK);
+	unsigned long cmd_next;
+	struct drm_bo_kmap_obj cmd_kmap;
+	uint32_t *cmd_page;
+	unsigned cmds;
+	int is_iomem;
+	int ret = 0;
+
+	if (cmd_size == 0)
+		return 0;
+
+	if (engine == PSB_ENGINE_2D)
+		psb_2d_lock(dev_priv);
+
+	do {
+		cmd_next = drm_bo_offset_end(cmd_offset, cmd_end);
+		ret = drm_bo_kmap(cmd_buffer, cmd_offset >> PAGE_SHIFT,
+				  1, &cmd_kmap);
+
+		if (ret)
+			return ret;
+		cmd_page = drm_bmo_virtual(&cmd_kmap, &is_iomem);
+		cmd_page_offset = (cmd_offset & ~PAGE_MASK) >> 2;
+		cmds = (cmd_next - cmd_offset) >> 2;
+
+		switch (engine) {
+		case PSB_ENGINE_2D:
+			ret =
+			    psb_2d_submit(dev_priv, cmd_page + cmd_page_offset,
+					  cmds);
+			break;
+		case PSB_ENGINE_RASTERIZER:
+		case PSB_ENGINE_TA:
+		case PSB_ENGINE_HPRAST:
+			PSB_DEBUG_GENERAL("Reg copy.\n");
+			ret = psb_memcpy_check(copy_buffer,
+					       cmd_page + cmd_page_offset,
+					       cmds * sizeof(uint32_t));
+			copy_buffer += cmds;
+			break;
+		default:
+			ret = -EINVAL;
+		}
+		drm_bo_kunmap(&cmd_kmap);
+		if (ret)
+			break;
+	} while (cmd_offset = cmd_next, cmd_offset != cmd_end);
+
+	if (engine == PSB_ENGINE_2D)
+		psb_2d_unlock(dev_priv);
+
+	return ret;
+}
+
+static void psb_clear_dstbuf_cache(struct psb_dstbuf_cache *dst_cache)
+{
+	if (dst_cache->dst_page) {
+		drm_bo_kunmap(&dst_cache->dst_kmap);
+		dst_cache->dst_page = NULL;
+	}
+	dst_cache->dst_buf = NULL;
+	dst_cache->dst = ~0;
+	dst_cache->use_page = NULL;
+}
+
+static int psb_update_dstbuf_cache(struct psb_dstbuf_cache *dst_cache,
+				   struct psb_buflist_item *buffers,
+				   unsigned int dst, unsigned long dst_offset)
+{
+	int ret;
+
+	PSB_DEBUG_RELOC("Destination buffer is %d.\n", dst);
+
+	if (unlikely(dst != dst_cache->dst || NULL == dst_cache->dst_buf)) {
+		psb_clear_dstbuf_cache(dst_cache);
+		dst_cache->dst = dst;
+		dst_cache->dst_buf = buffers[dst].bo;
+	}
+
+	if (unlikely(dst_offset > dst_cache->dst_buf->num_pages * PAGE_SIZE)) {
+		DRM_ERROR("Relocation destination out of bounds.\n");
+		return -EINVAL;
+	}
+
+	if (!drm_bo_same_page(dst_cache->dst_offset, dst_offset) ||
+	    NULL == dst_cache->dst_page) {
+		if (NULL != dst_cache->dst_page) {
+			drm_bo_kunmap(&dst_cache->dst_kmap);
+			dst_cache->dst_page = NULL;
+		}
+
+		ret = drm_bo_kmap(dst_cache->dst_buf, dst_offset >> PAGE_SHIFT,
+				  1, &dst_cache->dst_kmap);
+		if (ret) {
+			DRM_ERROR("Could not map destination buffer for "
+				  "relocation.\n");
+			return ret;
+		}
+
+		dst_cache->dst_page = drm_bmo_virtual(&dst_cache->dst_kmap,
+						      &dst_cache->dst_is_iomem);
+		dst_cache->dst_offset = dst_offset & PAGE_MASK;
+		dst_cache->dst_page_offset = dst_cache->dst_offset >> 2;
+	}
+	return 0;
+}
+
+static int psb_apply_reloc(struct drm_psb_private *dev_priv,
+			   uint32_t fence_class,
+			   const struct drm_psb_reloc *reloc,
+			   struct psb_buflist_item *buffers,
+			   int num_buffers,
+			   struct psb_dstbuf_cache *dst_cache,
+			   int no_wait, int interruptible)
+{
+	int reg;
+	uint32_t val;
+	uint32_t background;
+	unsigned int index;
+	int ret;
+	unsigned int shift;
+	unsigned int align_shift;
+	uint32_t fence_type;
+	struct drm_buffer_object *reloc_bo;
+
+	PSB_DEBUG_RELOC("Reloc type %d\n"
+			  "\t where 0x%04x\n"
+			  "\t buffer 0x%04x\n"
+			  "\t mask 0x%08x\n"
+			  "\t shift 0x%08x\n"
+			  "\t pre_add 0x%08x\n"
+			  "\t background 0x%08x\n"
+			  "\t dst_buffer 0x%08x\n"
+			  "\t arg0 0x%08x\n"
+			  "\t arg1 0x%08x\n",
+			  reloc->reloc_op,
+			  reloc->where,
+			  reloc->buffer,
+			  reloc->mask,
+			  reloc->shift,
+			  reloc->pre_add,
+			  reloc->background,
+			  reloc->dst_buffer, reloc->arg0, reloc->arg1);
+
+	if (unlikely(reloc->buffer >= num_buffers)) {
+		DRM_ERROR("Illegal relocation buffer %d.\n", reloc->buffer);
+		return -EINVAL;
+	}
+
+	if (buffers[reloc->buffer].presumed_offset_correct)
+		return 0;
+
+	if (unlikely(reloc->dst_buffer >= num_buffers)) {
+		DRM_ERROR("Illegal destination buffer for relocation %d.\n",
+			  reloc->dst_buffer);
+		return -EINVAL;
+	}
+
+	ret = psb_update_dstbuf_cache(dst_cache, buffers, reloc->dst_buffer,
+				      reloc->where << 2);
+	if (ret)
+		return ret;
+
+	reloc_bo = buffers[reloc->buffer].bo;
+
+	if (unlikely(reloc->pre_add > (reloc_bo->num_pages << PAGE_SHIFT))) {
+		DRM_ERROR("Illegal relocation offset add.\n");
+		return -EINVAL;
+	}
+
+	switch (reloc->reloc_op) {
+	case PSB_RELOC_OP_OFFSET:
+		val = reloc_bo->offset + reloc->pre_add;
+		break;
+	case PSB_RELOC_OP_2D_OFFSET:
+		val = reloc_bo->offset + reloc->pre_add -
+		    dev_priv->mmu_2d_offset;
+		if (unlikely(val >= PSB_2D_SIZE)) {
+			DRM_ERROR("2D relocation out of bounds\n");
+			return -EINVAL;
+		}
+		break;
+	case PSB_RELOC_OP_PDS_OFFSET:
+		val = reloc_bo->offset + reloc->pre_add - PSB_MEM_PDS_START;
+		if (unlikely(val >= (PSB_MEM_MMU_START - PSB_MEM_PDS_START))) {
+			DRM_ERROR("PDS relocation out of bounds\n");
+			return -EINVAL;
+		}
+		break;
+	case PSB_RELOC_OP_USE_OFFSET:
+	case PSB_RELOC_OP_USE_REG:
+
+		/*
+		 * Security:
+		 * Only allow VERTEX or PIXEL data masters, as
+		 * shaders run under other data masters may in theory
+		 * alter MMU mappings.
+		 */
+
+		if (unlikely(reloc->arg1 != _PSB_CUC_DM_PIXEL &&
+			     reloc->arg1 != _PSB_CUC_DM_VERTEX)) {
+			DRM_ERROR("Invalid data master in relocation. %d\n",
+				  reloc->arg1);
+			return -EPERM;
+		}
+
+		fence_type = reloc_bo->fence_type;
+		ret = psb_grab_use_base(dev_priv,
+					reloc_bo->offset +
+					reloc->pre_add, reloc->arg0,
+					reloc->arg1, fence_class,
+					fence_type, no_wait,
+					interruptible, &reg, &val);
+		if (ret)
+			return ret;
+
+		val = (reloc->reloc_op == PSB_RELOC_OP_USE_REG) ? reg : val;
+		break;
+	default:
+		DRM_ERROR("Unimplemented relocation.\n");
+		return -EINVAL;
+	}
+
+	shift = (reloc->shift & PSB_RELOC_SHIFT_MASK) >> PSB_RELOC_SHIFT_SHIFT;
+	align_shift = (reloc->shift & PSB_RELOC_ALSHIFT_MASK) >>
+	    PSB_RELOC_ALSHIFT_SHIFT;
+
+	val = ((val >> align_shift) << shift);
+	index = reloc->where - dst_cache->dst_page_offset;
+
+	background = reloc->background;
+
+	if (reloc->reloc_op == PSB_RELOC_OP_USE_OFFSET) {
+		if (dst_cache->use_page == dst_cache->dst_page &&
+		    dst_cache->use_index == index)
+			background = dst_cache->use_background;
+		else
+			background = dst_cache->dst_page[index];
+	}
+#if 0
+	if (dst_cache->dst_page[index] != PSB_RELOC_MAGIC &&
+	    reloc->reloc_op != PSB_RELOC_OP_USE_OFFSET)
+		DRM_ERROR("Inconsistent relocation 0x%08lx.\n",
+			  (unsigned long)dst_cache->dst_page[index]);
+#endif
+
+	val = (background & ~reloc->mask) | (val & reloc->mask);
+	dst_cache->dst_page[index] = val;
+
+	if (reloc->reloc_op == PSB_RELOC_OP_USE_OFFSET ||
+	    reloc->reloc_op == PSB_RELOC_OP_USE_REG) {
+		dst_cache->use_page = dst_cache->dst_page;
+		dst_cache->use_index = index;
+		dst_cache->use_background = val;
+	}
+
+	PSB_DEBUG_RELOC("Reloc buffer %d index 0x%08x, value 0x%08x\n",
+			  reloc->dst_buffer, index, dst_cache->dst_page[index]);
+
+	return 0;
+}
+
+static int psb_ok_to_map_reloc(struct drm_psb_private *dev_priv,
+			       unsigned int num_pages)
+{
+	int ret = 0;
+
+	spin_lock(&dev_priv->reloc_lock);
+	if (dev_priv->rel_mapped_pages + num_pages <= PSB_MAX_RELOC_PAGES) {
+		dev_priv->rel_mapped_pages += num_pages;
+		ret = 1;
+	}
+	spin_unlock(&dev_priv->reloc_lock);
+	return ret;
+}
+
+static int psb_fixup_relocs(struct drm_file *file_priv,
+			    uint32_t fence_class,
+			    unsigned int num_relocs,
+			    unsigned int reloc_offset,
+			    uint32_t reloc_handle,
+			    struct psb_buflist_item *buffers,
+			    unsigned int num_buffers,
+			    int no_wait, int interruptible)
+{
+	struct drm_device *dev = file_priv->minor->dev;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_buffer_object *reloc_buffer = NULL;
+	unsigned int reloc_num_pages;
+	unsigned int reloc_first_page;
+	unsigned int reloc_last_page;
+	struct psb_dstbuf_cache dst_cache;
+	struct drm_psb_reloc *reloc;
+	struct drm_bo_kmap_obj reloc_kmap;
+	int reloc_is_iomem;
+	int count;
+	int ret = 0;
+	int registered = 0;
+	int short_circuit = 1;
+	int i;
+
+	if (num_relocs == 0)
+		return 0;
+
+	for (i=0; i<num_buffers; ++i) {
+		if (!buffers[i].presumed_offset_correct) {
+			short_circuit = 0;
+			break;
+		}
+	}
+
+	if (short_circuit)
+		return 0;
+
+	memset(&dst_cache, 0, sizeof(dst_cache));
+	memset(&reloc_kmap, 0, sizeof(reloc_kmap));
+
+	mutex_lock(&dev->struct_mutex);
+	reloc_buffer = drm_lookup_buffer_object(file_priv, reloc_handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+	if (!reloc_buffer)
+		goto out;
+
+	reloc_first_page = reloc_offset >> PAGE_SHIFT;
+	reloc_last_page =
+	    (reloc_offset +
+	     num_relocs * sizeof(struct drm_psb_reloc)) >> PAGE_SHIFT;
+	reloc_num_pages = reloc_last_page - reloc_first_page + 1;
+	reloc_offset &= ~PAGE_MASK;
+
+	if (reloc_num_pages > PSB_MAX_RELOC_PAGES) {
+		DRM_ERROR("Relocation buffer is too large\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	DRM_WAIT_ON(ret, dev_priv->rel_mapped_queue, 3 * DRM_HZ,
+		    (registered =
+		     psb_ok_to_map_reloc(dev_priv, reloc_num_pages)));
+
+	if (ret == -EINTR) {
+		ret = -EAGAIN;
+		goto out;
+	}
+	if (ret) {
+		DRM_ERROR("Error waiting for space to map "
+			  "relocation buffer.\n");
+		goto out;
+	}
+
+	ret = drm_bo_kmap(reloc_buffer, reloc_first_page,
+			  reloc_num_pages, &reloc_kmap);
+
+	if (ret) {
+		DRM_ERROR("Could not map relocation buffer.\n"
+			  "\tReloc buffer id 0x%08x.\n"
+			  "\tReloc first page %d.\n"
+			  "\tReloc num pages %d.\n",
+			  reloc_handle, reloc_first_page, reloc_num_pages);
+		goto out;
+	}
+
+	reloc = (struct drm_psb_reloc *)
+	    ((unsigned long)drm_bmo_virtual(&reloc_kmap, &reloc_is_iomem) +
+	     reloc_offset);
+
+	for (count = 0; count < num_relocs; ++count) {
+		ret = psb_apply_reloc(dev_priv, fence_class,
+				      reloc, buffers,
+				      num_buffers, &dst_cache,
+				      no_wait, interruptible);
+		if (ret)
+			goto out1;
+		reloc++;
+	}
+
+      out1:
+	drm_bo_kunmap(&reloc_kmap);
+      out:
+	if (registered) {
+		spin_lock(&dev_priv->reloc_lock);
+		dev_priv->rel_mapped_pages -= reloc_num_pages;
+		spin_unlock(&dev_priv->reloc_lock);
+		DRM_WAKEUP(&dev_priv->rel_mapped_queue);
+	}
+
+	psb_clear_dstbuf_cache(&dst_cache);
+	if (reloc_buffer)
+		drm_bo_usage_deref_unlocked(&reloc_buffer);
+	return ret;
+}
+
+static int psb_cmdbuf_2d(struct drm_file *priv,
+			 struct drm_psb_cmdbuf_arg *arg,
+			 struct drm_buffer_object *cmd_buffer,
+			 struct drm_fence_arg *fence_arg)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	int ret;
+
+	ret = mutex_lock_interruptible(&dev_priv->reset_mutex);
+	if (ret)
+		return -EAGAIN;
+
+	ret = psb_submit_copy_cmdbuf(dev, cmd_buffer, arg->cmdbuf_offset,
+				     arg->cmdbuf_size, PSB_ENGINE_2D, NULL);
+	if (ret)
+		goto out_unlock;
+
+	psb_fence_or_sync(priv, PSB_ENGINE_2D, arg, fence_arg, NULL);
+
+	mutex_lock(&cmd_buffer->mutex);
+	if (cmd_buffer->fence != NULL)
+		drm_fence_usage_deref_unlocked(&cmd_buffer->fence);
+	mutex_unlock(&cmd_buffer->mutex);
+      out_unlock:
+	mutex_unlock(&dev_priv->reset_mutex);
+	return ret;
+}
+
+#if 0
+static int psb_dump_page(struct drm_buffer_object *bo,
+			 unsigned int page_offset, unsigned int num)
+{
+	struct drm_bo_kmap_obj kmobj;
+	int is_iomem;
+	uint32_t *p;
+	int ret;
+	unsigned int i;
+
+	ret = drm_bo_kmap(bo, page_offset, 1, &kmobj);
+	if (ret)
+		return ret;
+
+	p = drm_bmo_virtual(&kmobj, &is_iomem);
+	for (i = 0; i < num; ++i)
+		PSB_DEBUG_GENERAL("0x%04x: 0x%08x\n", i, *p++);
+
+	drm_bo_kunmap(&kmobj);
+	return 0;
+}
+#endif
+
+static void psb_idle_engine(struct drm_device *dev, int engine)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	uint32_t dummy;
+
+	switch (engine) {
+	case PSB_ENGINE_2D:
+
+		/*
+		 * Make sure we flush 2D properly using a dummy
+		 * fence sequence emit.
+		 */
+
+		(void)psb_fence_emit_sequence(dev, PSB_ENGINE_2D, 0,
+					      &dummy, &dummy);
+		psb_2d_lock(dev_priv);
+		(void)psb_idle_2d(dev);
+		psb_2d_unlock(dev_priv);
+		break;
+	case PSB_ENGINE_TA:
+	case PSB_ENGINE_RASTERIZER:
+	case PSB_ENGINE_HPRAST:
+		(void)psb_idle_3d(dev);
+		break;
+	default:
+
+		/*
+		 * FIXME: Insert video engine idle command here.
+		 */
+
+		break;
+	}
+}
+
+void psb_fence_or_sync(struct drm_file *priv,
+		       int engine,
+		       struct drm_psb_cmdbuf_arg *arg,
+		       struct drm_fence_arg *fence_arg,
+		       struct drm_fence_object **fence_p)
+{
+	struct drm_device *dev = priv->minor->dev;
+	int ret;
+	struct drm_fence_object *fence;
+
+	ret = drm_fence_buffer_objects(dev, NULL, arg->fence_flags,
+				       NULL, &fence);
+
+	if (ret) {
+
+		/*
+		 * Fence creation failed.
+		 * Fall back to synchronous operation and idle the engine.
+		 */
+
+		psb_idle_engine(dev, engine);
+		if (!(arg->fence_flags & DRM_FENCE_FLAG_NO_USER)) {
+
+			/*
+			 * Communicate to user-space that
+			 * fence creation has failed and that
+			 * the engine is idle.
+			 */
+
+			fence_arg->handle = ~0;
+			fence_arg->error = ret;
+		}
+
+		drm_putback_buffer_objects(dev);
+		if (fence_p)
+			*fence_p = NULL;
+		return;
+	}
+
+	if (!(arg->fence_flags & DRM_FENCE_FLAG_NO_USER)) {
+
+		ret = drm_fence_add_user_object(priv, fence,
+						arg->fence_flags &
+						DRM_FENCE_FLAG_SHAREABLE);
+		if (!ret)
+			drm_fence_fill_arg(fence, fence_arg);
+		else {
+			/*
+			 * Fence user object creation failed.
+			 * We must idle the engine here as well, as user-
+			 * space expects a fence object to wait on. Since we
+			 * have a fence object we wait for it to signal
+			 * to indicate engine "sufficiently" idle.
+			 */
+
+			(void)drm_fence_object_wait(fence, 0, 1, fence->type);
+			drm_fence_usage_deref_unlocked(&fence);
+			fence_arg->handle = ~0;
+			fence_arg->error = ret;
+		}
+	}
+
+	if (fence_p)
+		*fence_p = fence;
+	else if (fence)
+		drm_fence_usage_deref_unlocked(&fence);
+}
+
+int psb_handle_copyback(struct drm_device *dev,
+			struct psb_buflist_item *buffers,
+			unsigned int num_buffers, int ret, void *data)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	struct drm_bo_op_arg arg;
+	struct psb_buflist_item *item = buffers;
+	struct drm_buffer_object *bo;
+	int err = ret;
+	int i;
+
+	/*
+	 * Clear the unfenced use base register lists and buffer lists.
+	 */
+
+	if (ret) {
+		drm_regs_fence(&dev_priv->use_manager, NULL);
+		drm_putback_buffer_objects(dev);
+	}
+
+	if (ret != -EAGAIN) {
+		for (i = 0; i < num_buffers; ++i) {
+			arg.handled = 1;
+			arg.d.rep.ret = item->ret;
+			bo = item->bo;
+			mutex_lock(&bo->mutex);
+			drm_bo_fill_rep_arg(bo, &arg.d.rep.bo_info);
+			mutex_unlock(&bo->mutex);
+			if (copy_to_user(item->data, &arg, sizeof(arg)))
+				err = -EFAULT;
+			++item;
+		}
+	}
+
+	return err;
+}
+
+static int psb_cmdbuf_video(struct drm_file *priv,
+			    struct drm_psb_cmdbuf_arg *arg,
+			    unsigned int num_buffers,
+			    struct drm_buffer_object *cmd_buffer,
+			    struct drm_fence_arg *fence_arg)
+{
+	struct drm_device *dev = priv->minor->dev;
+	struct drm_fence_object *fence;
+	int ret;
+
+	/*
+	 * Check this. Doesn't seem right. Have fencing done AFTER command
+	 * submission and make sure drm_psb_idle idles the MSVDX completely.
+	 */
+
+	psb_fence_or_sync(priv, PSB_ENGINE_VIDEO, arg, fence_arg, &fence);
+	ret = psb_submit_video_cmdbuf(dev, cmd_buffer, arg->cmdbuf_offset,
+				      arg->cmdbuf_size, fence);
+
+	if (ret)
+		return ret;
+
+	drm_fence_usage_deref_unlocked(&fence);
+	mutex_lock(&cmd_buffer->mutex);
+	if (cmd_buffer->fence != NULL)
+		drm_fence_usage_deref_unlocked(&cmd_buffer->fence);
+	mutex_unlock(&cmd_buffer->mutex);
+	return 0;
+}
+
+int psb_feedback_buf(struct drm_file *file_priv,
+		     uint32_t feedback_ops,
+		     uint32_t handle,
+		     uint32_t offset,
+		     uint32_t feedback_breakpoints,
+		     uint32_t feedback_size, struct psb_feedback_info *feedback)
+{
+	struct drm_buffer_object *bo;
+	struct page *page;
+	uint32_t page_no;
+	uint32_t page_offset;
+	int ret;
+
+	if (feedback_ops & ~PSB_FEEDBACK_OP_VISTEST) {
+		DRM_ERROR("Illegal feedback op.\n");
+		return -EINVAL;
+	}
+
+	if (feedback_breakpoints != 0) {
+		DRM_ERROR("Feedback breakpoints not implemented yet.\n");
+		return -EINVAL;
+	}
+
+	if (feedback_size < PSB_HW_FEEDBACK_SIZE * sizeof(uint32_t)) {
+		DRM_ERROR("Feedback buffer size too small.\n");
+		return -EINVAL;
+	}
+
+	page_offset = offset & ~PAGE_MASK;
+	if ((PAGE_SIZE - PSB_HW_FEEDBACK_SIZE * sizeof(uint32_t))
+	    < page_offset) {
+		DRM_ERROR("Illegal feedback buffer alignment.\n");
+		return -EINVAL;
+	}
+
+	ret = drm_bo_handle_validate(file_priv,
+				     handle,
+				     PSB_ENGINE_TA,
+				     DRM_BO_FLAG_MEM_LOCAL |
+				     DRM_BO_FLAG_CACHED |
+				     DRM_BO_FLAG_WRITE |
+				     PSB_BO_FLAG_FEEDBACK,
+				     DRM_BO_MASK_MEM |
+				     DRM_BO_FLAG_CACHED |
+				     DRM_BO_FLAG_WRITE |
+				     PSB_BO_FLAG_FEEDBACK, 0, 0, NULL, &bo);
+	if (ret)
+		return ret;
+
+	page_no = offset >> PAGE_SHIFT;
+	if (page_no >= bo->num_pages) {
+		ret = -EINVAL;
+		DRM_ERROR("Illegal feedback buffer offset.\n");
+		goto out_unref;
+	}
+
+	if (bo->ttm == NULL) {
+		ret = -EINVAL;
+		DRM_ERROR("Vistest buffer without TTM.\n");
+		goto out_unref;
+	}
+
+	page = drm_ttm_get_page(bo->ttm, page_no);
+	if (!page) {
+		ret = -ENOMEM;
+		goto out_unref;
+	}
+
+	feedback->page = page;
+	feedback->bo = bo;
+	feedback->offset = page_offset;
+	return 0;
+
+      out_unref:
+	drm_bo_usage_deref_unlocked(&bo);
+	return ret;
+}
+
+int psb_cmdbuf_ioctl(struct drm_device *dev, void *data,
+		     struct drm_file *file_priv)
+{
+	drm_psb_cmdbuf_arg_t *arg = data;
+	int ret = 0;
+	unsigned num_buffers;
+	struct drm_buffer_object *cmd_buffer = NULL;
+	struct drm_buffer_object *ta_buffer = NULL;
+	struct drm_buffer_object *oom_buffer = NULL;
+	struct drm_fence_arg fence_arg;
+	struct drm_psb_scene user_scene;
+	struct psb_scene_pool *pool = NULL;
+	struct psb_scene *scene = NULL;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)file_priv->minor->dev->dev_private;
+	int engine;
+	struct psb_feedback_info feedback;
+
+	if (!dev_priv)
+		return -EINVAL;
+
+	ret = drm_bo_read_lock(&dev->bm.bm_lock);
+	if (ret)
+		return ret;
+
+	num_buffers = PSB_NUM_VALIDATE_BUFFERS;
+
+	ret = mutex_lock_interruptible(&dev_priv->cmdbuf_mutex);
+	if (ret) {
+		drm_bo_read_unlock(&dev->bm.bm_lock);
+		return -EAGAIN;
+	}
+	if (unlikely(dev_priv->buffers == NULL)) {
+		dev_priv->buffers = vmalloc(PSB_NUM_VALIDATE_BUFFERS *
+					    sizeof(*dev_priv->buffers));
+		if (dev_priv->buffers == NULL) {
+			drm_bo_read_unlock(&dev->bm.bm_lock);
+			return -ENOMEM;
+		}
+	}
+
+
+	engine = (arg->engine == PSB_ENGINE_RASTERIZER) ?
+	    PSB_ENGINE_TA : arg->engine;
+
+	ret =
+	    psb_validate_buffer_list(file_priv, engine,
+				     (unsigned long)arg->buffer_list,
+				     dev_priv->buffers, &num_buffers);
+	if (ret)
+		goto out_err0;
+
+	ret = psb_fixup_relocs(file_priv, engine, arg->num_relocs,
+			       arg->reloc_offset, arg->reloc_handle,
+			       dev_priv->buffers, num_buffers, 0, 1);
+	if (ret)
+		goto out_err0;
+
+	mutex_lock(&dev->struct_mutex);
+	cmd_buffer = drm_lookup_buffer_object(file_priv, arg->cmdbuf_handle, 1);
+	mutex_unlock(&dev->struct_mutex);
+	if (!cmd_buffer) {
+		ret = -EINVAL;
+		goto out_err0;
+	}
+
+	switch (arg->engine) {
+	case PSB_ENGINE_2D:
+		ret = psb_cmdbuf_2d(file_priv, arg, cmd_buffer, &fence_arg);
+		if (ret)
+			goto out_err0;
+		break;
+	case PSB_ENGINE_VIDEO:
+		ret =
+		    psb_cmdbuf_video(file_priv, arg, num_buffers, cmd_buffer,
+				     &fence_arg);
+		if (ret)
+			goto out_err0;
+		break;
+	case PSB_ENGINE_RASTERIZER:
+		ret = psb_cmdbuf_raster(file_priv, arg, cmd_buffer, &fence_arg);
+		if (ret)
+			goto out_err0;
+		break;
+	case PSB_ENGINE_TA:
+		if (arg->ta_handle == arg->cmdbuf_handle) {
+			mutex_lock(&dev->struct_mutex);
+			atomic_inc(&cmd_buffer->usage);
+			ta_buffer = cmd_buffer;
+			mutex_unlock(&dev->struct_mutex);
+		} else {
+			mutex_lock(&dev->struct_mutex);
+			ta_buffer =
+			    drm_lookup_buffer_object(file_priv,
+						     arg->ta_handle, 1);
+			mutex_unlock(&dev->struct_mutex);
+			if (!ta_buffer) {
+				ret = -EINVAL;
+				goto out_err0;
+			}
+		}
+		if (arg->oom_size != 0) {
+			if (arg->oom_handle == arg->cmdbuf_handle) {
+				mutex_lock(&dev->struct_mutex);
+				atomic_inc(&cmd_buffer->usage);
+				oom_buffer = cmd_buffer;
+				mutex_unlock(&dev->struct_mutex);
+			} else {
+				mutex_lock(&dev->struct_mutex);
+				oom_buffer =
+				    drm_lookup_buffer_object(file_priv,
+							     arg->oom_handle,
+							     1);
+				mutex_unlock(&dev->struct_mutex);
+				if (!oom_buffer) {
+					ret = -EINVAL;
+					goto out_err0;
+				}
+			}
+		}
+
+		ret = copy_from_user(&user_scene, (void __user *)
+				     ((unsigned long)arg->scene_arg),
+				     sizeof(user_scene));
+		if (ret)
+			goto out_err0;
+
+		if (!user_scene.handle_valid) {
+			pool = psb_scene_pool_alloc(file_priv, 0,
+						    user_scene.num_buffers,
+						    user_scene.w, user_scene.h);
+			if (!pool) {
+				ret = -ENOMEM;
+				goto out_err0;
+			}
+
+			user_scene.handle = psb_scene_pool_handle(pool);
+			user_scene.handle_valid = 1;
+			ret = copy_to_user((void __user *)
+					   ((unsigned long)arg->scene_arg),
+					   &user_scene, sizeof(user_scene));
+
+			if (ret)
+				goto out_err0;
+		} else {
+			mutex_lock(&dev->struct_mutex);
+			pool = psb_scene_pool_lookup_devlocked(file_priv,
+							       user_scene.
+							       handle, 1);
+			mutex_unlock(&dev->struct_mutex);
+			if (!pool) {
+				ret = -EINVAL;
+				goto out_err0;
+			}
+		}
+
+		mutex_lock(&dev_priv->reset_mutex);
+		ret = psb_validate_scene_pool(pool, 0, 0, 0,
+					      user_scene.w,
+					      user_scene.h,
+					      arg->ta_flags &
+					      PSB_TA_FLAG_LASTPASS, &scene);
+		mutex_unlock(&dev_priv->reset_mutex);
+
+		if (ret)
+			goto out_err0;
+
+		memset(&feedback, 0, sizeof(feedback));
+		if (arg->feedback_ops) {
+			ret = psb_feedback_buf(file_priv,
+					       arg->feedback_ops,
+					       arg->feedback_handle,
+					       arg->feedback_offset,
+					       arg->feedback_breakpoints,
+					       arg->feedback_size, &feedback);
+			if (ret)
+				goto out_err0;
+		}
+		ret = psb_cmdbuf_ta(file_priv, arg, cmd_buffer, ta_buffer,
+				    oom_buffer, scene, &feedback, &fence_arg);
+		if (ret)
+			goto out_err0;
+		break;
+	default:
+		DRM_ERROR("Unimplemented command submission mechanism (%x).\n",
+			  arg->engine);
+		ret = -EINVAL;
+		goto out_err0;
+	}
+
+	if (!(arg->fence_flags & DRM_FENCE_FLAG_NO_USER)) {
+		ret = copy_to_user((void __user *)
+				   ((unsigned long)arg->fence_arg),
+				   &fence_arg, sizeof(fence_arg));
+	}
+
+      out_err0:
+	ret =
+	    psb_handle_copyback(dev, dev_priv->buffers, num_buffers, ret, data);
+	mutex_lock(&dev->struct_mutex);
+	if (scene)
+		psb_scene_unref_devlocked(&scene);
+	if (pool)
+		psb_scene_pool_unref_devlocked(&pool);
+	if (cmd_buffer)
+		drm_bo_usage_deref_locked(&cmd_buffer);
+	if (ta_buffer)
+		drm_bo_usage_deref_locked(&ta_buffer);
+	if (oom_buffer)
+		drm_bo_usage_deref_locked(&oom_buffer);
+
+	psb_dereference_buffers_locked(dev_priv->buffers, num_buffers);
+	mutex_unlock(&dev->struct_mutex);
+	mutex_unlock(&dev_priv->cmdbuf_mutex);
+
+	drm_bo_read_unlock(&dev->bm.bm_lock);
+	return ret;
+}
Index: linux-2.6.27/drivers/gpu/drm/psb/psb_xhw.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/drivers/gpu/drm/psb/psb_xhw.c	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,614 @@
+/**************************************************************************
+ * Copyright (c) 2007, Intel Corporation.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
+ * develop this driver.
+ *
+ **************************************************************************/
+/*
+ * Make calls into closed source X server code.
+ */
+
+#include "drmP.h"
+#include "psb_drv.h"
+
+void
+psb_xhw_clean_buf(struct drm_psb_private *dev_priv, struct psb_xhw_buf *buf)
+{
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	list_del_init(&buf->head);
+	if (dev_priv->xhw_cur_buf == buf)
+		dev_priv->xhw_cur_buf = NULL;
+	atomic_set(&buf->done, 1);
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+}
+
+static inline int psb_xhw_add(struct drm_psb_private *dev_priv,
+			      struct psb_xhw_buf *buf)
+{
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	atomic_set(&buf->done, 0);
+	if (unlikely(!dev_priv->xhw_submit_ok)) {
+		spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+		DRM_ERROR("No Xpsb 3D extension available.\n");
+		return -EINVAL;
+	}
+	if (!list_empty(&buf->head)) {
+		DRM_ERROR("Recursive list adding.\n");
+		goto out;
+	}
+	list_add_tail(&buf->head, &dev_priv->xhw_in);
+	wake_up_interruptible(&dev_priv->xhw_queue);
+      out:
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+	return 0;
+}
+
+int psb_xhw_scene_info(struct drm_psb_private *dev_priv,
+		       struct psb_xhw_buf *buf,
+		       uint32_t w,
+		       uint32_t h,
+		       uint32_t * hw_cookie,
+		       uint32_t * bo_size,
+		       uint32_t * clear_p_start, uint32_t * clear_num_pages)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+	int ret;
+
+	buf->copy_back = 1;
+	xa->op = PSB_XHW_SCENE_INFO;
+	xa->irq_op = 0;
+	xa->issue_irq = 0;
+	xa->arg.si.w = w;
+	xa->arg.si.h = h;
+
+	ret = psb_xhw_add(dev_priv, buf);
+	if (ret)
+		return ret;
+
+	(void)wait_event_timeout(dev_priv->xhw_caller_queue,
+				 atomic_read(&buf->done), DRM_HZ);
+
+	if (!atomic_read(&buf->done)) {
+		psb_xhw_clean_buf(dev_priv, buf);
+		return -EBUSY;
+	}
+
+	if (!xa->ret) {
+		memcpy(hw_cookie, xa->cookie, sizeof(xa->cookie));
+		*bo_size = xa->arg.si.size;
+		*clear_p_start = xa->arg.si.clear_p_start;
+		*clear_num_pages = xa->arg.si.clear_num_pages;
+	}
+	return xa->ret;
+}
+
+int psb_xhw_fire_raster(struct drm_psb_private *dev_priv,
+			struct psb_xhw_buf *buf, uint32_t fire_flags)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+
+	buf->copy_back = 0;
+	xa->op = PSB_XHW_FIRE_RASTER;
+	xa->issue_irq = 0;
+	xa->arg.sb.fire_flags = 0;
+
+	return psb_xhw_add(dev_priv, buf);
+}
+
+int psb_xhw_vistest(struct drm_psb_private *dev_priv, struct psb_xhw_buf *buf)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+
+	buf->copy_back = 1;
+	xa->op = PSB_XHW_VISTEST;
+	/*
+	 * Could perhaps decrease latency somewhat by
+	 * issuing an irq in this case.
+	 */
+	xa->issue_irq = 0;
+	xa->irq_op = PSB_UIRQ_VISTEST;
+	return psb_xhw_add(dev_priv, buf);
+}
+
+int psb_xhw_scene_bind_fire(struct drm_psb_private *dev_priv,
+			    struct psb_xhw_buf *buf,
+			    uint32_t fire_flags,
+			    uint32_t hw_context,
+			    uint32_t * cookie,
+			    uint32_t * oom_cmds,
+			    uint32_t num_oom_cmds,
+			    uint32_t offset, uint32_t engine, uint32_t flags)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+
+	buf->copy_back = (fire_flags & PSB_FIRE_FLAG_XHW_OOM);
+	xa->op = PSB_XHW_SCENE_BIND_FIRE;
+	xa->issue_irq = (buf->copy_back) ? 1 : 0;
+	if (unlikely(buf->copy_back))
+		xa->irq_op = (engine == PSB_SCENE_ENGINE_TA) ?
+		    PSB_UIRQ_FIRE_TA_REPLY : PSB_UIRQ_FIRE_RASTER_REPLY;
+	else
+		xa->irq_op = 0;
+	xa->arg.sb.fire_flags = fire_flags;
+	xa->arg.sb.hw_context = hw_context;
+	xa->arg.sb.offset = offset;
+	xa->arg.sb.engine = engine;
+	xa->arg.sb.flags = flags;
+	xa->arg.sb.num_oom_cmds = num_oom_cmds;
+	memcpy(xa->cookie, cookie, sizeof(xa->cookie));
+	if (num_oom_cmds)
+		memcpy(xa->arg.sb.oom_cmds, oom_cmds,
+		       sizeof(uint32_t) * num_oom_cmds);
+	return psb_xhw_add(dev_priv, buf);
+}
+
+int psb_xhw_reset_dpm(struct drm_psb_private *dev_priv, struct psb_xhw_buf *buf)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+	int ret;
+
+	buf->copy_back = 1;
+	xa->op = PSB_XHW_RESET_DPM;
+	xa->issue_irq = 0;
+	xa->irq_op = 0;
+
+	ret = psb_xhw_add(dev_priv, buf);
+	if (ret)
+		return ret;
+
+	(void)wait_event_timeout(dev_priv->xhw_caller_queue,
+				 atomic_read(&buf->done), 3 * DRM_HZ);
+
+	if (!atomic_read(&buf->done)) {
+		psb_xhw_clean_buf(dev_priv, buf);
+		return -EBUSY;
+	}
+
+	return xa->ret;
+}
+
+int psb_xhw_check_lockup(struct drm_psb_private *dev_priv,
+			 struct psb_xhw_buf *buf, uint32_t * value)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+	int ret;
+
+	*value = 0;
+
+	buf->copy_back = 1;
+	xa->op = PSB_XHW_CHECK_LOCKUP;
+	xa->issue_irq = 0;
+	xa->irq_op = 0;
+
+	ret = psb_xhw_add(dev_priv, buf);
+	if (ret)
+		return ret;
+
+	(void)wait_event_timeout(dev_priv->xhw_caller_queue,
+				 atomic_read(&buf->done), DRM_HZ * 3);
+
+	if (!atomic_read(&buf->done)) {
+		psb_xhw_clean_buf(dev_priv, buf);
+		return -EBUSY;
+	}
+
+	if (!xa->ret)
+		*value = xa->arg.cl.value;
+
+	return xa->ret;
+}
+
+static int psb_xhw_terminate(struct drm_psb_private *dev_priv,
+			     struct psb_xhw_buf *buf)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+	unsigned long irq_flags;
+
+	buf->copy_back = 0;
+	xa->op = PSB_XHW_TERMINATE;
+	xa->issue_irq = 0;
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	dev_priv->xhw_submit_ok = 0;
+	atomic_set(&buf->done, 0);
+	if (!list_empty(&buf->head)) {
+		DRM_ERROR("Recursive list adding.\n");
+		goto out;
+	}
+	list_add_tail(&buf->head, &dev_priv->xhw_in);
+      out:
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+	wake_up_interruptible(&dev_priv->xhw_queue);
+
+	(void)wait_event_timeout(dev_priv->xhw_caller_queue,
+				 atomic_read(&buf->done), DRM_HZ / 10);
+
+	if (!atomic_read(&buf->done)) {
+		DRM_ERROR("Xpsb terminate timeout.\n");
+		psb_xhw_clean_buf(dev_priv, buf);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+int psb_xhw_ta_mem_info(struct drm_psb_private *dev_priv,
+			struct psb_xhw_buf *buf,
+			uint32_t pages, uint32_t * hw_cookie, uint32_t * size)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+	int ret;
+
+	buf->copy_back = 1;
+	xa->op = PSB_XHW_TA_MEM_INFO;
+	xa->issue_irq = 0;
+	xa->irq_op = 0;
+	xa->arg.bi.pages = pages;
+
+	ret = psb_xhw_add(dev_priv, buf);
+	if (ret)
+		return ret;
+
+	(void)wait_event_timeout(dev_priv->xhw_caller_queue,
+				 atomic_read(&buf->done), DRM_HZ);
+
+	if (!atomic_read(&buf->done)) {
+		psb_xhw_clean_buf(dev_priv, buf);
+		return -EBUSY;
+	}
+
+	if (!xa->ret)
+		memcpy(hw_cookie, xa->cookie, sizeof(xa->cookie));
+
+	*size = xa->arg.bi.size;
+	return xa->ret;
+}
+
+int psb_xhw_ta_mem_load(struct drm_psb_private *dev_priv,
+			struct psb_xhw_buf *buf,
+			uint32_t flags,
+			uint32_t param_offset,
+			uint32_t pt_offset, uint32_t * hw_cookie)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+	int ret;
+
+	buf->copy_back = 1;
+	xa->op = PSB_XHW_TA_MEM_LOAD;
+	xa->issue_irq = 0;
+	xa->irq_op = 0;
+	xa->arg.bl.flags = flags;
+	xa->arg.bl.param_offset = param_offset;
+	xa->arg.bl.pt_offset = pt_offset;
+	memcpy(xa->cookie, hw_cookie, sizeof(xa->cookie));
+
+	ret = psb_xhw_add(dev_priv, buf);
+	if (ret)
+		return ret;
+
+	(void)wait_event_timeout(dev_priv->xhw_caller_queue,
+				 atomic_read(&buf->done), 3 * DRM_HZ);
+
+	if (!atomic_read(&buf->done)) {
+		psb_xhw_clean_buf(dev_priv, buf);
+		return -EBUSY;
+	}
+
+	if (!xa->ret)
+		memcpy(hw_cookie, xa->cookie, sizeof(xa->cookie));
+
+	return xa->ret;
+}
+
+int psb_xhw_ta_oom(struct drm_psb_private *dev_priv,
+		   struct psb_xhw_buf *buf, uint32_t * cookie)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+
+	/*
+	 * This calls the extensive closed source
+	 * OOM handler, which resolves the condition and
+	 * sends a reply telling the scheduler what to do
+	 * with the task.
+	 */
+
+	buf->copy_back = 1;
+	xa->op = PSB_XHW_OOM;
+	xa->issue_irq = 1;
+	xa->irq_op = PSB_UIRQ_OOM_REPLY;
+	memcpy(xa->cookie, cookie, sizeof(xa->cookie));
+
+	return psb_xhw_add(dev_priv, buf);
+}
+
+void psb_xhw_ta_oom_reply(struct drm_psb_private *dev_priv,
+			  struct psb_xhw_buf *buf,
+			  uint32_t * cookie,
+			  uint32_t * bca, uint32_t * rca, uint32_t * flags)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+
+	/*
+	 * Get info about how to schedule an OOM task.
+	 */
+
+	memcpy(cookie, xa->cookie, sizeof(xa->cookie));
+	*bca = xa->arg.oom.bca;
+	*rca = xa->arg.oom.rca;
+	*flags = xa->arg.oom.flags;
+}
+
+void psb_xhw_fire_reply(struct drm_psb_private *dev_priv,
+			struct psb_xhw_buf *buf, uint32_t * cookie)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+
+	memcpy(cookie, xa->cookie, sizeof(xa->cookie));
+}
+
+int psb_xhw_resume(struct drm_psb_private *dev_priv, struct psb_xhw_buf *buf)
+{
+	struct drm_psb_xhw_arg *xa = &buf->arg;
+
+	buf->copy_back = 0;
+	xa->op = PSB_XHW_RESUME;
+	xa->issue_irq = 0;
+	xa->irq_op = 0;
+	return psb_xhw_add(dev_priv, buf);
+}
+
+void psb_xhw_takedown(struct drm_psb_private *dev_priv)
+{
+}
+
+int psb_xhw_init(struct drm_device *dev)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	unsigned long irq_flags;
+
+	INIT_LIST_HEAD(&dev_priv->xhw_in);
+	dev_priv->xhw_lock = SPIN_LOCK_UNLOCKED;
+	atomic_set(&dev_priv->xhw_client, 0);
+	init_waitqueue_head(&dev_priv->xhw_queue);
+	init_waitqueue_head(&dev_priv->xhw_caller_queue);
+	mutex_init(&dev_priv->xhw_mutex);
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	dev_priv->xhw_on = 0;
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+
+	return 0;
+}
+
+static int psb_xhw_init_init(struct drm_device *dev,
+			     struct drm_file *file_priv,
+			     struct drm_psb_xhw_init_arg *arg)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	int ret;
+	int is_iomem;
+
+	if (atomic_add_unless(&dev_priv->xhw_client, 1, 1)) {
+		unsigned long irq_flags;
+
+		mutex_lock(&dev->struct_mutex);
+		dev_priv->xhw_bo =
+		    drm_lookup_buffer_object(file_priv, arg->buffer_handle, 1);
+		mutex_unlock(&dev->struct_mutex);
+		if (!dev_priv->xhw_bo) {
+			ret = -EINVAL;
+			goto out_err;
+		}
+		ret = drm_bo_kmap(dev_priv->xhw_bo, 0,
+				  dev_priv->xhw_bo->num_pages,
+				  &dev_priv->xhw_kmap);
+		if (ret) {
+			DRM_ERROR("Failed mapping X server "
+				  "communications buffer.\n");
+			goto out_err0;
+		}
+		dev_priv->xhw = drm_bmo_virtual(&dev_priv->xhw_kmap, &is_iomem);
+		if (is_iomem) {
+			DRM_ERROR("X server communications buffer"
+				  "is in device memory.\n");
+			ret = -EINVAL;
+			goto out_err1;
+		}
+		dev_priv->xhw_file = file_priv;
+
+		spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+		dev_priv->xhw_on = 1;
+		dev_priv->xhw_submit_ok = 1;
+		spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+		return 0;
+	} else {
+		DRM_ERROR("Xhw is already initialized.\n");
+		return -EBUSY;
+	}
+      out_err1:
+	dev_priv->xhw = NULL;
+	drm_bo_kunmap(&dev_priv->xhw_kmap);
+      out_err0:
+	drm_bo_usage_deref_unlocked(&dev_priv->xhw_bo);
+      out_err:
+	atomic_dec(&dev_priv->xhw_client);
+	return ret;
+}
+
+static void psb_xhw_queue_empty(struct drm_psb_private *dev_priv)
+{
+	struct psb_xhw_buf *cur_buf, *next;
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	dev_priv->xhw_submit_ok = 0;
+
+	list_for_each_entry_safe(cur_buf, next, &dev_priv->xhw_in, head) {
+		list_del_init(&cur_buf->head);
+		if (cur_buf->copy_back) {
+			cur_buf->arg.ret = -EINVAL;
+		}
+		atomic_set(&cur_buf->done, 1);
+	}
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+	wake_up(&dev_priv->xhw_caller_queue);
+}
+
+void psb_xhw_init_takedown(struct drm_psb_private *dev_priv,
+			   struct drm_file *file_priv, int closing)
+{
+
+	if (dev_priv->xhw_file == file_priv &&
+	    atomic_add_unless(&dev_priv->xhw_client, -1, 0)) {
+
+		if (closing)
+			psb_xhw_queue_empty(dev_priv);
+		else {
+			struct psb_xhw_buf buf;
+			INIT_LIST_HEAD(&buf.head);
+
+			psb_xhw_terminate(dev_priv, &buf);
+			psb_xhw_queue_empty(dev_priv);
+		}
+
+		dev_priv->xhw = NULL;
+		drm_bo_kunmap(&dev_priv->xhw_kmap);
+		drm_bo_usage_deref_unlocked(&dev_priv->xhw_bo);
+		dev_priv->xhw_file = NULL;
+	}
+}
+
+int psb_xhw_init_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv)
+{
+	struct drm_psb_xhw_init_arg *arg = (struct drm_psb_xhw_init_arg *)data;
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+
+	switch (arg->operation) {
+	case PSB_XHW_INIT:
+		return psb_xhw_init_init(dev, file_priv, arg);
+	case PSB_XHW_TAKEDOWN:
+		psb_xhw_init_takedown(dev_priv, file_priv, 0);
+	}
+	return 0;
+}
+
+static int psb_xhw_in_empty(struct drm_psb_private *dev_priv)
+{
+	int empty;
+	unsigned long irq_flags;
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	empty = list_empty(&dev_priv->xhw_in);
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+	return empty;
+}
+
+int psb_xhw_handler(struct drm_psb_private *dev_priv)
+{
+	unsigned long irq_flags;
+	struct drm_psb_xhw_arg *xa;
+	struct psb_xhw_buf *buf;
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+
+	if (!dev_priv->xhw_on) {
+		spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+		return -EINVAL;
+	}
+
+	buf = dev_priv->xhw_cur_buf;
+	if (buf && buf->copy_back) {
+		xa = &buf->arg;
+		memcpy(xa, dev_priv->xhw, sizeof(*xa));
+		dev_priv->comm[PSB_COMM_USER_IRQ] = xa->irq_op;
+		atomic_set(&buf->done, 1);
+		wake_up(&dev_priv->xhw_caller_queue);
+	} else
+		dev_priv->comm[PSB_COMM_USER_IRQ] = 0;
+
+	dev_priv->xhw_cur_buf = 0;
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+	return 0;
+}
+
+int psb_xhw_ioctl(struct drm_device *dev, void *data,
+		  struct drm_file *file_priv)
+{
+	struct drm_psb_private *dev_priv =
+	    (struct drm_psb_private *)dev->dev_private;
+	unsigned long irq_flags;
+	struct drm_psb_xhw_arg *xa;
+	int ret;
+	struct list_head *list;
+	struct psb_xhw_buf *buf;
+
+	if (!dev_priv)
+		return -EINVAL;
+
+	if (mutex_lock_interruptible(&dev_priv->xhw_mutex))
+		return -EAGAIN;
+
+	if (psb_forced_user_interrupt(dev_priv)) {
+		mutex_unlock(&dev_priv->xhw_mutex);
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	while (list_empty(&dev_priv->xhw_in)) {
+		spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+		ret = wait_event_interruptible_timeout(dev_priv->xhw_queue,
+						       !psb_xhw_in_empty
+						       (dev_priv), DRM_HZ);
+		if (ret == -ERESTARTSYS || ret == 0) {
+			mutex_unlock(&dev_priv->xhw_mutex);
+			return -EAGAIN;
+		}
+		spin_lock_irqsave(&dev_priv->xhw_lock, irq_flags);
+	}
+
+	list = dev_priv->xhw_in.next;
+	list_del_init(list);
+
+	buf = list_entry(list, struct psb_xhw_buf, head);
+	xa = &buf->arg;
+	memcpy(dev_priv->xhw, xa, sizeof(*xa));
+
+	if (unlikely(buf->copy_back))
+		dev_priv->xhw_cur_buf = buf;
+	else {
+		atomic_set(&buf->done, 1);
+		dev_priv->xhw_cur_buf = NULL;
+	}
+
+	if (xa->op == PSB_XHW_TERMINATE) {
+		dev_priv->xhw_on = 0;
+		wake_up(&dev_priv->xhw_caller_queue);
+	}
+	spin_unlock_irqrestore(&dev_priv->xhw_lock, irq_flags);
+
+	mutex_unlock(&dev_priv->xhw_mutex);
+
+	return 0;
+}
Index: linux-2.6.27/drivers/gpu/drm/Kconfig
===================================================================
--- linux-2.6.27.orig/drivers/gpu/drm/Kconfig	2008-10-09 23:13:53.000000000 +0100
+++ linux-2.6.27/drivers/gpu/drm/Kconfig	2009-02-05 13:29:33.000000000 +0000
@@ -105,3 +105,9 @@
 	help
 	  Choose this option if you have a Savage3D/4/SuperSavage/Pro/Twister
 	  chipset. If M is selected the module will be called savage.
+
+config DRM_PSB
+	tristate "Intel Poulsbo"
+	depends on DRM && PCI && I2C_ALGOBIT
+	help
+	  Choose this option if you have an Intel Poulsbo chipset.
Index: linux-2.6.27/include/drm/drm_crtc.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/include/drm/drm_crtc.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,592 @@
+/*
+ * Copyright Â© 2006 Keith Packard
+ * Copyright Â© 2007 Intel Corporation
+ *   Jesse Barnes <jesse.barnes@intel.com>
+ */
+#ifndef __DRM_CRTC_H__
+#define __DRM_CRTC_H__
+
+#include <linux/i2c.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <linux/idr.h>
+
+#include <linux/fb.h>
+
+struct drm_device;
+
+/*
+ * Note on terminology:  here, for brevity and convenience, we refer to output
+ * control chips as 'CRTCs'.  They can control any type of output, VGA, LVDS,
+ * DVI, etc.  And 'screen' refers to the whole of the visible display, which
+ * may span multiple monitors (and therefore multiple CRTC and output
+ * structures).
+ */
+
+enum drm_mode_status {
+    MODE_OK	= 0,	/* Mode OK */
+    MODE_HSYNC,		/* hsync out of range */
+    MODE_VSYNC,		/* vsync out of range */
+    MODE_H_ILLEGAL,	/* mode has illegal horizontal timings */
+    MODE_V_ILLEGAL,	/* mode has illegal horizontal timings */
+    MODE_BAD_WIDTH,	/* requires an unsupported linepitch */
+    MODE_NOMODE,	/* no mode with a maching name */
+    MODE_NO_INTERLACE,	/* interlaced mode not supported */
+    MODE_NO_DBLESCAN,	/* doublescan mode not supported */
+    MODE_NO_VSCAN,	/* multiscan mode not supported */
+    MODE_MEM,		/* insufficient video memory */
+    MODE_VIRTUAL_X,	/* mode width too large for specified virtual size */
+    MODE_VIRTUAL_Y,	/* mode height too large for specified virtual size */
+    MODE_MEM_VIRT,	/* insufficient video memory given virtual size */
+    MODE_NOCLOCK,	/* no fixed clock available */
+    MODE_CLOCK_HIGH,	/* clock required is too high */
+    MODE_CLOCK_LOW,	/* clock required is too low */
+    MODE_CLOCK_RANGE,	/* clock/mode isn't in a ClockRange */
+    MODE_BAD_HVALUE,	/* horizontal timing was out of range */
+    MODE_BAD_VVALUE,	/* vertical timing was out of range */
+    MODE_BAD_VSCAN,	/* VScan value out of range */
+    MODE_HSYNC_NARROW,	/* horizontal sync too narrow */
+    MODE_HSYNC_WIDE,	/* horizontal sync too wide */
+    MODE_HBLANK_NARROW,	/* horizontal blanking too narrow */
+    MODE_HBLANK_WIDE,	/* horizontal blanking too wide */
+    MODE_VSYNC_NARROW,	/* vertical sync too narrow */
+    MODE_VSYNC_WIDE,	/* vertical sync too wide */
+    MODE_VBLANK_NARROW,	/* vertical blanking too narrow */
+    MODE_VBLANK_WIDE,	/* vertical blanking too wide */
+    MODE_PANEL,         /* exceeds panel dimensions */
+    MODE_INTERLACE_WIDTH, /* width too large for interlaced mode */
+    MODE_ONE_WIDTH,     /* only one width is supported */
+    MODE_ONE_HEIGHT,    /* only one height is supported */
+    MODE_ONE_SIZE,      /* only one resolution is supported */
+    MODE_NO_REDUCED,    /* monitor doesn't accept reduced blanking */
+    MODE_UNVERIFIED = -3, /* mode needs to reverified */
+    MODE_BAD = -2,	/* unspecified reason */
+    MODE_ERROR	= -1	/* error condition */
+};
+
+#define DRM_MODE_TYPE_CLOCK_CRTC_C (DRM_MODE_TYPE_CLOCK_C | \
+				    DRM_MODE_TYPE_CRTC_C)
+
+#define DRM_MODE(nm, t, c, hd, hss, hse, ht, hsk, vd, vss, vse, vt, vs, f) \
+	.name = nm, .status = 0, .type = (t), .clock = (c), \
+	.hdisplay = (hd), .hsync_start = (hss), .hsync_end = (hse), \
+	.htotal = (ht), .hskew = (hsk), .vdisplay = (vd), \
+	.vsync_start = (vss), .vsync_end = (vse), .vtotal = (vt), \
+	.vscan = (vs), .flags = (f), .vrefresh = 0
+
+struct drm_display_mode {
+	/* Header */
+	struct list_head head;
+	char name[DRM_DISPLAY_MODE_LEN];
+	int mode_id;
+	int output_count;
+	enum drm_mode_status status;
+	int type;
+
+	/* Proposed mode values */
+	int clock;
+	int hdisplay;
+	int hsync_start;
+	int hsync_end;
+	int htotal;
+	int hskew;
+	int vdisplay;
+	int vsync_start;
+	int vsync_end;
+	int vtotal;
+	int vscan;
+	unsigned int flags;
+
+	/* Actual mode we give to hw */
+	int clock_index;
+	int synth_clock;
+	int crtc_hdisplay;
+	int crtc_hblank_start;
+	int crtc_hblank_end;
+	int crtc_hsync_start;
+	int crtc_hsync_end;
+	int crtc_htotal;
+	int crtc_hskew;
+	int crtc_vdisplay;
+	int crtc_vblank_start;
+	int crtc_vblank_end;
+	int crtc_vsync_start;
+	int crtc_vsync_end;
+	int crtc_vtotal;
+	int crtc_hadjusted;
+	int crtc_vadjusted;
+
+	/* Driver private mode info */
+	int private_size;
+	int *private;
+	int private_flags;
+
+	int vrefresh;
+	float hsync;
+};
+
+/* Video mode flags */
+#define V_PHSYNC	(1<<0)
+#define V_NHSYNC	(1<<1)
+#define V_PVSYNC	(1<<2)
+#define V_NVSYNC	(1<<3)
+#define V_INTERLACE	(1<<4)
+#define V_DBLSCAN	(1<<5)
+#define V_CSYNC		(1<<6)
+#define V_PCSYNC	(1<<7)
+#define V_NCSYNC	(1<<8)
+#define V_HSKEW		(1<<9) /* hskew provided */
+#define V_BCAST		(1<<10)
+#define V_PIXMUX	(1<<11)
+#define V_DBLCLK	(1<<12)
+#define V_CLKDIV2	(1<<13)
+
+#define CRTC_INTERLACE_HALVE_V 0x1 /* halve V values for interlacing */
+#define DPMSModeOn 0
+#define DPMSModeStandby 1
+#define DPMSModeSuspend 2
+#define DPMSModeOff 3
+
+enum drm_output_status {
+	output_status_connected = 1,
+	output_status_disconnected = 2,
+	output_status_unknown = 3,
+};
+
+enum subpixel_order {
+	SubPixelUnknown = 0,
+	SubPixelHorizontalRGB,
+	SubPixelHorizontalBGR,
+	SubPixelVerticalRGB,
+	SubPixelVerticalBGR,
+	SubPixelNone,
+};
+
+/*
+ * Describes a given display (e.g. CRT or flat panel) and its limitations.
+ */
+struct drm_display_info {
+	char name[DRM_DISPLAY_INFO_LEN];
+	/* Input info */
+	bool serration_vsync;
+	bool sync_on_green;
+	bool composite_sync;
+	bool separate_syncs;
+	bool blank_to_black;
+	unsigned char video_level;
+	bool digital;
+	/* Physical size */
+        unsigned int width_mm;
+	unsigned int height_mm;
+
+	/* Display parameters */
+	unsigned char gamma; /* FIXME: storage format */
+	bool gtf_supported;
+	bool standard_color;
+	enum {
+		monochrome,
+		rgb,
+		other,
+		unknown,
+	} display_type;
+	bool active_off_supported;
+	bool suspend_supported;
+	bool standby_supported;
+
+	/* Color info FIXME: storage format */
+	unsigned short redx, redy;
+	unsigned short greenx, greeny;
+	unsigned short bluex, bluey;
+	unsigned short whitex, whitey;
+
+	/* Clock limits FIXME: storage format */
+	unsigned int min_vfreq, max_vfreq;
+	unsigned int min_hfreq, max_hfreq;
+	unsigned int pixel_clock;
+
+	/* White point indices FIXME: storage format */
+	unsigned int wpx1, wpy1;
+	unsigned int wpgamma1;
+	unsigned int wpx2, wpy2;
+	unsigned int wpgamma2;
+
+	/* Preferred mode (if any) */
+	struct drm_display_mode *preferred_mode;
+	char *raw_edid; /* if any */
+};
+
+struct drm_framebuffer {
+	struct drm_device *dev;
+	struct list_head head;
+	int id; /* idr assigned */
+	unsigned int pitch;
+	unsigned long offset;
+	unsigned int width;
+	unsigned int height;
+	/* depth can be 15 or 16 */
+	unsigned int depth;
+	int bits_per_pixel;
+	int flags;
+	struct drm_buffer_object *bo;
+	void *fbdev;
+	u32 pseudo_palette[16];
+	struct drm_bo_kmap_obj kmap;
+	struct list_head filp_head;
+};
+
+struct drm_property_enum {
+	struct list_head head;
+	uint32_t value;
+	unsigned char name[DRM_PROP_NAME_LEN];
+};
+
+struct drm_property {
+	struct list_head head;
+	int id; /* idr assigned */
+	uint32_t flags;
+	char name[DRM_PROP_NAME_LEN];
+	uint32_t num_values;
+	uint32_t *values;
+
+	struct list_head enum_list;
+};
+
+struct drm_crtc;
+struct drm_output;
+
+/**
+ * drm_crtc_funcs - control CRTCs for a given device
+ * @dpms: control display power levels
+ * @save: save CRTC state
+ * @resore: restore CRTC state
+ * @lock: lock the CRTC
+ * @unlock: unlock the CRTC
+ * @shadow_allocate: allocate shadow pixmap
+ * @shadow_create: create shadow pixmap for rotation support
+ * @shadow_destroy: free shadow pixmap
+ * @mode_fixup: fixup proposed mode
+ * @mode_set: set the desired mode on the CRTC
+ * @gamma_set: specify color ramp for CRTC
+ * @cleanup: cleanup driver private state prior to close
+ *
+ * The drm_crtc_funcs structure is the central CRTC management structure
+ * in the DRM.  Each CRTC controls one or more outputs (note that the name
+ * CRTC is simply historical, a CRTC may control LVDS, VGA, DVI, TV out, etc.
+ * outputs, not just CRTs).
+ *
+ * Each driver is responsible for filling out this structure at startup time,
+ * in addition to providing other modesetting features, like i2c and DDC
+ * bus accessors.
+ */
+struct drm_crtc_funcs {
+	/*
+	 * Control power levels on the CRTC.  If the mode passed in is
+	 * unsupported, the provider must use the next lowest power level.
+	 */
+	void (*dpms)(struct drm_crtc *crtc, int mode);
+
+	/* JJJ:  Are these needed? */
+	/* Save CRTC state */
+	void (*save)(struct drm_crtc *crtc); /* suspend? */
+	/* Restore CRTC state */
+	void (*restore)(struct drm_crtc *crtc); /* resume? */
+	bool (*lock)(struct drm_crtc *crtc);
+	void (*unlock)(struct drm_crtc *crtc);
+
+	void (*prepare)(struct drm_crtc *crtc);
+	void (*commit)(struct drm_crtc *crtc);
+
+	/* Provider can fixup or change mode timings before modeset occurs */
+	bool (*mode_fixup)(struct drm_crtc *crtc,
+			   struct drm_display_mode *mode,
+			   struct drm_display_mode *adjusted_mode);
+	/* Actually set the mode */
+	void (*mode_set)(struct drm_crtc *crtc, struct drm_display_mode *mode,
+			 struct drm_display_mode *adjusted_mode, int x, int y);
+	/* Set gamma on the CRTC */
+	void (*gamma_set)(struct drm_crtc *crtc, u16 r, u16 g, u16 b,
+			  int regno);
+	/* Driver cleanup routine */
+	void (*cleanup)(struct drm_crtc *crtc);
+};
+
+/**
+ * drm_crtc - central CRTC control structure
+ * @enabled: is this CRTC enabled?
+ * @x: x position on screen
+ * @y: y position on screen
+ * @desired_mode: new desired mode
+ * @desired_x: desired x for desired_mode
+ * @desired_y: desired y for desired_mode
+ * @funcs: CRTC control functions
+ * @driver_private: arbitrary driver data
+ *
+ * Each CRTC may have one or more outputs associated with it.  This structure
+ * allows the CRTC to be controlled.
+ */
+struct drm_crtc {
+	struct drm_device *dev;
+	struct list_head head;
+
+	int id; /* idr assigned */
+
+	/* framebuffer the output is currently bound to */
+	struct drm_framebuffer *fb;
+
+	bool enabled;
+
+	/* JJJ: are these needed? */
+	bool cursor_in_range;
+	bool cursor_shown;
+
+	struct drm_display_mode mode;
+
+	int x, y;
+	struct drm_display_mode *desired_mode;
+	int desired_x, desired_y;
+	const struct drm_crtc_funcs *funcs;
+	void *driver_private;
+
+	/* RRCrtcPtr randr_crtc? */
+};
+
+extern struct drm_crtc *drm_crtc_create(struct drm_device *dev,
+					const struct drm_crtc_funcs *funcs);
+
+/**
+ * drm_output_funcs - control outputs on a given device
+ * @init: setup this output
+ * @dpms: set power state (see drm_crtc_funcs above)
+ * @save: save output state
+ * @restore: restore output state
+ * @mode_valid: is this mode valid on the given output?
+ * @mode_fixup: try to fixup proposed mode for this output
+ * @mode_set: set this mode
+ * @detect: is this output active?
+ * @get_modes: get mode list for this output
+ * @set_property: property for this output may need update
+ * @cleanup: output is going away, cleanup
+ *
+ * Each CRTC may have one or more outputs attached to it.  The functions
+ * below allow the core DRM code to control outputs, enumerate available modes,
+ * etc.
+ */
+struct drm_output_funcs {
+	void (*init)(struct drm_output *output);
+	void (*dpms)(struct drm_output *output, int mode);
+	void (*save)(struct drm_output *output);
+	void (*restore)(struct drm_output *output);
+	int (*mode_valid)(struct drm_output *output,
+			  struct drm_display_mode *mode);
+	bool (*mode_fixup)(struct drm_output *output,
+			   struct drm_display_mode *mode,
+			   struct drm_display_mode *adjusted_mode);
+	void (*prepare)(struct drm_output *output);
+	void (*commit)(struct drm_output *output);
+	void (*mode_set)(struct drm_output *output,
+			 struct drm_display_mode *mode,
+			 struct drm_display_mode *adjusted_mode);
+	enum drm_output_status (*detect)(struct drm_output *output);
+	int (*get_modes)(struct drm_output *output);
+	/* JJJ: type checking for properties via property value type */
+	bool (*set_property)(struct drm_output *output, int prop, void *val);
+	void (*cleanup)(struct drm_output *output);
+};
+
+#define DRM_OUTPUT_MAX_UMODES 16
+#define DRM_OUTPUT_MAX_PROPERTY 16
+#define DRM_OUTPUT_LEN 32
+/**
+ * drm_output - central DRM output control structure
+ * @crtc: CRTC this output is currently connected to, NULL if none
+ * @possible_crtcs: bitmap of CRTCS this output could be attached to
+ * @possible_clones: bitmap of possible outputs this output could clone
+ * @interlace_allowed: can this output handle interlaced modes?
+ * @doublescan_allowed: can this output handle doublescan?
+ * @available_modes: modes available on this output (from get_modes() + user)
+ * @initial_x: initial x position for this output
+ * @initial_y: initial y position for this output
+ * @status: output connected?
+ * @subpixel_order: for this output
+ * @mm_width: displayable width of output in mm
+ * @mm_height: displayable height of output in mm
+ * @name: name of output (should be one of a few standard names)
+ * @funcs: output control functions
+ * @driver_private: private driver data
+ *
+ * Each output may be connected to one or more CRTCs, or may be clonable by
+ * another output if they can share a CRTC.  Each output also has a specific
+ * position in the broader display (referred to as a 'screen' though it could
+ * span multiple monitors).
+ */
+struct drm_output {
+	struct drm_device *dev;
+	struct list_head head;
+	struct drm_crtc *crtc;
+	int id; /* idr assigned */
+	unsigned long possible_crtcs;
+	unsigned long possible_clones;
+	bool interlace_allowed;
+	bool doublescan_allowed;
+	struct list_head modes; /* list of modes on this output */
+
+	/*
+	  OptionInfoPtr options;
+	  XF86ConfMonitorPtr conf_monitor;
+	 */
+	int initial_x, initial_y;
+	enum drm_output_status status;
+
+	/* these are modes added by probing with DDC or the BIOS */
+	struct list_head probed_modes;
+
+	/* xf86MonPtr MonInfo; */
+	enum subpixel_order subpixel_order;
+	int mm_width, mm_height;
+	struct drm_display_info *monitor_info; /* if any */
+	char name[DRM_OUTPUT_LEN];
+	const struct drm_output_funcs *funcs;
+	void *driver_private;
+
+	u32 user_mode_ids[DRM_OUTPUT_MAX_UMODES];
+
+	u32 property_ids[DRM_OUTPUT_MAX_PROPERTY];
+	u32 property_values[DRM_OUTPUT_MAX_PROPERTY];
+};
+
+/**
+ * struct drm_mode_config_funcs - configure CRTCs for a given screen layout
+ * @resize: adjust CRTCs as necessary for the proposed layout
+ *
+ * Currently only a resize hook is available.  DRM will call back into the
+ * driver with a new screen width and height.  If the driver can't support
+ * the proposed size, it can return false.  Otherwise it should adjust
+ * the CRTC<->output mappings as needed and update its view of the screen.
+ */
+struct drm_mode_config_funcs {
+	bool (*resize)(struct drm_device *dev, int width, int height);
+};
+
+/**
+ * drm_mode_config - Mode configuration control structure
+ *
+ */
+struct drm_mode_config {
+	struct mutex mutex; /* protects configuration and IDR */
+	struct idr crtc_idr; /* use this idr for all IDs, fb, crtc, output, modes - just makes life easier */
+	/* this is limited to one for now */
+	int num_fb;
+	struct list_head fb_list;
+	int num_output;
+	struct list_head output_list;
+
+	/* int compat_output? */
+	int num_crtc;
+	struct list_head crtc_list;
+
+	struct list_head usermode_list;
+
+	struct list_head property_list;
+
+	int min_width, min_height;
+	int max_width, max_height;
+	/* DamagePtr rotationDamage? */
+	/* DGA stuff? */
+	struct drm_mode_config_funcs *funcs;
+	unsigned long fb_base;
+};
+
+struct drm_output *drm_output_create(struct drm_device *dev,
+				     const struct drm_output_funcs *funcs,
+				     const char *name);
+extern void drm_output_destroy(struct drm_output *output);
+extern bool drm_output_rename(struct drm_output *output, const char *name);
+extern void drm_fb_release(struct file *filp);
+
+extern struct edid *drm_get_edid(struct drm_output *output,
+				 struct i2c_adapter *adapter);
+extern int drm_add_edid_modes(struct drm_output *output, struct edid *edid);
+extern void drm_mode_probed_add(struct drm_output *output, struct drm_display_mode *mode);
+extern void drm_mode_remove(struct drm_output *output, struct drm_display_mode *mode);
+extern struct drm_display_mode *drm_mode_duplicate(struct drm_device *dev,
+						   struct drm_display_mode *mode);
+extern void drm_mode_debug_printmodeline(struct drm_device *dev,
+					 struct drm_display_mode *mode);
+extern void drm_mode_config_init(struct drm_device *dev);
+extern void drm_mode_config_cleanup(struct drm_device *dev);
+extern void drm_mode_set_name(struct drm_display_mode *mode);
+extern bool drm_mode_equal(struct drm_display_mode *mode1, struct drm_display_mode *mode2);
+extern void drm_disable_unused_functions(struct drm_device *dev);
+
+extern void drm_mode_addmode(struct drm_device *dev, struct drm_display_mode *user_mode);
+extern int drm_mode_rmmode(struct drm_device *dev, struct drm_display_mode *mode);
+
+/* for us by fb module */
+extern int drm_mode_attachmode_crtc(struct drm_device *dev,
+				    struct drm_crtc *crtc,
+				    struct drm_display_mode *mode);
+extern int drm_mode_detachmode_crtc(struct drm_device *dev, struct drm_display_mode *mode);
+
+extern struct drm_display_mode *drm_mode_create(struct drm_device *dev);
+extern void drm_mode_destroy(struct drm_device *dev, struct drm_display_mode *mode);
+extern void drm_mode_list_concat(struct list_head *head,
+				 struct list_head *new);
+extern void drm_mode_validate_size(struct drm_device *dev,
+				   struct list_head *mode_list,
+				   int maxX, int maxY, int maxPitch);
+extern void drm_mode_prune_invalid(struct drm_device *dev,
+				   struct list_head *mode_list, bool verbose);
+extern void drm_mode_sort(struct list_head *mode_list);
+extern int drm_mode_vrefresh(struct drm_display_mode *mode);
+extern void drm_mode_set_crtcinfo(struct drm_display_mode *p,
+				  int adjust_flags);
+extern void drm_mode_output_list_update(struct drm_output *output);
+
+extern struct drm_display_mode *drm_crtc_mode_create(struct drm_device *dev);
+extern bool drm_initial_config(struct drm_device *dev, bool cangrow);
+extern void drm_framebuffer_set_object(struct drm_device *dev,
+				       unsigned long handle);
+extern struct drm_framebuffer *drm_framebuffer_create(struct drm_device *dev);
+extern void drm_framebuffer_destroy(struct drm_framebuffer *fb);
+extern int drmfb_probe(struct drm_device *dev, struct drm_crtc *crtc);
+extern int drmfb_remove(struct drm_device *dev, struct drm_crtc *crtc);
+extern bool drm_crtc_set_mode(struct drm_crtc *crtc, struct drm_display_mode *mode,
+		       int x, int y);
+
+extern int drm_output_attach_property(struct drm_output *output,
+				      struct drm_property *property, int init_val);
+extern struct drm_property *drm_property_create(struct drm_device *dev, int flags,
+						const char *name, int num_values);
+extern void drm_property_destroy(struct drm_device *dev, struct drm_property *property);
+extern int drm_property_add_enum(struct drm_property *property, int index,
+				 uint32_t value, const char *name);
+
+/* IOCTLs */
+extern int drm_mode_getresources(struct drm_device *dev,
+				 void *data, struct drm_file *file_priv);
+
+extern int drm_mode_getcrtc(struct drm_device *dev,
+			    void *data, struct drm_file *file_priv);
+extern int drm_mode_getoutput(struct drm_device *dev,
+			      void *data, struct drm_file *file_priv);
+extern int drm_mode_setcrtc(struct drm_device *dev,
+			    void *data, struct drm_file *file_priv);
+extern int drm_mode_addfb(struct drm_device *dev,
+			  void *data, struct drm_file *file_priv);
+extern int drm_mode_rmfb(struct drm_device *dev,
+			 void *data, struct drm_file *file_priv);
+extern int drm_mode_getfb(struct drm_device *dev,
+			  void *data, struct drm_file *file_priv);
+extern int drm_mode_addmode_ioctl(struct drm_device *dev,
+				  void *data, struct drm_file *file_priv);
+extern int drm_mode_rmmode_ioctl(struct drm_device *dev,
+				 void *data, struct drm_file *file_priv);
+extern int drm_mode_attachmode_ioctl(struct drm_device *dev,
+				     void *data, struct drm_file *file_priv);
+extern int drm_mode_detachmode_ioctl(struct drm_device *dev,
+				     void *data, struct drm_file *file_priv);
+
+extern int drm_mode_getproperty_ioctl(struct drm_device *dev,
+				      void *data, struct drm_file *file_priv);
+#endif /* __DRM_CRTC_H__ */
+
Index: linux-2.6.27/include/drm/drm_edid.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/include/drm/drm_edid.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,179 @@
+#ifndef __DRM_EDID_H__
+#define __DRM_EDID_H__
+
+#include <linux/types.h>
+
+#define EDID_LENGTH 128
+#define DDC_ADDR 0x50
+
+#ifdef BIG_ENDIAN
+#error "EDID structure is little endian, need big endian versions"
+#endif
+
+struct est_timings {
+	u8 t1;
+	u8 t2;
+	u8 mfg_rsvd;
+} __attribute__((packed));
+
+struct std_timing {
+	u8 hsize; /* need to multiply by 8 then add 248 */
+	u8 vfreq:6; /* need to add 60 */
+	u8 aspect_ratio:2; /* 00=16:10, 01=4:3, 10=5:4, 11=16:9 */
+} __attribute__((packed));
+
+/* If detailed data is pixel timing */
+struct detailed_pixel_timing {
+	u8 hactive_lo;
+	u8 hblank_lo;
+	u8 hblank_hi:4;
+	u8 hactive_hi:4;
+	u8 vactive_lo;
+	u8 vblank_lo;
+	u8 vblank_hi:4;
+	u8 vactive_hi:4;
+	u8 hsync_offset_lo;
+	u8 hsync_pulse_width_lo;
+	u8 vsync_pulse_width_lo:4;
+	u8 vsync_offset_lo:4;
+	u8 hsync_pulse_width_hi:2;
+	u8 hsync_offset_hi:2;
+	u8 vsync_pulse_width_hi:2;
+	u8 vsync_offset_hi:2;
+	u8 width_mm_lo;
+	u8 height_mm_lo;
+	u8 height_mm_hi:4;
+	u8 width_mm_hi:4;
+	u8 hborder;
+	u8 vborder;
+	u8 unknown0:1;
+	u8 vsync_positive:1;
+	u8 hsync_positive:1;
+	u8 separate_sync:2;
+	u8 stereo:1;
+	u8 unknown6:1;
+	u8 interlaced:1;
+} __attribute__((packed));
+
+/* If it's not pixel timing, it'll be one of the below */
+struct detailed_data_string {
+	u8 str[13];
+} __attribute__((packed));
+
+struct detailed_data_monitor_range {
+	u8 min_vfreq;
+	u8 max_vfreq;
+	u8 min_hfreq_khz;
+	u8 max_hfreq_khz;
+	u8 pixel_clock_mhz; /* need to multiply by 10 */
+	u16 sec_gtf_toggle; /* A000=use above, 20=use below */ /* FIXME: byte order */
+	u8 hfreq_start_khz; /* need to multiply by 2 */
+	u8 c; /* need to divide by 2 */
+	u16 m; /* FIXME: byte order */
+	u8 k;
+	u8 j; /* need to divide by 2 */
+} __attribute__((packed));
+
+struct detailed_data_wpindex {
+	u8 white_y_lo:2;
+	u8 white_x_lo:2;
+	u8 pad:4;
+	u8 white_x_hi;
+	u8 white_y_hi;
+	u8 gamma; /* need to divide by 100 then add 1 */
+} __attribute__((packed));
+
+struct detailed_data_color_point {
+	u8 windex1;
+	u8 wpindex1[3];
+	u8 windex2;
+	u8 wpindex2[3];
+} __attribute__((packed));
+
+struct detailed_non_pixel {
+	u8 pad1;
+	u8 type; /* ff=serial, fe=string, fd=monitor range, fc=monitor name
+		    fb=color point data, fa=standard timing data,
+		    f9=undefined, f8=mfg. reserved */
+	u8 pad2;
+	union {
+		struct detailed_data_string str;
+		struct detailed_data_monitor_range range;
+		struct detailed_data_wpindex color;
+		struct std_timing timings[5];
+	} data;
+} __attribute__((packed));
+
+#define EDID_DETAIL_STD_MODES 0xfa
+#define EDID_DETAIL_MONITOR_CPDATA 0xfb
+#define EDID_DETAIL_MONITOR_NAME 0xfc
+#define EDID_DETAIL_MONITOR_RANGE 0xfd
+#define EDID_DETAIL_MONITOR_STRING 0xfe
+#define EDID_DETAIL_MONITOR_SERIAL 0xff
+
+struct detailed_timing {
+	u16 pixel_clock; /* need to multiply by 10 KHz */ /* FIXME: byte order */
+	union {
+		struct detailed_pixel_timing pixel_data;
+		struct detailed_non_pixel other_data;
+	} data;
+} __attribute__((packed));
+
+struct edid {
+	u8 header[8];
+	/* Vendor & product info */
+	u16 mfg_id; /* FIXME: byte order */
+	u16 prod_code; /* FIXME: byte order */
+	u32 serial; /* FIXME: byte order */
+	u8 mfg_week;
+	u8 mfg_year;
+	/* EDID version */
+	u8 version;
+	u8 revision;
+	/* Display info: */
+	/*   input definition */
+	u8 serration_vsync:1;
+	u8 sync_on_green:1;
+	u8 composite_sync:1;
+	u8 separate_syncs:1;
+	u8 blank_to_black:1;
+	u8 video_level:2;
+	u8 digital:1; /* bits below must be zero if set */
+	u8 width_cm;
+	u8 height_cm;
+	u8 gamma;
+	/*   feature support */
+	u8 default_gtf:1;
+	u8 preferred_timing:1;
+	u8 standard_color:1;
+	u8 display_type:2; /* 00=mono, 01=rgb, 10=non-rgb, 11=unknown */
+	u8 pm_active_off:1;
+	u8 pm_suspend:1;
+	u8 pm_standby:1;
+	/* Color characteristics */
+	u8 red_green_lo;
+	u8 black_white_lo;
+	u8 red_x;
+	u8 red_y;
+	u8 green_x;
+	u8 green_y;
+	u8 blue_x;
+	u8 blue_y;
+	u8 white_x;
+	u8 white_y;
+	/* Est. timings and mfg rsvd timings*/
+	struct est_timings established_timings;
+	/* Standard timings 1-8*/
+	struct std_timing standard_timings[8];
+	/* Detailing timings 1-4 */
+	struct detailed_timing detailed_timings[4];
+	/* Number of 128 byte ext. blocks */
+	u8 extensions;
+	/* Checksum */
+	u8 checksum;
+} __attribute__((packed));
+
+extern unsigned char *drm_ddc_read(struct i2c_adapter *adapter);
+extern int drm_get_acpi_edid(char *method, char *edid, ssize_t length);
+
+#endif /* __DRM_EDID_H__ */
Index: linux-2.6.27/include/drm/drm_objects.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.27/include/drm/drm_objects.h	2009-02-05 13:29:33.000000000 +0000
@@ -0,0 +1,717 @@
+/**************************************************************************
+ *
+ * Copyright (c) 2006-2007 Tungsten Graphics, Inc., Cedar Park, TX., USA
+ * All Rights Reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the
+ * "Software"), to deal in the Software without restriction, including
+ * without limitation the rights to use, copy, modify, merge, publish,
+ * distribute, sub license, and/or sell copies of the Software, and to
+ * permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the
+ * next paragraph) shall be included in all copies or substantial portions
+ * of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ **************************************************************************/
+/*
+ * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
+ */
+
+#ifndef _DRM_OBJECTS_H
+#define _DRM_OBJECTS_H
+
+struct drm_device;
+struct drm_bo_mem_reg;
+
+/***************************************************
+ * User space objects. (drm_object.c)
+ */
+
+#define drm_user_object_entry(_ptr, _type, _member) container_of(_ptr, _type, _member)
+
+enum drm_object_type {
+	drm_fence_type,
+	drm_buffer_type,
+	drm_lock_type,
+	    /*
+	     * Add other user space object types here.
+	     */
+	drm_driver_type0 = 256,
+	drm_driver_type1,
+	drm_driver_type2,
+	drm_driver_type3,
+	drm_driver_type4
+};
+
+/*
+ * A user object is a structure that helps the drm give out user handles
+ * to kernel internal objects and to keep track of these objects so that
+ * they can be destroyed, for example when the user space process exits.
+ * Designed to be accessible using a user space 32-bit handle.
+ */
+
+struct drm_user_object {
+	struct drm_hash_item hash;
+	struct list_head list;
+	enum drm_object_type type;
+	atomic_t refcount;
+	int shareable;
+	struct drm_file *owner;
+	void (*ref_struct_locked) (struct drm_file *priv,
+				   struct drm_user_object *obj,
+				   enum drm_ref_type ref_action);
+	void (*unref) (struct drm_file *priv, struct drm_user_object *obj,
+		       enum drm_ref_type unref_action);
+	void (*remove) (struct drm_file *priv, struct drm_user_object *obj);
+};
+
+/*
+ * A ref object is a structure which is used to
+ * keep track of references to user objects and to keep track of these
+ * references so that they can be destroyed for example when the user space
+ * process exits. Designed to be accessible using a pointer to the _user_ object.
+ */
+
+struct drm_ref_object {
+	struct drm_hash_item hash;
+	struct list_head list;
+	atomic_t refcount;
+	enum drm_ref_type unref_action;
+};
+
+/**
+ * Must be called with the struct_mutex held.
+ */
+
+extern int drm_add_user_object(struct drm_file *priv, struct drm_user_object *item,
+			       int shareable);
+/**
+ * Must be called with the struct_mutex held.
+ */
+
+extern struct drm_user_object *drm_lookup_user_object(struct drm_file *priv,
+						 uint32_t key);
+
+/*
+ * Must be called with the struct_mutex held. May temporarily release it.
+ */
+
+extern int drm_add_ref_object(struct drm_file *priv,
+			      struct drm_user_object *referenced_object,
+			      enum drm_ref_type ref_action);
+
+/*
+ * Must be called with the struct_mutex held.
+ */
+
+struct drm_ref_object *drm_lookup_ref_object(struct drm_file *priv,
+					struct drm_user_object *referenced_object,
+					enum drm_ref_type ref_action);
+/*
+ * Must be called with the struct_mutex held.
+ * If "item" has been obtained by a call to drm_lookup_ref_object. You may not
+ * release the struct_mutex before calling drm_remove_ref_object.
+ * This function may temporarily release the struct_mutex.
+ */
+
+extern void drm_remove_ref_object(struct drm_file *priv, struct drm_ref_object *item);
+extern int drm_user_object_ref(struct drm_file *priv, uint32_t user_token,
+			       enum drm_object_type type,
+			       struct drm_user_object **object);
+extern int drm_user_object_unref(struct drm_file *priv, uint32_t user_token,
+				 enum drm_object_type type);
+
+/***************************************************
+ * Fence objects. (drm_fence.c)
+ */
+
+struct drm_fence_object {
+	struct drm_user_object base;
+	struct drm_device *dev;
+	atomic_t usage;
+
+	/*
+	 * The below three fields are protected by the fence manager spinlock.
+	 */
+
+	struct list_head ring;
+	int fence_class;
+	uint32_t native_types;
+	uint32_t type;
+	uint32_t signaled_types;
+	uint32_t sequence;
+	uint32_t waiting_types;
+	uint32_t error;
+};
+
+#define _DRM_FENCE_CLASSES 8
+
+struct drm_fence_class_manager {
+	struct list_head ring;
+	uint32_t pending_flush;
+	uint32_t waiting_types;
+	wait_queue_head_t fence_queue;
+	uint32_t highest_waiting_sequence;
+        uint32_t latest_queued_sequence;
+};
+
+struct drm_fence_manager {
+	int initialized;
+	rwlock_t lock;
+	struct drm_fence_class_manager fence_class[_DRM_FENCE_CLASSES];
+	uint32_t num_classes;
+	atomic_t count;
+};
+
+struct drm_fence_driver {
+	unsigned long *waiting_jiffies;
+	uint32_t num_classes;
+	uint32_t wrap_diff;
+	uint32_t flush_diff;
+	uint32_t sequence_mask;
+
+	/*
+	 * Driver implemented functions:
+	 * has_irq() : 1 if the hardware can update the indicated type_flags using an
+	 * irq handler. 0 if polling is required.
+	 *
+	 * emit() : Emit a sequence number to the command stream.
+	 * Return the sequence number.
+	 *
+	 * flush() : Make sure the flags indicated in fc->pending_flush will eventually
+	 * signal for fc->highest_received_sequence and all preceding sequences.
+	 * Acknowledge by clearing the flags fc->pending_flush.
+	 *
+	 * poll() : Call drm_fence_handler with any new information.
+	 *
+	 * needed_flush() : Given the current state of the fence->type flags and previusly
+	 * executed or queued flushes, return the type_flags that need flushing.
+	 *
+	 * wait(): Wait for the "mask" flags to signal on a given fence, performing
+	 * whatever's necessary to make this happen.
+	 */
+
+	int (*has_irq) (struct drm_device *dev, uint32_t fence_class,
+			uint32_t flags);
+	int (*emit) (struct drm_device *dev, uint32_t fence_class,
+		     uint32_t flags, uint32_t *breadcrumb,
+		     uint32_t *native_type);
+	void (*flush) (struct drm_device *dev, uint32_t fence_class);
+	void (*poll) (struct drm_device *dev, uint32_t fence_class,
+		uint32_t types);
+	uint32_t (*needed_flush) (struct drm_fence_object *fence);
+	int (*wait) (struct drm_fence_object *fence, int lazy,
+		     int interruptible, uint32_t mask);
+};
+
+extern int drm_fence_wait_polling(struct drm_fence_object *fence, int lazy,
+				  int interruptible, uint32_t mask,
+				  unsigned long end_jiffies);
+extern void drm_fence_handler(struct drm_device *dev, uint32_t fence_class,
+			      uint32_t sequence, uint32_t type,
+			      uint32_t error);
+extern void drm_fence_manager_init(struct drm_device *dev);
+extern void drm_fence_manager_takedown(struct drm_device *dev);
+extern void drm_fence_flush_old(struct drm_device *dev, uint32_t fence_class,
+				uint32_t sequence);
+extern int drm_fence_object_flush(struct drm_fence_object *fence,
+				  uint32_t type);
+extern int drm_fence_object_signaled(struct drm_fence_object *fence,
+				     uint32_t type);
+extern void drm_fence_usage_deref_locked(struct drm_fence_object **fence);
+extern void drm_fence_usage_deref_unlocked(struct drm_fence_object **fence);
+extern struct drm_fence_object *drm_fence_reference_locked(struct drm_fence_object *src);
+extern void drm_fence_reference_unlocked(struct drm_fence_object **dst,
+					 struct drm_fence_object *src);
+extern int drm_fence_object_wait(struct drm_fence_object *fence,
+				 int lazy, int ignore_signals, uint32_t mask);
+extern int drm_fence_object_create(struct drm_device *dev, uint32_t type,
+				   uint32_t fence_flags, uint32_t fence_class,
+				   struct drm_fence_object **c_fence);
+extern int drm_fence_object_emit(struct drm_fence_object *fence,
+				 uint32_t fence_flags, uint32_t class,
+				 uint32_t type);
+extern void drm_fence_fill_arg(struct drm_fence_object *fence,
+			       struct drm_fence_arg *arg);
+
+extern int drm_fence_add_user_object(struct drm_file *priv,
+				     struct drm_fence_object *fence,
+				     int shareable);
+
+extern int drm_fence_create_ioctl(struct drm_device *dev, void *data,
+				  struct drm_file *file_priv);
+extern int drm_fence_destroy_ioctl(struct drm_device *dev, void *data,
+				   struct drm_file *file_priv);
+extern int drm_fence_reference_ioctl(struct drm_device *dev, void *data,
+				     struct drm_file *file_priv);
+extern int drm_fence_unreference_ioctl(struct drm_device *dev, void *data,
+				       struct drm_file *file_priv);
+extern int drm_fence_signaled_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *file_priv);
+extern int drm_fence_flush_ioctl(struct drm_device *dev, void *data,
+				 struct drm_file *file_priv);
+extern int drm_fence_wait_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *file_priv);
+extern int drm_fence_emit_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *file_priv);
+extern int drm_fence_buffers_ioctl(struct drm_device *dev, void *data,
+				   struct drm_file *file_priv);
+/**************************************************
+ *TTMs
+ */
+
+/*
+ * The ttm backend GTT interface. (In our case AGP).
+ * Any similar type of device (PCIE?)
+ * needs only to implement these functions to be usable with the TTM interface.
+ * The AGP backend implementation lives in drm_agpsupport.c
+ * basically maps these calls to available functions in agpgart.
+ * Each drm device driver gets an
+ * additional function pointer that creates these types,
+ * so that the device can choose the correct aperture.
+ * (Multiple AGP apertures, etc.)
+ * Most device drivers will let this point to the standard AGP implementation.
+ */
+
+#define DRM_BE_FLAG_NEEDS_FREE     0x00000001
+#define DRM_BE_FLAG_BOUND_CACHED   0x00000002
+
+struct drm_ttm_backend;
+struct drm_ttm_backend_func {
+	int (*needs_ub_cache_adjust) (struct drm_ttm_backend *backend);
+	int (*populate) (struct drm_ttm_backend *backend,
+			 unsigned long num_pages, struct page **pages);
+	void (*clear) (struct drm_ttm_backend *backend);
+	int (*bind) (struct drm_ttm_backend *backend,
+		     struct drm_bo_mem_reg *bo_mem);
+	int (*unbind) (struct drm_ttm_backend *backend);
+	void (*destroy) (struct drm_ttm_backend *backend);
+};
+
+
+struct drm_ttm_backend {
+	struct drm_device *dev;
+	uint32_t flags;
+	struct drm_ttm_backend_func *func;
+};
+
+struct drm_ttm {
+	struct page *dummy_read_page;
+	struct page **pages;
+	uint32_t page_flags;
+	unsigned long num_pages;
+	atomic_t vma_count;
+	struct drm_device *dev;
+	int destroy;
+	uint32_t mapping_offset;
+	struct drm_ttm_backend *be;
+	enum {
+		ttm_bound,
+		ttm_evicted,
+		ttm_unbound,
+		ttm_unpopulated,
+	} state;
+
+};
+
+extern struct drm_ttm *drm_ttm_init(struct drm_device *dev, unsigned long size);
+extern int drm_bind_ttm(struct drm_ttm *ttm, struct drm_bo_mem_reg *bo_mem);
+extern void drm_ttm_unbind(struct drm_ttm *ttm);
+extern void drm_ttm_evict(struct drm_ttm *ttm);
+extern void drm_ttm_fixup_caching(struct drm_ttm *ttm);
+extern struct page *drm_ttm_get_page(struct drm_ttm *ttm, int index);
+extern void drm_ttm_cache_flush(void);
+extern int drm_ttm_populate(struct drm_ttm *ttm);
+extern int drm_ttm_set_user(struct drm_ttm *ttm,
+			    struct task_struct *tsk,
+			    int write,
+			    unsigned long start,
+			    unsigned long num_pages,
+			    struct page *dummy_read_page);
+unsigned long drm_ttm_size(struct drm_device *dev,
+			   unsigned long num_pages,
+			   int user_bo);
+
+
+/*
+ * Destroy a ttm. The user normally calls drmRmMap or a similar IOCTL to do
+ * this which calls this function iff there are no vmas referencing it anymore.
+ * Otherwise it is called when the last vma exits.
+ */
+
+extern int drm_destroy_ttm(struct drm_ttm *ttm);
+
+#define DRM_FLAG_MASKED(_old, _new, _mask) {\
+(_old) ^= (((_old) ^ (_new)) & (_mask)); \
+}
+
+#define DRM_TTM_MASK_FLAGS ((1 << PAGE_SHIFT) - 1)
+#define DRM_TTM_MASK_PFN (0xFFFFFFFFU - DRM_TTM_MASK_FLAGS)
+
+/*
+ * Page flags.
+ */
+
+#define DRM_TTM_PAGE_UNCACHED   (1 << 0)
+#define DRM_TTM_PAGE_USED       (1 << 1)
+#define DRM_TTM_PAGE_BOUND      (1 << 2)
+#define DRM_TTM_PAGE_PRESENT    (1 << 3)
+#define DRM_TTM_PAGE_VMALLOC    (1 << 4)
+#define DRM_TTM_PAGE_USER       (1 << 5)
+#define DRM_TTM_PAGE_USER_WRITE (1 << 6)
+#define DRM_TTM_PAGE_USER_DIRTY (1 << 7)
+#define DRM_TTM_PAGE_USER_DMA   (1 << 8)
+
+/***************************************************
+ * Buffer objects. (drm_bo.c, drm_bo_move.c)
+ */
+
+struct drm_bo_mem_reg {
+	struct drm_mm_node *mm_node;
+	unsigned long size;
+	unsigned long num_pages;
+	uint32_t page_alignment;
+	uint32_t mem_type;
+	uint64_t flags;
+	uint64_t mask;
+	uint32_t desired_tile_stride;
+	uint32_t hw_tile_stride;
+};
+
+enum drm_bo_type {
+	drm_bo_type_dc,
+	drm_bo_type_user,
+	drm_bo_type_kernel, /* for initial kernel allocations */
+};
+
+struct drm_buffer_object {
+	struct drm_device *dev;
+	struct drm_user_object base;
+
+	/*
+	 * If there is a possibility that the usage variable is zero,
+	 * then dev->struct_mutext should be locked before incrementing it.
+	 */
+
+	atomic_t usage;
+	unsigned long buffer_start;
+	enum drm_bo_type type;
+	unsigned long offset;
+	atomic_t mapped;
+	struct drm_bo_mem_reg mem;
+
+	struct list_head lru;
+	struct list_head ddestroy;
+
+	uint32_t fence_type;
+	uint32_t fence_class;
+	uint32_t new_fence_type;
+	uint32_t new_fence_class;
+	struct drm_fence_object *fence;
+	uint32_t priv_flags;
+	wait_queue_head_t event_queue;
+	struct mutex mutex;
+	unsigned long num_pages;
+	unsigned long reserved_size;
+
+	/* For pinned buffers */
+	struct drm_mm_node *pinned_node;
+	uint32_t pinned_mem_type;
+	struct list_head pinned_lru;
+
+	/* For vm */
+	struct drm_ttm *ttm;
+	struct drm_map_list map_list;
+	uint32_t memory_type;
+	unsigned long bus_offset;
+	uint32_t vm_flags;
+	void *iomap;
+
+#ifdef DRM_ODD_MM_COMPAT
+	/* dev->struct_mutex only protected. */
+	struct list_head vma_list;
+	struct list_head p_mm_list;
+#endif
+
+};
+
+#define _DRM_BO_FLAG_UNFENCED 0x00000001
+#define _DRM_BO_FLAG_EVICTED  0x00000002
+
+struct drm_mem_type_manager {
+	int has_type;
+	int use_type;
+	struct drm_mm manager;
+	struct list_head lru;
+	struct list_head pinned;
+	uint32_t flags;
+	uint32_t drm_bus_maptype;
+	unsigned long gpu_offset;
+	unsigned long io_offset;
+	unsigned long io_size;
+	void *io_addr;
+};
+
+struct drm_bo_lock {
+	struct drm_user_object base;
+	wait_queue_head_t queue;
+	atomic_t write_lock_pending;
+	atomic_t readers;
+};
+
+#define _DRM_FLAG_MEMTYPE_FIXED     0x00000001	/* Fixed (on-card) PCI memory */
+#define _DRM_FLAG_MEMTYPE_MAPPABLE  0x00000002	/* Memory mappable */
+#define _DRM_FLAG_MEMTYPE_CACHED    0x00000004	/* Cached binding */
+#define _DRM_FLAG_NEEDS_IOREMAP     0x00000008	/* Fixed memory needs ioremap
+						   before kernel access. */
+#define _DRM_FLAG_MEMTYPE_CMA       0x00000010	/* Can't map aperture */
+#define _DRM_FLAG_MEMTYPE_CSELECT   0x00000020	/* Select caching */
+
+struct drm_buffer_manager {
+	struct drm_bo_lock bm_lock;
+	struct mutex evict_mutex;
+	int nice_mode;
+	int initialized;
+	struct drm_file *last_to_validate;
+	struct drm_mem_type_manager man[DRM_BO_MEM_TYPES];
+	struct list_head unfenced;
+	struct list_head ddestroy;
+	struct delayed_work wq;
+	uint32_t fence_type;
+	unsigned long cur_pages;
+	atomic_t count;
+	struct page *dummy_read_page;
+};
+
+struct drm_bo_driver {
+	const uint32_t *mem_type_prio;
+	const uint32_t *mem_busy_prio;
+	uint32_t num_mem_type_prio;
+	uint32_t num_mem_busy_prio;
+	struct drm_ttm_backend *(*create_ttm_backend_entry)
+	 (struct drm_device *dev);
+	int (*backend_size) (struct drm_device *dev,
+			     unsigned long num_pages);
+	int (*fence_type) (struct drm_buffer_object *bo, uint32_t *fclass,
+			   uint32_t *type);
+	int (*invalidate_caches) (struct drm_device *dev, uint64_t flags);
+	int (*init_mem_type) (struct drm_device *dev, uint32_t type,
+			      struct drm_mem_type_manager *man);
+	 uint32_t(*evict_mask) (struct drm_buffer_object *bo);
+	int (*move) (struct drm_buffer_object *bo,
+		     int evict, int no_wait, struct drm_bo_mem_reg *new_mem);
+	void (*ttm_cache_flush)(struct drm_ttm *ttm);
+
+	/*
+	 * command_stream_barrier
+	 *
+	 * @dev: The drm device.
+	 *
+	 * @bo: The buffer object to validate.
+	 *
+	 * @new_fence_class: The new fence class for the buffer object.
+	 *
+	 * @new_fence_type: The new fence type for the buffer object.
+	 *
+	 * @no_wait: whether this should give up and return -EBUSY
+	 * if this operation would require sleeping
+	 *
+	 * Insert a command stream barrier that makes sure that the
+	 * buffer is idle once the commands associated with the
+	 * current validation are starting to execute. If an error
+	 * condition is returned, or the function pointer is NULL,
+	 * the drm core will force buffer idle
+	 * during validation.
+	 */
+
+	int (*command_stream_barrier) (struct drm_buffer_object *bo,
+				       uint32_t new_fence_class,
+				       uint32_t new_fence_type,
+				       int no_wait);
+};
+
+/*
+ * buffer objects (drm_bo.c)
+ */
+extern int drm_bo_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_destroy_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_map_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_unmap_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_reference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_set_pin(struct drm_device *dev, struct drm_buffer_object *bo, int pin);
+extern int drm_bo_unreference_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_wait_idle_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_info_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_setstatus_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_init_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_takedown_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_lock_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_mm_unlock_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_version_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv);
+extern int drm_bo_driver_finish(struct drm_device *dev);
+extern int drm_bo_driver_init(struct drm_device *dev);
+extern int drm_bo_pci_offset(struct drm_device *dev,
+			     struct drm_bo_mem_reg *mem,
+			     unsigned long *bus_base,
+			     unsigned long *bus_offset,
+			     unsigned long *bus_size);
+extern int drm_mem_reg_is_pci(struct drm_device *dev, struct drm_bo_mem_reg *mem);
+
+extern void drm_bo_usage_deref_locked(struct drm_buffer_object **bo);
+extern void drm_bo_usage_deref_unlocked(struct drm_buffer_object **bo);
+extern void drm_putback_buffer_objects(struct drm_device *dev);
+extern int drm_fence_buffer_objects(struct drm_device *dev,
+				    struct list_head *list,
+				    uint32_t fence_flags,
+				    struct drm_fence_object *fence,
+				    struct drm_fence_object **used_fence);
+extern void drm_bo_add_to_lru(struct drm_buffer_object *bo);
+extern int drm_buffer_object_create(struct drm_device *dev, unsigned long size,
+				    enum drm_bo_type type, uint64_t mask,
+				    uint32_t hint, uint32_t page_alignment,
+				    unsigned long buffer_start,
+				    struct drm_buffer_object **bo);
+extern int drm_bo_wait(struct drm_buffer_object *bo, int lazy, int ignore_signals,
+		       int no_wait);
+extern int drm_bo_mem_space(struct drm_buffer_object *bo,
+			    struct drm_bo_mem_reg *mem, int no_wait);
+extern int drm_bo_move_buffer(struct drm_buffer_object *bo,
+			      uint64_t new_mem_flags,
+			      int no_wait, int move_unfenced);
+extern int drm_bo_clean_mm(struct drm_device *dev, unsigned mem_type);
+extern int drm_bo_init_mm(struct drm_device *dev, unsigned type,
+			  unsigned long p_offset, unsigned long p_size);
+extern int drm_bo_handle_validate(struct drm_file *file_priv, uint32_t handle,
+				  uint32_t fence_class, uint64_t flags,
+				  uint64_t mask, uint32_t hint,
+				  int use_old_fence_class,
+				  struct drm_bo_info_rep *rep,
+				  struct drm_buffer_object **bo_rep);
+extern struct drm_buffer_object *drm_lookup_buffer_object(struct drm_file *file_priv,
+							  uint32_t handle,
+							  int check_owner);
+extern int drm_bo_do_validate(struct drm_buffer_object *bo,
+			      uint64_t flags, uint64_t mask, uint32_t hint,
+			      uint32_t fence_class,
+			      int no_wait,
+			      struct drm_bo_info_rep *rep);
+extern void drm_bo_fill_rep_arg(struct drm_buffer_object *bo,
+				struct drm_bo_info_rep *rep);
+/*
+ * Buffer object memory move- and map helpers.
+ * drm_bo_move.c
+ */
+
+extern int drm_bo_move_ttm(struct drm_buffer_object *bo,
+			   int evict, int no_wait,
+			   struct drm_bo_mem_reg *new_mem);
+extern int drm_bo_move_memcpy(struct drm_buffer_object *bo,
+			      int evict,
+			      int no_wait, struct drm_bo_mem_reg *new_mem);
+extern int drm_bo_move_accel_cleanup(struct drm_buffer_object *bo,
+				     int evict, int no_wait,
+				     uint32_t fence_class, uint32_t fence_type,
+				     uint32_t fence_flags,
+				     struct drm_bo_mem_reg *new_mem);
+extern int drm_bo_same_page(unsigned long offset, unsigned long offset2);
+extern unsigned long drm_bo_offset_end(unsigned long offset,
+				       unsigned long end);
+
+struct drm_bo_kmap_obj {
+	void *virtual;
+	struct page *page;
+	enum {
+		bo_map_iomap,
+		bo_map_vmap,
+		bo_map_kmap,
+		bo_map_premapped,
+	} bo_kmap_type;
+};
+
+static inline void *drm_bmo_virtual(struct drm_bo_kmap_obj *map, int *is_iomem)
+{
+	*is_iomem = (map->bo_kmap_type == bo_map_iomap ||
+		     map->bo_kmap_type == bo_map_premapped);
+	return map->virtual;
+}
+extern void drm_bo_kunmap(struct drm_bo_kmap_obj *map);
+extern int drm_bo_kmap(struct drm_buffer_object *bo, unsigned long start_page,
+		       unsigned long num_pages, struct drm_bo_kmap_obj *map);
+
+
+/*
+ * drm_regman.c
+ */
+
+struct drm_reg {
+	struct list_head head;
+	struct drm_fence_object *fence;
+	uint32_t fence_type;
+	uint32_t new_fence_type;
+};
+
+struct drm_reg_manager {
+	struct list_head free;
+	struct list_head lru;
+	struct list_head unfenced;
+
+	int (*reg_reusable)(const struct drm_reg *reg, const void *data);
+	void (*reg_destroy)(struct drm_reg *reg);
+};
+
+extern int drm_regs_alloc(struct drm_reg_manager *manager,
+			  const void *data,
+			  uint32_t fence_class,
+			  uint32_t fence_type,
+			  int interruptible,
+			  int no_wait,
+			  struct drm_reg **reg);
+
+extern void drm_regs_fence(struct drm_reg_manager *regs,
+			   struct drm_fence_object *fence);
+
+extern void drm_regs_free(struct drm_reg_manager *manager);
+extern void drm_regs_add(struct drm_reg_manager *manager, struct drm_reg *reg);
+extern void drm_regs_init(struct drm_reg_manager *manager,
+			  int (*reg_reusable)(const struct drm_reg *,
+					      const void *),
+			  void (*reg_destroy)(struct drm_reg *));
+
+extern int drm_mem_reg_ioremap(struct drm_device *dev, struct drm_bo_mem_reg * mem,
+			       void **virtual);
+extern void drm_mem_reg_iounmap(struct drm_device *dev, struct drm_bo_mem_reg * mem,
+				void *virtual);
+/*
+ * drm_bo_lock.c
+ * Simple replacement for the hardware lock on buffer manager init and clean.
+ */
+
+
+extern void drm_bo_init_lock(struct drm_bo_lock *lock);
+extern void drm_bo_read_unlock(struct drm_bo_lock *lock);
+extern int drm_bo_read_lock(struct drm_bo_lock *lock);
+extern int drm_bo_write_lock(struct drm_bo_lock *lock,
+			     struct drm_file *file_priv);
+
+extern int drm_bo_write_unlock(struct drm_bo_lock *lock,
+			       struct drm_file *file_priv);
+
+#ifdef CONFIG_DEBUG_MUTEXES
+#define DRM_ASSERT_LOCKED(_mutex)					\
+	BUG_ON(!mutex_is_locked(_mutex) ||				\
+	       ((_mutex)->owner != current_thread_info()))
+#else
+#define DRM_ASSERT_LOCKED(_mutex)
+#endif
+#endif
